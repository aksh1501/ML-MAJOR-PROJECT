{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Trainingfin Notebook","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"jYHbhbZqf7go","colab_type":"code","outputId":"f43fc61f-1d5b-4ef9-e22d-701505963ff6","executionInfo":{"status":"ok","timestamp":1587485952483,"user_tz":-330,"elapsed":1704,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pge9BCKKuMNy","colab_type":"code","colab":{}},"source":["# !pip3 uninstall gym\n","# !pip3 install gym"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Te5ZHDSXBPkt","colab_type":"code","colab":{}},"source":["import gym\n","import random\n","import numpy as np\n","from statistics import median, mean\n","from collections import Counter\n","\n","from matplotlib import pyplot as plt\n","from IPython.display import clear_output\n","\n","import keras\n","from keras import models\n","from keras.layers import Dense, Dropout\n","from keras.layers import  Activation, Flatten\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","from keras.models import load_model\n","\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L35JT6LIuL4q","colab_type":"code","outputId":"3accd84c-e25b-479c-8e14-5c70ca93a60d","executionInfo":{"status":"ok","timestamp":1587485961501,"user_tz":-330,"elapsed":10677,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import tensorflow as tf\n","tf.__version__"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.4.0'"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"3GH5HiIjuOwv","colab_type":"code","colab":{}},"source":["if tf.__version__ != \"1.4.0\":\n","  !pip3 uninstall tensorflow\n","  !pip3 install tensorflow==1.4.0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oE8--hSOuocG","colab_type":"text"},"source":["\\"]},{"cell_type":"markdown","metadata":{"id":"cY0rC0shWJpZ","colab_type":"text"},"source":["Dont Restart Runtime"]},{"cell_type":"code","metadata":{"id":"pxjs6jy4uRb3","colab_type":"code","outputId":"a4d597f8-124d-4819-e314-c59023234039","executionInfo":{"status":"ok","timestamp":1587485961503,"user_tz":-330,"elapsed":10644,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import keras\n","keras.__version__"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.2.4'"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"hU_QwHuZmsEn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U5G2exdmuXF-","colab_type":"code","colab":{}},"source":["if keras.__version__!=\"2.2.4\":\n","  !pip3 uninstall keras\n","  !pip3 install Keras==2.2.4"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iizRucWw4R5I","colab_type":"text"},"source":["Restart Runtime"]},{"cell_type":"markdown","metadata":{"id":"kqpSa-HU4Utp","colab_type":"text"},"source":["\\"]},{"cell_type":"code","metadata":{"id":"wXyjjhisf7lY","colab_type":"code","colab":{}},"source":["# print(gym.__file__)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B-mHHWBZrR4X","colab_type":"code","outputId":"04d709be-d3d3-4598-c0b3-3161d6a54208","executionInfo":{"status":"ok","timestamp":1587485961507,"user_tz":-330,"elapsed":10584,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["keras.__version__"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.2.4'"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"WrILkWPyrSCD","colab_type":"code","outputId":"cb9751b0-ca1d-40d8-a0e3-a6b8c57398db","executionInfo":{"status":"ok","timestamp":1587485961508,"user_tz":-330,"elapsed":10508,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tf.__version__"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.4.0'"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"eDYENGVeBzrv","colab_type":"code","colab":{}},"source":["!pip3 freeze > env1.yml"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HuBhsn3w9Yhz","colab_type":"code","outputId":"348c2357-1e27-44e0-9dee-1ea1b2480195","executionInfo":{"status":"ok","timestamp":1587485976732,"user_tz":-330,"elapsed":25634,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!cat env1.yml"],"execution_count":53,"outputs":[{"output_type":"stream","text":["absl-py==0.9.0\n","alabaster==0.7.12\n","albumentations==0.1.12\n","altair==4.1.0\n","asgiref==3.2.7\n","astor==0.8.1\n","astropy==4.0.1.post1\n","astunparse==1.6.3\n","atari-py==0.2.6\n","atomicwrites==1.3.0\n","attrs==19.3.0\n","audioread==2.1.8\n","autograd==1.3\n","Babel==2.8.0\n","backcall==0.1.0\n","beautifulsoup4==4.6.3\n","bleach==1.5.0\n","blis==0.4.1\n","bokeh==1.4.0\n","boto==2.49.0\n","boto3==1.12.40\n","botocore==1.15.40\n","Bottleneck==1.3.2\n","branca==0.4.0\n","bs4==0.0.1\n","CacheControl==0.12.6\n","cachetools==3.1.1\n","catalogue==1.0.0\n","certifi==2020.4.5.1\n","cffi==1.14.0\n","chainer==6.5.0\n","chardet==3.0.4\n","click==7.1.1\n","cloudpickle==1.3.0\n","cmake==3.12.0\n","cmdstanpy==0.4.0\n","colorlover==0.3.0\n","community==1.0.0b1\n","contextlib2==0.5.5\n","convertdate==2.2.0\n","coverage==3.7.1\n","coveralls==0.5\n","crcmod==1.7\n","cufflinks==0.17.3\n","cupy-cuda101==6.5.0\n","cvxopt==1.2.5\n","cvxpy==1.0.31\n","cycler==0.10.0\n","cymem==2.0.3\n","Cython==0.29.16\n","daft==0.0.4\n","dask==2.12.0\n","dataclasses==0.7\n","datascience==0.10.6\n","decorator==4.4.2\n","defusedxml==0.6.0\n","descartes==1.1.0\n","dill==0.3.1.1\n","distributed==1.25.3\n","Django==3.0.5\n","dlib==19.18.0\n","docopt==0.6.2\n","docutils==0.15.2\n","dopamine-rl==1.0.5\n","earthengine-api==0.1.218\n","easydict==1.9\n","ecos==2.0.7.post1\n","editdistance==0.5.3\n","en-core-web-sm==2.2.5\n","entrypoints==0.3\n","enum34==1.1.10\n","ephem==3.7.7.1\n","et-xmlfile==1.0.1\n","fa2==0.3.5\n","fancyimpute==0.4.3\n","fastai==1.0.60\n","fastdtw==0.3.4\n","fastprogress==0.2.3\n","fastrlock==0.4\n","fbprophet==0.6\n","feather-format==0.4.0\n","featuretools==0.4.1\n","filelock==3.0.12\n","firebase-admin==4.0.1\n","fix-yahoo-finance==0.0.22\n","Flask==1.1.2\n","folium==0.8.3\n","fsspec==0.7.2\n","future==0.16.0\n","gast==0.3.3\n","GDAL==2.2.2\n","gdown==3.6.4\n","gensim==3.6.0\n","geographiclib==1.50\n","geopy==1.17.0\n","gin-config==0.3.0\n","glob2==0.7\n","google==2.0.3\n","google-api-core==1.16.0\n","google-api-python-client==1.7.12\n","google-auth==1.7.2\n","google-auth-httplib2==0.0.3\n","google-auth-oauthlib==0.4.1\n","google-cloud-bigquery==1.21.0\n","google-cloud-core==1.0.3\n","google-cloud-datastore==1.8.0\n","google-cloud-firestore==1.6.2\n","google-cloud-language==1.2.0\n","google-cloud-storage==1.18.1\n","google-cloud-translate==1.5.0\n","google-colab==1.0.0\n","google-pasta==0.2.0\n","google-resumable-media==0.4.1\n","googleapis-common-protos==1.51.0\n","googledrivedownloader==0.4\n","graphviz==0.10.1\n","grpcio==1.28.1\n","gspread==3.0.1\n","gspread-dataframe==3.0.5\n","gym==0.17.1\n","h5py==2.10.0\n","HeapDict==1.0.1\n","holidays==0.9.12\n","html5lib==0.9999999\n","httpimport==0.5.18\n","httplib2==0.17.2\n","httplib2shim==0.0.3\n","humanize==0.5.1\n","hyperopt==0.1.2\n","ideep4py==2.0.0.post3\n","idna==2.8\n","image==1.5.30\n","imageio==2.4.1\n","imagesize==1.2.0\n","imbalanced-learn==0.4.3\n","imblearn==0.0\n","imgaug==0.2.9\n","importlib-metadata==1.6.0\n","imutils==0.5.3\n","inflect==2.1.0\n","intel-openmp==2020.0.133\n","intervaltree==2.1.0\n","ipykernel==4.10.1\n","ipython==5.5.0\n","ipython-genutils==0.2.0\n","ipython-sql==0.3.9\n","ipywidgets==7.5.1\n","itsdangerous==1.1.0\n","jax==0.1.62\n","jaxlib==0.1.42\n","jdcal==1.4.1\n","jedi==0.17.0\n","jieba==0.42.1\n","Jinja2==2.11.2\n","jmespath==0.9.5\n","joblib==0.14.1\n","jpeg4py==0.1.4\n","jsonschema==2.6.0\n","jupyter==1.0.0\n","jupyter-client==5.3.4\n","jupyter-console==5.2.0\n","jupyter-core==4.6.3\n","kaggle==1.5.6\n","kapre==0.1.3.1\n","Keras==2.2.4\n","Keras-Applications==1.0.8\n","Keras-Preprocessing==1.1.0\n","keras-rl==0.4.2\n","keras-vis==0.4.1\n","kiwisolver==1.2.0\n","knnimpute==0.1.0\n","librosa==0.6.3\n","lightgbm==2.2.3\n","llvmlite==0.31.0\n","lmdb==0.98\n","lucid==0.3.8\n","LunarCalendar==0.0.9\n","lxml==4.2.6\n","Markdown==3.2.1\n","MarkupSafe==1.1.1\n","matplotlib==3.2.1\n","matplotlib-venn==0.11.5\n","missingno==0.4.2\n","mistune==0.8.4\n","mizani==0.6.0\n","mkl==2019.0\n","mlxtend==0.14.0\n","more-itertools==8.2.0\n","moviepy==0.2.3.5\n","mpmath==1.1.0\n","msgpack==1.0.0\n","multiprocess==0.70.9\n","multitasking==0.0.9\n","murmurhash==1.0.2\n","music21==5.5.0\n","natsort==5.5.0\n","nbconvert==5.6.1\n","nbformat==5.0.5\n","networkx==2.4\n","nibabel==3.0.2\n","nltk==3.2.5\n","notebook==5.2.2\n","np-utils==0.5.12.1\n","numba==0.48.0\n","numexpr==2.7.1\n","numpy==1.18.2\n","nvidia-ml-py3==7.352.0\n","oauth2client==4.1.3\n","oauthlib==3.1.0\n","okgrade==0.4.3\n","opencv-contrib-python==4.1.2.30\n","opencv-python==4.1.2.30\n","openpyxl==2.5.9\n","opt-einsum==3.2.1\n","osqp==0.6.1\n","packaging==20.3\n","palettable==3.3.0\n","pandas==1.0.3\n","pandas-datareader==0.8.1\n","pandas-gbq==0.11.0\n","pandas-profiling==1.4.1\n","pandocfilters==1.4.2\n","parso==0.7.0\n","pathlib==1.0.1\n","patsy==0.5.1\n","pexpect==4.8.0\n","pickleshare==0.7.5\n","Pillow==7.0.0\n","pip-tools==4.5.1\n","plac==1.1.3\n","plotly==4.4.1\n","plotnine==0.6.0\n","pluggy==0.7.1\n","portpicker==1.3.1\n","prefetch-generator==1.0.1\n","preshed==3.0.2\n","prettytable==0.7.2\n","progressbar2==3.38.0\n","prometheus-client==0.7.1\n","promise==2.3\n","prompt-toolkit==1.0.18\n","protobuf==3.10.0\n","psutil==5.4.8\n","psycopg2==2.7.6.1\n","ptvsd==5.0.0a12\n","ptyprocess==0.6.0\n","py==1.8.1\n","pyarrow==0.14.1\n","pyasn1==0.4.8\n","pyasn1-modules==0.2.8\n","pycocotools==2.0.0\n","pycparser==2.20\n","pydata-google-auth==0.3.0\n","pydot==1.3.0\n","pydot-ng==2.0.0\n","pydotplus==2.0.2\n","PyDrive==1.3.1\n","pyemd==0.5.1\n","pyglet==1.5.0\n","Pygments==2.1.3\n","pygobject==3.26.1\n","pymc3==3.7\n","PyMeeus==0.3.7\n","pymongo==3.10.1\n","pymystem3==0.2.0\n","PyOpenGL==3.1.5\n","pyparsing==2.4.7\n","pyrsistent==0.16.0\n","pysndfile==1.3.8\n","PySocks==1.7.1\n","pystan==2.19.1.1\n","pytest==3.6.4\n","python-apt==1.6.5+ubuntu0.2\n","python-chess==0.23.11\n","python-dateutil==2.8.1\n","python-louvain==0.14\n","python-slugify==4.0.0\n","python-utils==2.4.0\n","pytz==2018.9\n","PyWavelets==1.1.1\n","PyYAML==3.13\n","pyzmq==19.0.0\n","qtconsole==4.7.3\n","QtPy==1.9.0\n","regex==2019.12.20\n","requests==2.21.0\n","requests-oauthlib==1.3.0\n","resampy==0.2.2\n","retrying==1.3.3\n","rpy2==3.2.7\n","rsa==4.0\n","s3fs==0.4.2\n","s3transfer==0.3.3\n","scikit-image==0.16.2\n","scikit-learn==0.22.2.post1\n","scipy==1.4.1\n","screen-resolution-extra==0.0.0\n","scs==2.1.2\n","seaborn==0.10.0\n","Send2Trash==1.5.0\n","setuptools-git==1.2\n","Shapely==1.7.0\n","simplegeneric==0.8.1\n","six==1.12.0\n","sklearn==0.0\n","sklearn-pandas==1.8.0\n","smart-open==1.11.1\n","snowballstemmer==2.0.0\n","sortedcontainers==2.1.0\n","spacy==2.2.4\n","Sphinx==1.8.5\n","sphinxcontrib-websupport==1.2.1\n","SQLAlchemy==1.3.16\n","sqlparse==0.3.1\n","srsly==1.0.2\n","statsmodels==0.10.2\n","sympy==1.1.1\n","tables==3.4.4\n","tabulate==0.8.7\n","tbb==2020.0.133\n","tblib==1.6.0\n","tensorboard==2.2.1\n","tensorboard-plugin-wit==1.6.0.post3\n","tensorboardcolab==0.0.22\n","tensorflow==1.4.0\n","tensorflow-addons==0.8.3\n","tensorflow-datasets==2.1.0\n","tensorflow-estimator==2.2.0\n","tensorflow-gcs-config==2.1.8\n","tensorflow-hub==0.8.0\n","tensorflow-metadata==0.21.2\n","tensorflow-privacy==0.2.2\n","tensorflow-probability==0.10.0rc0\n","tensorflow-tensorboard==0.4.0\n","termcolor==1.1.0\n","terminado==0.8.3\n","testpath==0.4.4\n","text-unidecode==1.3\n","textblob==0.15.3\n","textgenrnn==1.4.1\n","Theano==1.0.4\n","thinc==7.4.0\n","toolz==0.10.0\n","torch==1.4.0\n","torchsummary==1.5.1\n","torchtext==0.3.1\n","torchvision==0.5.0\n","tornado==4.5.3\n","tqdm==4.38.0\n","traitlets==4.3.3\n","tweepy==3.6.0\n","typeguard==2.7.1\n","typing==3.6.6\n","typing-extensions==3.6.6\n","tzlocal==1.5.1\n","umap-learn==0.4.1\n","uritemplate==3.0.1\n","urllib3==1.24.3\n","vega-datasets==0.8.0\n","wasabi==0.6.0\n","wcwidth==0.1.9\n","webencodings==0.5.1\n","Werkzeug==1.0.1\n","widgetsnbextension==3.5.1\n","wordcloud==1.5.0\n","wrapt==1.12.1\n","xarray==0.15.1\n","xgboost==0.90\n","xkit==0.0.0\n","xlrd==1.1.0\n","xlwt==1.3.0\n","yellowbrick==0.9.1\n","zict==2.0.0\n","zipp==3.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1OXA-K2xsoG1","colab_type":"code","outputId":"56f56df9-08b0-4269-9bc2-146ea78fd4c8","executionInfo":{"status":"ok","timestamp":1587486013760,"user_tz":-330,"elapsed":62646,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["TASK_NUM = 1\n","\n","gym_dir = \"/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/\"\n","task_dir = \"/content/drive/My Drive/ML Major Project/Question/\"\n","model_dir = \"/content/drive/My Drive/ML Major Project/Models/Akshit's Models/Task\"+str(TASK_NUM)+\"/\"\n","\n","gym_file = gym_dir+'cartpole.py'\n","task_file = task_dir+'task'+str(TASK_NUM)+'.py'\n","task_file_copy = task_dir+'cartpole.py'\n","\n","model_count = !(ls \"$model_dir\" | wc -l)\n","\n","model_count = ((int)(model_count[0]))//2 + 1\n","\n","#model_count=4\n","model_count=11\n","\n","model_file = model_dir + 'Model#' + str(model_count) + '.h5'\n","plot_file = model_dir + 'Plot#' + str(model_count) + '.png'\n","model_summary_file =  model_dir + 'ModelSummary#' + str(model_count) + '.txt'\n","log_file = model_dir + 'log#' + str(model_count) + '.json'\n","\n","!rm \"$gym_file\"\n","!cp \"$task_file\" \"$task_file_copy\"\n","!mv \"$task_file_copy\" \"$gym_dir\"\n","\n","#check for friction (TASK1, TASK2, TASK3)\n","!cat \"$gym_file\" | head -n24 | tail -n2 \n","\n","#check for uncommented force error (TASK2, TASK3)\n","!cat \"$gym_file\" | head -n19 | tail -n2\n","\n","#check for uncommented noise (TASK3)\n","!cat \"$gym_file\" | head -n63 | tail -n2"],"execution_count":54,"outputs":[{"output_type":"stream","text":["\n","\t\tself.force_mag = 10.0\n","\t\tself.masspole = 0.4\n","\t\tself.total_mass = (self.masspole + self.masscart)\n","\t\tforce = self.force_mag if action==1 else -self.force_mag\n","\t\tcostheta = math.cos(theta)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mynsgq7LAer_","colab_type":"code","colab":{}},"source":["# print(task_file)\n","# !cat \"$task_file\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Do_o5K4yo5Pn","colab_type":"code","outputId":"6bfe23cb-ebaf-4891-ae28-11a37de91d96","executionInfo":{"status":"ok","timestamp":1587486019882,"user_tz":-330,"elapsed":68741,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["!pip3 install keras-rl"],"execution_count":56,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras-rl in /usr/local/lib/python3.6/dist-packages (0.4.2)\n","Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.4)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.4.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.18.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.10.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LdzbwNS7vmyD","colab_type":"code","outputId":"b8407cb2-2e76-444c-dfbd-de03ae71c526","executionInfo":{"status":"ok","timestamp":1587486026122,"user_tz":-330,"elapsed":74935,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!cat /usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/cartpole.py"],"execution_count":57,"outputs":[{"output_type":"stream","text":["import math\n","import gym\n","from gym import spaces, logger\n","from gym.utils import seeding\n","import numpy as np\n","\n","class CartPoleEnv(gym.Env):\n","\tmetadata = {\n","\t\t\t'render.modes': ['human', 'rgb_array'],\n","\t\t\t'video.frames_per_second' : 50\n","\t}\n","\n","\tdef __init__(self,case=1):\n","\t\tself.__version__ = \"0.2.0\"\n","\t\tprint(\"CartPoleEnv - Version {}, Noise case: {}\".format(self.__version__,case))\n","\t\tself.gravity = 9.8\n","\t\tself.masscart = 1.0\n","\t\tself.masspole = 0.4\n","\t\tself.total_mass = (self.masspole + self.masscart)\n","\t\tself.length = 0.5 \n","\t\tself.polemass_length = (self.masspole * self.length)\n","\t\tself._seed()\n","\n","\t\tself.force_mag = 10.0\n","\t\t#self.force_mag = 10.0*(1+self.np_random.uniform(low=-0.10, high=0.10))\n","\n","\t\t \n","\t\tself.tau = 0.02  # seconds between state updates\n","\t\tself.frictioncart = 5e-4 # AA Added cart friction\n","\t\tself.frictionpole = 2e-6 # AA Added cart friction\n","\t\tself.gravity_eps = 0.99 # Random scaling for gravity\n","\t\tself.frictioncart_eps = 0.99 # Random scaling for friction\n","\t\tself.frictionpole_eps = 0.99 # Random scaling for friction\n","\n","\t\t# Angle at which to fail the episode\n","\t\tself.theta_threshold_radians = 12 * 2 * math.pi / 360\n","\t\tself.x_threshold = 2.4\n","\n","\t\t# Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n","\t\thigh = np.array([\n","\t\t\tself.x_threshold * 2,\n","\t\t\tnp.finfo(np.float32).max,\n","\t\t\tself.theta_threshold_radians * 2,\n","\t\t\tnp.finfo(np.float32).max])\n","\n","\t\tself.action_space = spaces.Discrete(2) # AA Set discrete states back to 2\n","\t\tself.observation_space = spaces.Box(-high, high)\n","\n","\t\tself.viewer = None\n","\t\tself.state = None\n","\n","\t\tself.steps_beyond_done = None\n","\n","\tdef _seed(self, seed=None): # Set appropriate seed value\n","\t\tself.np_random, seed = seeding.np_random(seed)\n","\t\treturn [seed]\n","\n","\tdef _step(self, action):\n","\t\tassert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n","\t\tstate = self.state\n","\t\tx, x_dot, theta, theta_dot = state\n","\t\tforce = self.force_mag if action==1 else -self.force_mag\n","\t\tcostheta = math.cos(theta)\n","\t\tsintheta = math.sin(theta)\n","\t\ttemp = (force + self.polemass_length * theta_dot * theta_dot * sintheta - self.frictioncart * (4 + self.frictioncart_eps*np.random.randn()) *np.sign(x_dot)) / self.total_mass # AA Added cart friction\n","\t\tthetaacc = (self.gravity * (4 + self.gravity_eps*np.random.randn()) * sintheta - costheta* temp - self.frictionpole * (4 + self.frictionpole_eps*np.random.randn()) *theta_dot/self.polemass_length) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass)) # AA Added pole friction\n","\t\txacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n","\t\tnoise = 0\n","\t\t#noise = self.np_random.uniform(low=-0.10, high=0.10) \n","\t\tx  = (x + self.tau * x_dot)\n","\t\tx_dot = (x_dot + self.tau * xacc)\n","\t\ttheta = (theta + self.tau * theta_dot)*(1 + noise)\n","\t\ttheta_dot = (theta_dot + self.tau * thetaacc)\n","\t\tself.state = (x,x_dot,theta,theta_dot)\n","\t\tdone =  x < -self.x_threshold \\\n","\t\t\t\tor x > self.x_threshold \\\n","\t\t\t\tor theta < -self.theta_threshold_radians \\\n","\t\t\t\tor theta > self.theta_threshold_radians\n","\t\tdone = bool(done)\n","\n","\t\tif not done:\n","\t\t\treward = 1.0\n","\t\telif self.steps_beyond_done is None:\n","\t\t\t# Pole just fell!\n","\t\t\tself.steps_beyond_done = 0\n","\t\t\treward = 1.0\n","\t\telse:\n","\t\t\tif self.steps_beyond_done == 0:\n","\t\t\t\tlogger.warning(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n","\t\t\tself.steps_beyond_done += 1\n","\t\t\treward = 0.0\n","\n","\t\treturn np.array(self.state), reward, done, {}\n","\n","\tdef _reset(self):\n","\t\tself.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n","\t\tself.steps_beyond_done = None\n","\t\treturn np.array(self.state)\n","\n","\tdef _render(self, mode='human', close=False):\n","\t\tif close:\n","\t\t\tif self.viewer is not None:\n","\t\t\t\tself.viewer.close()\n","\t\t\t\tself.viewer = None\n","\t\t\treturn\n","\n","\t\tscreen_width = 600\n","\t\tscreen_height = 400\n","\n","\t\tworld_width = self.x_threshold*2\n","\t\tscale = screen_width/world_width\n","\t\tcarty = 100 # TOP OF CART\n","\t\tpolewidth = 10.0\n","\t\tpolelen = scale * 1.0\n","\t\tcartwidth = 50.0\n","\t\tcartheight = 30.0\n","\n","\t\tif self.viewer is None:\n","\t\t\tfrom gym.envs.classic_control import rendering\n","\t\t\tself.viewer = rendering.Viewer(screen_width, screen_height)\n","\t\t\tl,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n","\t\t\taxleoffset =cartheight/4.0\n","\t\t\tcart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n","\t\t\tself.carttrans = rendering.Transform()\n","\t\t\tcart.add_attr(self.carttrans)\n","\t\t\tself.viewer.add_geom(cart)\n","\t\t\tl,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n","\t\t\tpole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n","\t\t\tpole.set_color(.8,.6,.4)\n","\t\t\tself.poletrans = rendering.Transform(translation=(0, axleoffset))\n","\t\t\tpole.add_attr(self.poletrans)\n","\t\t\tpole.add_attr(self.carttrans)\n","\t\t\tself.viewer.add_geom(pole)\n","\t\t\tself.axle = rendering.make_circle(polewidth/2)\n","\t\t\tself.axle.add_attr(self.poletrans)\n","\t\t\tself.axle.add_attr(self.carttrans)\n","\t\t\tself.axle.set_color(.5,.5,.8)\n","\t\t\tself.viewer.add_geom(self.axle)\n","\t\t\tself.track = rendering.Line((0,carty), (screen_width,carty))\n","\t\t\tself.track.set_color(0,0,0)\n","\t\t\tself.viewer.add_geom(self.track)\n","\n","\t\tif self.state is None: return None\n","\n","\t\tx = self.state\n","\t\tcartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n","\t\tself.carttrans.set_translation(cartx, carty)\n","\t\tself.poletrans.set_rotation(-x[2])\n","\t\treturn self.viewer.render(return_rgb_array = mode=='rgb_array')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4nmlt2w-o7-n","colab_type":"code","colab":{}},"source":["from rl.agents.dqn import DQNAgent\n","from rl.policy import BoltzmannQPolicy\n","from rl.memory import SequentialMemory\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UYARHvr8vcYi","colab_type":"code","colab":{}},"source":["#HYPERPARAMETERS\n","LR = 1e-3\n","goal_steps = 500\n","score_requirement = 50\n","initial_games = 10000\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJeuI8rcf7WF","colab_type":"code","outputId":"537aa535-989b-4007-e90b-d7204a932d6c","executionInfo":{"status":"ok","timestamp":1587486026125,"user_tz":-330,"elapsed":74873,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["env = gym.make(\"CartPole-v1\")\n","env.reset()"],"execution_count":60,"outputs":[{"output_type":"stream","text":["CartPoleEnv - Version 0.2.0, Noise case: 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["array([-0.00675904,  0.02004715,  0.00288407,  0.01573368])"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"Jwso6Yg8oZqt","colab_type":"code","colab":{}},"source":["np.random.seed(123)\n","env.seed(123)\n","nb_actions = env.action_space.n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1W5865b9C2_y","colab_type":"code","colab":{}},"source":["def some_random_games_first():\n","    # Each of these is its own game.\n","    for episode in range(10):\n","        env.reset()\n","        # this is each frame, up to 200...but we wont make it that far.\n","        for t in range(200):\n","            # This will display the environment\n","            # Only display if you really want to see it.\n","            # Takes much longer to display it.\n","            #env.render()\n","            \n","            # This will just create a sample action in any environment.\n","            # In this environment, the action can be 0 or 1, which is left or right\n","            action = env.action_space.sample()\n","            #print(action)\n","            \n","            # this executes the environment with an action, \n","            # and returns the observation of the environment, \n","            # the reward, if the env is over, and other info.\n","            observation, reward, done, info = env.step(action)\n","            #print(info)\n","            if done:\n","                break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2EoASShFC3Cl","colab_type":"code","colab":{}},"source":["def initial_population():\n","    # [OBS, MOVES]\n","    training_data = []\n","    # all scores:\n","    scores = []\n","    # just the scores that met our threshold:\n","    accepted_scores = []\n","    # iterate through however many games we want:\n","    for _ in range(initial_games):\n","        score = 0\n","        # moves specifically from this environment:\n","        game_memory = []\n","        # previous observation that we saw\n","        prev_observation = []\n","        # for each frame in 200\n","        for _ in range(goal_steps):\n","            # choose random action (0 or 1)\n","            #env.render()\n","            action = random.randrange(0,2)\n","            # do it!\n","            observation, reward, done, info = env.step(action)\n","            \n","            # notice that the observation is returned FROM the action\n","            # so we'll store the previous observation here, pairing\n","            # the prev observation to the action we'll take.\n","            if len(prev_observation) > 0 :\n","                game_memory.append([prev_observation, action])\n","            prev_observation = observation\n","            score+=reward\n","            if done: break\n","\n","        # IF our score is higher than our threshold, we'd like to save\n","        # every move we made\n","        # NOTE the reinforcement methodology here. \n","        # all we're doing is reinforcing the score, we're not trying \n","        # to influence the machine in any way as to HOW that score is \n","        # reached.\n","        if score >= score_requirement:\n","            accepted_scores.append(score)\n","            for data in game_memory:\n","                # convert to one-hot (this is the output layer for our neural network)\n","                if data[1] == 1:\n","                    output = [0,1]\n","                elif data[1] == 0:\n","                    output = [1,0]\n","                    \n","                # saving our training data\n","                training_data.append([data[0], output])\n","\n","        # reset env to play again\n","        env.reset()\n","        # save overall scores\n","        scores.append(score)\n","    \n","    # just in case you wanted to reference later\n","    training_data_save = np.array(training_data)\n","    np.save('saved.npy',training_data_save)\n","\n","    # some stats here, to further illustrate the neural network magic!\n","    print('Average accepted score:',mean(accepted_scores))\n","    print('Median score for accepted scores:',median(accepted_scores))\n","    print(Counter(accepted_scores))\n","    #for i in training_data:\n","        #print(i[0])\n","        #print(\"% %\",i[1][0],i[1][1])\n","\n","    return training_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ImBXp44YC3Pk","colab_type":"code","colab":{}},"source":["# training_data = initial_population()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wI_pAhNQF-xq","colab_type":"code","colab":{}},"source":["# X = np.array([i[0] for i in training_data])\n","# y = np.array([i[1] for i in training_data])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2IChis4lFpzf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKOFwucEsHvJ","colab_type":"code","colab":{}},"source":["model = Sequential()\n","model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n","model.add(Dense(16))\n","model.add(Activation('relu'))\n","model.add(Dense(64))\n","model.add(Activation('relu'))\n","model.add(Dense(16))\n","model.add(Activation('relu'))\n","model.add(Dense(nb_actions, activation='sigmoid'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wn6suT4IOxsa","colab_type":"code","outputId":"746b8b2f-4120-4483-8dbe-a3891c90a8fb","executionInfo":{"status":"ok","timestamp":1587486026131,"user_tz":-330,"elapsed":74730,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}},"colab":{"base_uri":"https://localhost:8080/","height":425}},"source":["print(model.summary())"],"execution_count":67,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_1 (Flatten)          (None, 4)                 0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 16)                80        \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 16)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 64)                1088      \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 64)                0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 16)                1040      \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 16)                0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 2)                 34        \n","=================================================================\n","Total params: 2,242\n","Trainable params: 2,242\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7uS_afiJF6-a","colab_type":"code","colab":{}},"source":["# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n","# model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n","\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n","# print(X_train.shape,y_train.shape)\n","# print(X_test.shape,y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LpoPPkhDlI2T","colab_type":"code","colab":{}},"source":["class PlotAcc(keras.callbacks.Callback):\n","  def on_train_begin(self, logs={}):\n","    self.i = 0\n","    self.x = []\n","    self.acc = []\n","    self.val_acc = []\n","    self.loss = []\n","    self.val_loss= []\n","    \n","    self.fig = plt.figure()\n","\n","  def on_epoch_end(self, epoch, logs={}):  \n","    self.x.append(self.i)\n","    self.acc.append(logs.get('accuracy'))\n","    self.val_acc.append(logs.get('val_accuracy'))\n","    self.loss.append(logs.get('loss'))\n","    self.val_loss.append(logs.get('val_loss'))\n","    self.i += 1\n","    \n","    clear_output(wait=True)\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n","    ax1.plot(self.x, self.acc, label=\"acc\")\n","    ax1.plot(self.x, self.val_acc, label=\"val_acc\")\n","    ax2.plot(self.x, self.loss, label=\"loss\")\n","    ax2.plot(self.x, self.val_loss, label=\"val_loss\")\n","    \n","    ax1.legend()\n","    ax2.legend()\n","    plt.show();\n","\n","  def on_train_end(self, epoch, logs={}):\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n","    ax1.plot(self.x, self.acc, label=\"acc\")\n","    ax1.plot(self.x, self.val_acc, label=\"val_acc\")\n","    ax2.plot(self.x, self.loss, label=\"loss\")\n","    ax2.plot(self.x, self.val_loss, label=\"val_loss\")\n","    \n","    ax1.legend()\n","    ax2.legend()\n","    fig = plt.gcf()\n","    fig.savefig(plot_file)\n","        \n","plot_acc = PlotAcc()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uUK6_DbFpw62","colab_type":"code","colab":{}},"source":["from rl.callbacks import ModelIntervalCheckpoint, FileLogger"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cAOqyy_bq0OC","colab_type":"code","colab":{}},"source":["from keras.layers import Dense, Activation, Flatten\n","from keras.optimizers import Adam"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HKXMKgt0VGVG","colab_type":"code","colab":{}},"source":["import numpy as np\n","#import wandb\n","\n","from keras import __version__ as KERAS_VERSION\n","from keras.callbacks import Callback as KerasCallback, CallbackList as KerasCallbackList\n","from keras.utils.generic_utils import Progbar\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUFXQxhWP81J","colab_type":"code","colab":{}},"source":["class Callback(KerasCallback):\n","  def _set_env(self, env):\n","    self.env = env\n","    self.x=[]\n","    self.y=[]\n","    self.eps=0\n","    self.step=0\n","\n","  def on_episode_begin(self, episode, logs={}):\n","    self.step=0\n","\n","  def on_episode_end(self, episode, logs={}):\n","    self.eps+=1\n","    self.x.append(self.eps+1)\n","    self.y.append(self.step+1)\n","    plt.plot(self.x,self.y)\n","    plt.ylabel('#steps')\n","    plt.xlabel('episode#')\n","    fig = plt.gcf()\n","    fig.savefig(plot_file) \n","\n","  def on_step_begin(self, step, logs={}):\n","    pass\n","\n","  def on_step_end(self, step, logs={}):\n","    self.step+=1\n","\n","  def on_action_begin(self, action, logs={}):\n","    pass\n","\n","  def on_action_end(self, action, logs={}):\n","    pass\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lrjIFH9dp-fe","colab_type":"code","colab":{}},"source":["def build_callbacks(env_name):\n","    checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n","    log_filename = 'dqn_{}_log.json'.format(env_name)\n","    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=5000)]\n","    callbacks += [FileLogger(log_file, interval=1000)]\n","    callbacks += [Callback()]\n","    return callbacks"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e30FYmoSqzyk","colab_type":"code","colab":{}},"source":["callbacks = build_callbacks(\"cartpole-env1\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUafZlrFqprR","colab_type":"code","colab":{}},"source":["memory = SequentialMemory(limit=50000, window_length=1)\n","\n","policy = BoltzmannQPolicy()\n","# enable the dueling network\n","# you can specify the dueling_type to one of {'avg','max','naive'}\n","dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, policy=policy, nb_steps_warmup=30,\n","               enable_dueling_network=True, dueling_type='avg', target_model_update=1e-2)\n","\n","dqn.compile(Adam(lr=1e-3), metrics=['mae'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5HKxbE0Ix9UE","colab_type":"code","colab":{}},"source":["#history = model.fit(X_train, y_train,validation_data=(X_test, y_test), epochs=10, batch_size=128,callbacks=[plot_acc])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mo-3wvEJq8Bw","colab_type":"code","colab":{}},"source":["dqn.fit(env, nb_steps=100000, visualize=False, verbose=2, callbacks=callbacks)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k67zdedvnbSP","colab_type":"code","colab":{}},"source":["print(model.summary())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXpaNZ19rCv7","colab_type":"code","colab":{}},"source":["dqn.save_weights(model_file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sz8QwoKmnWD7","colab_type":"code","colab":{}},"source":["#model.save(model_file)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gXwguvJVSnAU","colab_type":"code","colab":{}},"source":["scores=dqn.test(env, nb_episodes=1000, nb_max_episode_steps=500, visualize=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_K8C73APsdoq","colab_type":"code","colab":{}},"source":["print('Average score over 1000 test games: {}'.format(np.mean(scores.history['episode_reward'])))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DKL44X_9sduD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RmnVvrDrqQYh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5LORJR8MqQke","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7ev8KFpqQh-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wyxaEeH0qQf_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vVeVh_JOsdz0","colab_type":"code","colab":{}},"source":["# !pip3 uninstall tensorflow"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V6nY0-wAsd6e","colab_type":"code","colab":{}},"source":["# pip install tensorflow==1.4.0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7UXrjpnexuaE","colab_type":"text"},"source":["Make sure you restart runtime\n","*and then execute from next line*\n"]},{"cell_type":"markdown","metadata":{"id":"HC56mgEFx_L_","colab_type":"text"},"source":["\\"]},{"cell_type":"markdown","metadata":{"id":"AqxF8nH1x_Sc","colab_type":"text"},"source":["Make sure you restart runtime\n","*and then execute from next line*\n"]},{"cell_type":"code","metadata":{"id":"f4ginOlisd-7","colab_type":"code","colab":{}},"source":["# import gym\n","# import tensorflow as tf\n","# #import tensorflow.compat.v1 as tf\n","# #tf.disable_v2_behavior()\n","# import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tex4E-e2sd9u","colab_type":"code","colab":{}},"source":["# env = gym.make('CartPole-v1')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yCZFGU8DxT9v","colab_type":"code","colab":{}},"source":["# !cat /usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/cartpole.py"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V_2WhYRoxVIL","colab_type":"text"},"source":["make sure it is task1 code\n"]},{"cell_type":"code","metadata":{"id":"na7VJrRTsd5L","colab_type":"code","colab":{}},"source":["# env.reset()\n","# rewards = []\n","# for _ in range(100):\n","#     #env.render()\n","#     state, reward, done, info = env.step(env.action_space.sample()) # take a random action\n","#     rewards.append(reward)\n","#     if done:\n","#         rewards = []\n","#         env.reset()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0_89l98Nsd4N","colab_type":"code","colab":{}},"source":["# print(rewards[-20:])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ugv6ugwasdyN","colab_type":"code","colab":{}},"source":["# class QNetwork:\n","#     def __init__(self, learning_rate=0.01, state_size=4, \n","#                  action_size=2, hidden_size=10, \n","#                  name='QNetwork'):\n","#         # state inputs to the Q-network\n","#         with tf.variable_scope(name):\n","#             self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n","            \n","#             # One hot encode the actions to later choose the Q-value for the action\n","#             self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n","#             one_hot_actions = tf.one_hot(self.actions_, action_size)\n","            \n","#             # Target Q values for training\n","#             self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n","            \n","#             # ReLU hidden layers\n","#             self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)\n","#             self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)\n","\n","#             # Linear output layer\n","#             self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, \n","#                                                             activation_fn=None)\n","            \n","#             ### Train with loss (targetQ - Q)^2\n","#             # output has length 2, for two actions. This next line chooses\n","#             # one value from output (per row) according to the one-hot encoded actions.\n","#             self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n","            \n","#             self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n","#             self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-kClSICs4Vc","colab_type":"code","colab":{}},"source":["# from collections import deque\n","# class Memory():\n","#     def __init__(self, max_size = 1000):\n","#         self.buffer = deque(maxlen=max_size)\n","    \n","#     def add(self, experience):\n","#         self.buffer.append(experience)\n","            \n","#     def sample(self, batch_size):\n","#         idx = np.random.choice(np.arange(len(self.buffer)), \n","#                                size=batch_size, \n","#                                replace=False)\n","#         return [self.buffer[ii] for ii in idx]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oTH3n8Ws4YL","colab_type":"code","colab":{}},"source":["\n","# train_episodes = 1000          # max number of episodes to learn from\n","# max_steps = 200                # max steps in an episode\n","# gamma = 0.99                   # future reward discount\n","\n","# # Exploration parameters\n","# explore_start = 1.0            # exploration probability at start\n","# explore_stop = 0.01            # minimum exploration probability \n","# decay_rate = 0.0001            # exponential decay rate for exploration prob\n","\n","# # Network parameters\n","# hidden_size = 64               # number of units in each Q-network hidden layer\n","# learning_rate = 0.0001         # Q-network learning rate\n","\n","# # Memory parameters\n","# memory_size = 10000            # memory capacity\n","# batch_size = 20                # experience mini-batch size\n","# pretrain_length = batch_size"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZK7tdAgGs4a3","colab_type":"code","colab":{}},"source":["# from tensorflow.python.framework import ops\n","# ops.reset_default_graph()\n","# mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CgDIEygfs4gI","colab_type":"code","colab":{}},"source":["# # Initialize the simulation\n","# env.reset()\n","# # Take one random step to get the pole and cart moving\n","# state, reward, done, _ = env.step(env.action_space.sample())\n","\n","# memory = Memory(max_size=memory_size)\n","\n","# # Make a bunch of random actions and store the experiences\n","# for ii in range(pretrain_length):\n","#     # Uncomment the line below to watch the simulation\n","#     # env.render()\n","\n","#     # Make a random action\n","#     action = env.action_space.sample()\n","#     next_state, reward, done, _ = env.step(action)\n","\n","#     if done:\n","#         # The simulation fails so no next state\n","#         next_state = np.zeros(state.shape)\n","#         # Add experience to memory\n","#         memory.add((state, action, reward, next_state))\n","        \n","#         # Start new episode\n","#         env.reset()\n","#         # Take one random step to get the pole and cart moving\n","#         state, reward, done, _ = env.step(env.action_space.sample())\n","#     else:\n","#         # Add experience to memory\n","#         memory.add((state, action, reward, next_state))\n","#         state = next_state"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iIUfDsnXs4jL","colab_type":"code","colab":{}},"source":["# # Now train with experiences\n","# saver = tf.train.Saver()\n","# rewards_list = []\n","# with tf.Session() as sess:\n","#     # Initialize variables\n","#     sess.run(tf.global_variables_initializer())\n","    \n","#     step = 0\n","#     for ep in range(1, train_episodes):\n","#         total_reward = 0\n","#         t = 0\n","#         while t < max_steps:\n","#             step += 1\n","#             # Uncomment this next line to watch the training\n","#             # env.render() \n","            \n","#             # Explore or Exploit\n","#             explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n","#             if explore_p > np.random.rand():\n","#                 # Make a random action\n","#                 action = env.action_space.sample()\n","#             else:\n","#                 # Get action from Q-network\n","#                 feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n","#                 Qs = sess.run(mainQN.output, feed_dict=feed)\n","#                 action = np.argmax(Qs)\n","            \n","#             # Take action, get new state and reward\n","#             next_state, reward, done, _ = env.step(action)\n","    \n","#             total_reward += reward\n","            \n","#             if done:\n","#                 # the episode ends so no next state\n","#                 next_state = np.zeros(state.shape)\n","#                 t = max_steps\n","                \n","#                 print('Episode: {}'.format(ep),\n","#                       'Total reward: {}'.format(total_reward),\n","#                       'Training loss: {:.4f}'.format(loss),\n","#                       'Explore P: {:.4f}'.format(explore_p))\n","#                 rewards_list.append((ep, total_reward))\n","                \n","#                 # Add experience to memory\n","#                 memory.add((state, action, reward, next_state))\n","                \n","#                 # Start new episode\n","#                 env.reset()\n","#                 # Take one random step to get the pole and cart moving\n","#                 state, reward, done, _ = env.step(env.action_space.sample())\n","\n","#             else:\n","#                 # Add experience to memory\n","#                 memory.add((state, action, reward, next_state))\n","#                 state = next_state\n","#                 t += 1\n","            \n","#             # Sample mini-batch from memory\n","#             batch = memory.sample(batch_size)\n","#             states = np.array([each[0] for each in batch])\n","#             actions = np.array([each[1] for each in batch])\n","#             rewards = np.array([each[2] for each in batch])\n","#             next_states = np.array([each[3] for each in batch])\n","            \n","#             # Train network\n","#             target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n","            \n","#             # Set target_Qs to 0 for states where episode ends\n","#             episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n","#             target_Qs[episode_ends] = (0, 0)\n","            \n","#             targets = rewards + gamma * np.max(target_Qs, axis=1)\n","\n","#             loss, _ = sess.run([mainQN.loss, mainQN.opt],\n","#                                 feed_dict={mainQN.inputs_: states,\n","#                                            mainQN.targetQs_: targets,\n","#                                            mainQN.actions_: actions})\n","        \n","#     saver.save(sess, \"checkpoints/cartpole.ckpt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0S79eZC4tWT-","colab_type":"code","colab":{}},"source":["# %matplotlib inline\n","# import matplotlib.pyplot as plt\n","\n","# def running_mean(x, N):\n","#     cumsum = np.cumsum(np.insert(x, 0, 0)) \n","#     return (cumsum[N:] - cumsum[:-N]) / N"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uHG3zFB8tWW_","colab_type":"code","colab":{}},"source":["# eps, rews = np.array(rewards_list).T\n","# smoothed_rews = running_mean(rews, 10)\n","# plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n","# plt.plot(eps, rews, color='grey', alpha=0.3)\n","# plt.xlabel('Episode')\n","# plt.ylabel('Total Reward')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YUiHzedTtWZ3","colab_type":"code","colab":{}},"source":["# test_episodes = 100\n","# test_max_steps = 500\n","# scores=[]\n","# env.reset()\n","# with tf.Session() as sess:\n","#     saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n","    \n","#     for ep in range(1, test_episodes):\n","#         t = 0\n","#         score=0\n","#         while t < test_max_steps:\n","#             #env.render() \n","            \n","#             # Get action from Q-network\n","#             feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n","#             Qs = sess.run(mainQN.output, feed_dict=feed)\n","#             action = np.argmax(Qs)\n","            \n","#             # Take action, get new state and reward\n","#             next_state, reward, done, _ = env.step(action)\n","            \n","#             score+=reward\n","               \n","#             if done:\n","#                 t = test_max_steps\n","#                 env.reset()\n","#                 # Take one random step to get the pole and cart moving\n","#                 state, reward, done, _ = env.step(env.action_space.sample())\n","\n","#             else:\n","#                 state = next_state\n","#                 t += 1\n","#         scores.append(score)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2e8YEVEgtWcr","colab_type":"code","colab":{}},"source":["# print('Average Score:',sum(scores)/len(scores))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GLX_1Hcos4eh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BVMHJ-RKsdrd","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HSd6RhchsdmQ","colab_type":"code","colab":{}},"source":["# import numpy as np\n","# import gym\n","\n","# from keras.models import Sequential\n","# from keras.layers import Dense, Activation, Flatten\n","# from keras.optimizers import Adam\n","\n","# from rl.agents.cem import CEMAgent\n","# from rl.memory import EpisodeParameterMemory\n","\n","# ENV_NAME = 'CartPole-v0'\n","\n","\n","# # Get the environment and extract the number of actions.\n","# env = gym.make(ENV_NAME)\n","# np.random.seed(123)\n","# env.seed(123)\n","\n","# nb_actions = env.action_space.n\n","# obs_dim = env.observation_space.shape[0]\n","\n","# # Option 1 : Simple model\n","# # model = Sequential()\n","# # model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n","# # model.add(Dense(nb_actions))\n","# # model.add(Activation('softmax'))\n","\n","# # Option 2: deep network\n","# model = Sequential()\n","# model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n","# model.add(Dense(16))\n","# model.add(Activation('relu'))\n","# model.add(Dense(16))\n","# model.add(Activation('relu'))\n","# model.add(Dense(16))\n","# model.add(Activation('relu'))\n","# model.add(Dense(nb_actions))\n","# model.add(Activation('softmax'))\n","\n","\n","# print(model.summary())\n","\n","\n","# # Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n","# # even the metrics!\n","# memory = EpisodeParameterMemory(limit=50000, window_length=1)\n","\n","# cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n","#                batch_size=128, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\n","# cem.compile()\n","\n","# # Okay, now it's time to learn something! We visualize the training here for show, but this\n","# # slows down training quite a lot. You can always safely abort the training prematurely using\n","# # Ctrl + C.\n","# cem.fit(env, nb_steps=100000, visualize=False, verbose=2)\n","\n","# # After training is done, we save the best weights.\n","# cem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)\n","\n","# # Finally, evaluate our algorithm for 5 episodes.\n","# scores=cem.test(env, nb_episodes=1000,nb_max_episode_steps=500,visualize=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8DVqM1l_FmK7","colab_type":"code","colab":{}},"source":["# # Finally, evaluate our algorithm for 5 episodes.\n","# print('Average score over 1000 test games: {}'.format(np.mean(scores.history['episode_reward'])))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZDRiaEeFvBy","colab_type":"code","outputId":"f979c54c-9e91-4467-cdaf-39a2f25db86b","executionInfo":{"status":"ok","timestamp":1587486210629,"user_tz":-330,"elapsed":182257,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# import numpy as np\n","# import gym\n","\n","# from keras.models import Sequential\n","# from keras.layers import Dense, Activation, Flatten\n","# from keras.optimizers import Adam\n","\n","# from rl.agents import SARSAAgent\n","# from rl.policy import BoltzmannQPolicy\n","# from rl.policy import EpsGreedyQPolicy\n","\n","\n","\n","\n","# ENV_NAME = 'CartPole-v1'\n","\n","# # Get the environment and extract the number of actions.\n","# env = gym.make(ENV_NAME)\n","# np.random.seed(123)\n","# env.seed(123)\n","# nb_actions = env.action_space.n\n","\n","# # Next, we build a very simple model.\n","# model = Sequential()\n","# model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n","# model.add(Dense(16))\n","# model.add(Activation('relu'))\n","# model.add(Dense(64))\n","# model.add(Activation('relu'))\n","# model.add(Dense(16))\n","# model.add(Activation('relu'))\n","# model.add(Dense(nb_actions))\n","# model.add(Activation('linear'))\n","# print(model.summary())\n","\n","# # SARSA does not require a memory.\n","# policy = BoltzmannQPolicy()\n","# #policy = EpsGreedyQPolicy()\n","\n","# sarsa = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=20, policy=policy)\n","\n","# sarsa.compile(Adam(lr=1e-3), metrics=['mae'])\n","\n","# # Okay, now it's time to learn something! We visualize the training here for show, but this\n","# # slows down training quite a lot. You can always safely abort the training prematurely using\n","# # Ctrl + C.\n","# sarsa.fit(env, nb_steps=50000, visualize=False, verbose=2,callbacks=callbacks)\n","\n","# # After training is done, we save the final weights.\n","# sarsa.save_weights('sarsa_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n","\n","# # Finally, evaluate our algorithm for 5 episodes.\n","# #sarsa.test(env, nb_episodes=1000, nb_max_episode_steps=500, visualize=False)"],"execution_count":76,"outputs":[{"output_type":"stream","text":["CartPoleEnv - Version 0.2.0, Noise case: 1\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_2 (Flatten)          (None, 4)                 0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 16)                80        \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 16)                0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 64)                1088      \n","_________________________________________________________________\n","activation_5 (Activation)    (None, 64)                0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 16)                1040      \n","_________________________________________________________________\n","activation_6 (Activation)    (None, 16)                0         \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 2)                 34        \n","_________________________________________________________________\n","activation_7 (Activation)    (None, 2)                 0         \n","=================================================================\n","Total params: 2,242\n","Trainable params: 2,242\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Training for 50000 steps ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"},{"output_type":"stream","text":["     9/50000: episode: 1, duration: 1.677s, episode steps: 9, steps per second: 5, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.177 [-2.217, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n","    20/50000: episode: 2, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.222 [-1.080, 2.977], loss: --, mean_absolute_error: --, mean_q: --\n","    30/50000: episode: 3, duration: 0.328s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.164 [-2.344, 1.097], loss: 0.493614, mean_absolute_error: 0.517795, mean_q: 0.117127\n","    43/50000: episode: 4, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.129 [-0.831, 1.643], loss: 0.496893, mean_absolute_error: 0.530147, mean_q: 0.077606\n","    61/50000: episode: 5, duration: 0.076s, episode steps: 18, steps per second: 236, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.153 [-2.447, 0.663], loss: 0.494711, mean_absolute_error: 0.599474, mean_q: 0.271564\n","    73/50000: episode: 6, duration: 0.083s, episode steps: 12, steps per second: 145, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.136 [-2.328, 0.907], loss: 0.521277, mean_absolute_error: 0.768477, mean_q: 0.690116\n","    88/50000: episode: 7, duration: 0.069s, episode steps: 15, steps per second: 216, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.127 [-1.147, 2.398], loss: 0.491742, mean_absolute_error: 0.703285, mean_q: 0.486472\n","    98/50000: episode: 8, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.155 [-2.645, 1.353], loss: 0.919177, mean_absolute_error: 1.247743, mean_q: 1.533380\n","   116/50000: episode: 9, duration: 0.088s, episode steps: 18, steps per second: 206, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: 0.184 [-0.508, 1.224], loss: 0.482094, mean_absolute_error: 0.834316, mean_q: 0.847636\n","   127/50000: episode: 10, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.174 [-0.799, 1.910], loss: 0.827385, mean_absolute_error: 1.323517, mean_q: 1.620965\n","   140/50000: episode: 11, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.139 [-2.529, 1.039], loss: 0.930266, mean_absolute_error: 1.611815, mean_q: 2.196862\n","   151/50000: episode: 12, duration: 0.062s, episode steps: 11, steps per second: 179, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.163 [-2.660, 1.128], loss: 1.587754, mean_absolute_error: 2.113942, mean_q: 3.005395\n","   160/50000: episode: 13, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.185 [-2.989, 1.454], loss: 3.279761, mean_absolute_error: 2.902974, mean_q: 4.248577\n","   178/50000: episode: 14, duration: 0.076s, episode steps: 18, steps per second: 236, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.154 [-2.048, 0.446], loss: 1.549835, mean_absolute_error: 2.239339, mean_q: 3.300117\n","   209/50000: episode: 15, duration: 0.102s, episode steps: 31, steps per second: 305, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.113 [-2.602, 0.923], loss: 2.158741, mean_absolute_error: 2.683800, mean_q: 4.153856\n","   225/50000: episode: 16, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.079 [-1.346, 0.884], loss: 2.344432, mean_absolute_error: 3.965122, mean_q: 6.532498\n","   234/50000: episode: 17, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.214 [-3.185, 1.461], loss: 8.884493, mean_absolute_error: 5.201552, mean_q: 7.770285\n","   242/50000: episode: 18, duration: 0.058s, episode steps: 8, steps per second: 138, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.178 [-2.488, 1.200], loss: 7.650162, mean_absolute_error: 5.197029, mean_q: 7.914315\n","   255/50000: episode: 19, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.129 [-2.165, 0.906], loss: 4.874554, mean_absolute_error: 4.349893, mean_q: 6.911616\n","   284/50000: episode: 20, duration: 0.103s, episode steps: 29, steps per second: 282, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.071 [-2.361, 0.842], loss: 2.658120, mean_absolute_error: 4.095100, mean_q: 6.892433\n","   300/50000: episode: 21, duration: 0.073s, episode steps: 16, steps per second: 218, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.055 [-2.934, 1.577], loss: 6.355041, mean_absolute_error: 5.397260, mean_q: 9.148554\n","   332/50000: episode: 22, duration: 0.106s, episode steps: 32, steps per second: 303, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.656 [0.000, 1.000], mean observation: 0.015 [-3.104, 1.853], loss: 4.373318, mean_absolute_error: 5.288936, mean_q: 9.469304\n","   346/50000: episode: 23, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.193 [-0.399, 1.957], loss: 7.894677, mean_absolute_error: 6.439637, mean_q: 11.003435\n","   363/50000: episode: 24, duration: 0.076s, episode steps: 17, steps per second: 224, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.165 [-0.431, 1.513], loss: 5.413386, mean_absolute_error: 6.241513, mean_q: 10.972339\n","   374/50000: episode: 25, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.207 [-2.345, 0.739], loss: 9.150392, mean_absolute_error: 6.732551, mean_q: 11.145295\n","   392/50000: episode: 26, duration: 0.203s, episode steps: 18, steps per second: 89, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.224 [-0.364, 2.227], loss: 4.631822, mean_absolute_error: 6.182546, mean_q: 10.914303\n","   404/50000: episode: 27, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.190 [-2.432, 1.041], loss: 9.253203, mean_absolute_error: 6.744880, mean_q: 11.450645\n","   418/50000: episode: 28, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.148 [-2.769, 1.201], loss: 7.971508, mean_absolute_error: 6.848678, mean_q: 11.836690\n","   432/50000: episode: 29, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.125 [-2.506, 0.968], loss: 6.203800, mean_absolute_error: 6.426045, mean_q: 11.348741\n","   441/50000: episode: 30, duration: 0.063s, episode steps: 9, steps per second: 143, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.158 [-2.657, 1.565], loss: 13.698847, mean_absolute_error: 7.244445, mean_q: 12.549944\n","   450/50000: episode: 31, duration: 0.063s, episode steps: 9, steps per second: 142, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.191 [-2.993, 1.391], loss: 10.088544, mean_absolute_error: 6.513358, mean_q: 11.296733\n","   464/50000: episode: 32, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.116 [-2.547, 1.174], loss: 4.811277, mean_absolute_error: 5.988666, mean_q: 11.013597\n","   479/50000: episode: 33, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.098 [-2.267, 1.052], loss: 4.342513, mean_absolute_error: 5.890416, mean_q: 10.640095\n","   507/50000: episode: 34, duration: 0.102s, episode steps: 28, steps per second: 274, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.607 [0.000, 1.000], mean observation: 0.022 [-1.963, 1.278], loss: 2.815306, mean_absolute_error: 6.196379, mean_q: 11.594642\n","   519/50000: episode: 35, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.111 [-2.357, 1.260], loss: 5.160139, mean_absolute_error: 6.449251, mean_q: 11.940589\n","   533/50000: episode: 36, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.158 [-3.264, 1.578], loss: 6.256361, mean_absolute_error: 6.381027, mean_q: 11.779970\n","   542/50000: episode: 37, duration: 0.065s, episode steps: 9, steps per second: 138, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.219 [-3.220, 1.463], loss: 8.068691, mean_absolute_error: 6.244941, mean_q: 11.333768\n","   570/50000: episode: 38, duration: 0.103s, episode steps: 28, steps per second: 271, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: 0.104 [-3.321, 2.481], loss: 4.788221, mean_absolute_error: 6.975724, mean_q: 12.945134\n","   581/50000: episode: 39, duration: 0.074s, episode steps: 11, steps per second: 150, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.193 [-2.298, 0.750], loss: 4.212238, mean_absolute_error: 5.925511, mean_q: 10.790753\n","   636/50000: episode: 40, duration: 0.157s, episode steps: 55, steps per second: 351, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.249 [-0.668, 1.421], loss: 4.030278, mean_absolute_error: 7.455890, mean_q: 13.749961\n","   646/50000: episode: 41, duration: 0.070s, episode steps: 10, steps per second: 144, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.198 [-0.595, 1.484], loss: 20.948857, mean_absolute_error: 10.732820, mean_q: 18.036974\n","   663/50000: episode: 42, duration: 0.078s, episode steps: 17, steps per second: 218, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.117 [-0.723, 1.735], loss: 17.257120, mean_absolute_error: 9.541067, mean_q: 16.334586\n","   678/50000: episode: 43, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.165 [-1.403, 3.191], loss: 22.410928, mean_absolute_error: 9.300296, mean_q: 16.011579\n","   692/50000: episode: 44, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.141 [-1.537, 3.105], loss: 22.566078, mean_absolute_error: 9.701659, mean_q: 16.896040\n","   702/50000: episode: 45, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.183 [-1.182, 2.613], loss: 23.944835, mean_absolute_error: 9.675029, mean_q: 16.337530\n","   714/50000: episode: 46, duration: 0.178s, episode steps: 12, steps per second: 67, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.237 [-0.937, 2.917], loss: 17.078156, mean_absolute_error: 9.052515, mean_q: 15.598593\n","   727/50000: episode: 47, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.183 [-0.701, 2.209], loss: 11.479733, mean_absolute_error: 8.407077, mean_q: 14.726401\n","   739/50000: episode: 48, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.108 [-1.252, 2.565], loss: 16.321406, mean_absolute_error: 8.759739, mean_q: 14.828318\n","   750/50000: episode: 49, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.177 [-1.097, 2.678], loss: 14.895897, mean_absolute_error: 8.761053, mean_q: 15.125338\n","   780/50000: episode: 50, duration: 0.105s, episode steps: 30, steps per second: 284, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.071 [-1.831, 2.702], loss: 6.020358, mean_absolute_error: 7.468650, mean_q: 13.821652\n","   794/50000: episode: 51, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.123 [-0.791, 1.816], loss: 10.767688, mean_absolute_error: 8.345636, mean_q: 14.872857\n","   805/50000: episode: 52, duration: 0.079s, episode steps: 11, steps per second: 139, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.193 [-1.920, 0.598], loss: 8.148519, mean_absolute_error: 7.565862, mean_q: 13.405969\n","   818/50000: episode: 53, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.105 [-1.204, 2.254], loss: 11.870843, mean_absolute_error: 8.416442, mean_q: 14.980665\n","   831/50000: episode: 54, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.176 [-1.146, 3.047], loss: 12.008906, mean_absolute_error: 8.256913, mean_q: 14.450091\n","   847/50000: episode: 55, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.169 [-0.410, 1.906], loss: 8.030942, mean_absolute_error: 7.815206, mean_q: 13.875067\n","   868/50000: episode: 56, duration: 0.093s, episode steps: 21, steps per second: 226, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.218 [-2.007, 0.470], loss: 4.052470, mean_absolute_error: 6.941179, mean_q: 12.530090\n","   878/50000: episode: 57, duration: 0.072s, episode steps: 10, steps per second: 139, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.167 [-0.892, 2.183], loss: 13.287943, mean_absolute_error: 8.489826, mean_q: 14.320824\n","   892/50000: episode: 58, duration: 0.092s, episode steps: 14, steps per second: 152, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.145 [-1.647, 0.637], loss: 5.465376, mean_absolute_error: 7.432303, mean_q: 13.417759\n","   904/50000: episode: 59, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.179 [-1.340, 3.224], loss: 12.841199, mean_absolute_error: 8.130828, mean_q: 14.022311\n","   922/50000: episode: 60, duration: 0.087s, episode steps: 18, steps per second: 207, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.107 [-2.318, 0.853], loss: 4.830721, mean_absolute_error: 7.237987, mean_q: 13.125784\n","   933/50000: episode: 61, duration: 0.085s, episode steps: 11, steps per second: 129, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.152 [-1.074, 2.458], loss: 10.683696, mean_absolute_error: 8.060861, mean_q: 14.028521\n","   945/50000: episode: 62, duration: 0.078s, episode steps: 12, steps per second: 155, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.180 [-1.543, 0.626], loss: 7.633578, mean_absolute_error: 7.497402, mean_q: 13.207934\n","   963/50000: episode: 63, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.056 [-1.010, 1.908], loss: 6.199181, mean_absolute_error: 7.445929, mean_q: 13.266269\n","   980/50000: episode: 64, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.063 [-1.107, 2.182], loss: 6.839839, mean_absolute_error: 7.588589, mean_q: 13.361966\n","   999/50000: episode: 65, duration: 0.091s, episode steps: 19, steps per second: 210, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.217 [-2.155, 0.281], loss: 4.189647, mean_absolute_error: 6.817966, mean_q: 12.225922\n","  1011/50000: episode: 66, duration: 0.217s, episode steps: 12, steps per second: 55, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.131 [-1.134, 2.084], loss: 11.227244, mean_absolute_error: 8.129929, mean_q: 13.831024\n","  1020/50000: episode: 67, duration: 0.076s, episode steps: 9, steps per second: 118, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.204 [-1.740, 0.808], loss: 9.621045, mean_absolute_error: 7.685049, mean_q: 13.156585\n","  1056/50000: episode: 68, duration: 0.133s, episode steps: 36, steps per second: 270, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.101 [-2.714, 0.685], loss: 2.179488, mean_absolute_error: 6.763736, mean_q: 12.440395\n","  1079/50000: episode: 69, duration: 0.103s, episode steps: 23, steps per second: 224, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.106 [-0.836, 2.489], loss: 5.725826, mean_absolute_error: 7.733564, mean_q: 13.935731\n","  1096/50000: episode: 70, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.098 [-1.177, 2.516], loss: 7.692816, mean_absolute_error: 7.851672, mean_q: 14.088895\n","  1108/50000: episode: 71, duration: 0.091s, episode steps: 12, steps per second: 132, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: 0.165 [-0.502, 1.209], loss: 8.641393, mean_absolute_error: 8.017547, mean_q: 14.149430\n","  1140/50000: episode: 72, duration: 0.120s, episode steps: 32, steps per second: 266, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.173 [-0.760, 2.745], loss: 4.453715, mean_absolute_error: 7.910730, mean_q: 14.651971\n","  1154/50000: episode: 73, duration: 0.094s, episode steps: 14, steps per second: 149, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.163 [-2.019, 0.564], loss: 7.447086, mean_absolute_error: 8.082125, mean_q: 14.441117\n","  1169/50000: episode: 74, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.183 [-0.672, 1.994], loss: 8.192657, mean_absolute_error: 8.210337, mean_q: 14.517762\n","  1190/50000: episode: 75, duration: 0.102s, episode steps: 21, steps per second: 207, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.140 [-0.750, 2.413], loss: 5.256376, mean_absolute_error: 7.918951, mean_q: 14.330911\n","  1211/50000: episode: 76, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.131 [-1.050, 2.735], loss: 4.860331, mean_absolute_error: 7.839682, mean_q: 14.259963\n","  1225/50000: episode: 77, duration: 0.088s, episode steps: 14, steps per second: 159, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.183 [-2.337, 0.631], loss: 7.919487, mean_absolute_error: 8.230449, mean_q: 14.766151\n","  1242/50000: episode: 78, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.215 [-0.456, 2.516], loss: 7.033369, mean_absolute_error: 8.350882, mean_q: 15.205227\n","  1266/50000: episode: 79, duration: 0.112s, episode steps: 24, steps per second: 215, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.048 [-2.095, 1.046], loss: 5.668784, mean_absolute_error: 8.367961, mean_q: 15.508728\n","  1277/50000: episode: 80, duration: 0.084s, episode steps: 11, steps per second: 131, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.144 [-2.356, 1.091], loss: 11.247841, mean_absolute_error: 8.751646, mean_q: 15.465635\n","  1289/50000: episode: 81, duration: 0.085s, episode steps: 12, steps per second: 140, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.120 [-2.095, 1.172], loss: 8.168070, mean_absolute_error: 8.117034, mean_q: 14.614540\n","  1301/50000: episode: 82, duration: 0.091s, episode steps: 12, steps per second: 132, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.154 [-2.459, 0.959], loss: 6.509025, mean_absolute_error: 7.526731, mean_q: 13.706843\n","  1325/50000: episode: 83, duration: 0.111s, episode steps: 24, steps per second: 217, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: 0.087 [-0.899, 1.160], loss: 5.245448, mean_absolute_error: 7.648939, mean_q: 14.028838\n","  1342/50000: episode: 84, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.112 [-2.595, 1.117], loss: 3.803800, mean_absolute_error: 7.008578, mean_q: 12.977135\n","  1357/50000: episode: 85, duration: 0.099s, episode steps: 15, steps per second: 151, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.080 [-2.999, 1.694], loss: 3.843543, mean_absolute_error: 7.007083, mean_q: 12.846998\n","  1374/50000: episode: 86, duration: 0.222s, episode steps: 17, steps per second: 77, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.154 [-0.571, 2.169], loss: 6.737465, mean_absolute_error: 7.961048, mean_q: 14.178328\n","  1386/50000: episode: 87, duration: 0.092s, episode steps: 12, steps per second: 130, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.152 [-1.949, 0.840], loss: 4.010736, mean_absolute_error: 6.619431, mean_q: 12.023425\n","  1399/50000: episode: 88, duration: 0.090s, episode steps: 13, steps per second: 145, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.184 [-3.334, 1.429], loss: 2.591488, mean_absolute_error: 6.333604, mean_q: 11.673974\n","  1426/50000: episode: 89, duration: 0.122s, episode steps: 27, steps per second: 222, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.370 [0.000, 1.000], mean observation: -0.042 [-1.607, 2.145], loss: 4.468176, mean_absolute_error: 7.165232, mean_q: 12.907211\n","  1438/50000: episode: 90, duration: 0.090s, episode steps: 12, steps per second: 134, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.140 [-1.333, 2.742], loss: 9.440443, mean_absolute_error: 8.036001, mean_q: 14.130420\n","  1504/50000: episode: 91, duration: 0.203s, episode steps: 66, steps per second: 325, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.227 [0.000, 1.000], mean observation: -0.530 [-5.605, 3.629], loss: 12.595175, mean_absolute_error: 8.888090, mean_q: 17.694737\n","  1515/50000: episode: 92, duration: 0.085s, episode steps: 11, steps per second: 129, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-2.088, 3.556], loss: 15.160864, mean_absolute_error: 8.825143, mean_q: 17.845604\n","  1526/50000: episode: 93, duration: 0.086s, episode steps: 11, steps per second: 127, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.130 [-1.717, 3.055], loss: 10.173045, mean_absolute_error: 8.016859, mean_q: 15.646953\n","  1536/50000: episode: 94, duration: 0.087s, episode steps: 10, steps per second: 115, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.695, 2.769], loss: 9.143603, mean_absolute_error: 7.422119, mean_q: 14.622569\n","  1543/50000: episode: 95, duration: 0.079s, episode steps: 7, steps per second: 89, episode reward: 7.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [-1.373, 2.969], loss: 11.964882, mean_absolute_error: 7.550357, mean_q: 14.267203\n","  1554/50000: episode: 96, duration: 0.088s, episode steps: 11, steps per second: 125, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.139 [-1.336, 2.703], loss: 5.252367, mean_absolute_error: 7.186693, mean_q: 13.797569\n","  1566/50000: episode: 97, duration: 0.099s, episode steps: 12, steps per second: 121, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.091 [-1.880, 3.153], loss: 5.099230, mean_absolute_error: 6.645283, mean_q: 12.847734\n","  1575/50000: episode: 98, duration: 0.089s, episode steps: 9, steps per second: 101, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.195 [-1.386, 2.983], loss: 6.562699, mean_absolute_error: 7.023535, mean_q: 13.311991\n","  1584/50000: episode: 99, duration: 0.084s, episode steps: 9, steps per second: 107, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.172 [-0.963, 1.911], loss: 7.221721, mean_absolute_error: 6.739734, mean_q: 12.399222\n","  1598/50000: episode: 100, duration: 0.095s, episode steps: 14, steps per second: 148, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.108 [-1.300, 2.842], loss: 3.794513, mean_absolute_error: 6.444929, mean_q: 12.064984\n","  1609/50000: episode: 101, duration: 0.089s, episode steps: 11, steps per second: 124, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.135 [-1.720, 3.146], loss: 3.653766, mean_absolute_error: 6.000821, mean_q: 11.372157\n","  1621/50000: episode: 102, duration: 0.095s, episode steps: 12, steps per second: 126, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.107 [-1.687, 2.728], loss: 3.253458, mean_absolute_error: 6.061655, mean_q: 11.391114\n","  1635/50000: episode: 103, duration: 0.102s, episode steps: 14, steps per second: 137, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.161 [-2.244, 0.682], loss: 5.184927, mean_absolute_error: 7.280210, mean_q: 13.229894\n","  1660/50000: episode: 104, duration: 0.123s, episode steps: 25, steps per second: 204, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.014 [-1.800, 0.877], loss: 3.024219, mean_absolute_error: 7.392975, mean_q: 13.613250\n","  1681/50000: episode: 105, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.148 [-1.741, 0.459], loss: 3.065143, mean_absolute_error: 7.162612, mean_q: 13.208706\n","  1701/50000: episode: 106, duration: 0.221s, episode steps: 20, steps per second: 91, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: -0.174 [-1.007, 0.463], loss: 4.067709, mean_absolute_error: 7.175028, mean_q: 13.076532\n","  1713/50000: episode: 107, duration: 0.096s, episode steps: 12, steps per second: 125, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.133 [-2.233, 1.048], loss: 5.045269, mean_absolute_error: 7.322699, mean_q: 13.216080\n","  1730/50000: episode: 108, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: -0.154 [-1.478, 0.612], loss: 3.382169, mean_absolute_error: 6.892494, mean_q: 12.596721\n","  1748/50000: episode: 109, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.184 [-0.326, 1.597], loss: 4.098637, mean_absolute_error: 7.582492, mean_q: 13.872721\n","  1769/50000: episode: 110, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.035 [-1.218, 1.532], loss: 3.018582, mean_absolute_error: 6.773118, mean_q: 12.495495\n","  1782/50000: episode: 111, duration: 0.107s, episode steps: 13, steps per second: 121, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.141 [-0.754, 1.771], loss: 5.350761, mean_absolute_error: 7.001658, mean_q: 12.488174\n","  1803/50000: episode: 112, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.146 [-2.123, 0.458], loss: 2.934598, mean_absolute_error: 7.256687, mean_q: 13.564885\n","  1819/50000: episode: 113, duration: 0.107s, episode steps: 16, steps per second: 150, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.150 [-0.789, 1.834], loss: 4.126260, mean_absolute_error: 7.215951, mean_q: 13.077005\n","  1837/50000: episode: 114, duration: 0.108s, episode steps: 18, steps per second: 166, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.185 [-0.244, 1.821], loss: 4.703227, mean_absolute_error: 7.880815, mean_q: 14.252535\n","  1867/50000: episode: 115, duration: 0.141s, episode steps: 30, steps per second: 213, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: 0.197 [-0.486, 1.846], loss: 3.335270, mean_absolute_error: 8.057294, mean_q: 15.200933\n","  1886/50000: episode: 116, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.016 [-2.243, 1.288], loss: 3.739704, mean_absolute_error: 7.812679, mean_q: 14.789114\n","  1899/50000: episode: 117, duration: 0.103s, episode steps: 13, steps per second: 126, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.173 [-1.844, 0.547], loss: 5.754218, mean_absolute_error: 7.778209, mean_q: 13.986233\n","  1916/50000: episode: 118, duration: 0.109s, episode steps: 17, steps per second: 157, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.178 [-1.851, 0.349], loss: 2.752221, mean_absolute_error: 7.329475, mean_q: 13.583361\n","  1925/50000: episode: 119, duration: 0.102s, episode steps: 9, steps per second: 88, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.204 [-2.521, 1.218], loss: 3.597288, mean_absolute_error: 6.603780, mean_q: 12.218692\n","  1946/50000: episode: 120, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.187 [-2.343, 0.630], loss: 2.043882, mean_absolute_error: 6.922289, mean_q: 12.664552\n","  1962/50000: episode: 121, duration: 0.116s, episode steps: 16, steps per second: 138, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.103 [-0.783, 1.197], loss: 5.642365, mean_absolute_error: 7.275727, mean_q: 13.196833\n","  1985/50000: episode: 122, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.019 [-2.405, 1.036], loss: 1.441727, mean_absolute_error: 6.476608, mean_q: 12.083394\n","  1999/50000: episode: 123, duration: 0.107s, episode steps: 14, steps per second: 131, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.191 [-1.839, 0.493], loss: 1.748317, mean_absolute_error: 6.670655, mean_q: 12.183274\n","  2010/50000: episode: 124, duration: 0.105s, episode steps: 11, steps per second: 105, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.171 [-0.740, 2.263], loss: 6.983853, mean_absolute_error: 7.454842, mean_q: 13.224973\n","  2026/50000: episode: 125, duration: 0.118s, episode steps: 16, steps per second: 136, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.139 [-2.050, 0.857], loss: 1.203966, mean_absolute_error: 6.224581, mean_q: 11.411669\n","  2037/50000: episode: 126, duration: 0.227s, episode steps: 11, steps per second: 49, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.219 [-2.031, 0.594], loss: 1.700092, mean_absolute_error: 5.817944, mean_q: 10.453451\n","  2050/50000: episode: 127, duration: 0.112s, episode steps: 13, steps per second: 116, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.158 [-2.327, 0.926], loss: 0.803286, mean_absolute_error: 5.671232, mean_q: 10.627307\n","  2059/50000: episode: 128, duration: 0.106s, episode steps: 9, steps per second: 85, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.191 [-1.156, 2.459], loss: 4.975244, mean_absolute_error: 6.869606, mean_q: 12.405546\n","  2069/50000: episode: 129, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.193 [-1.068, 2.431], loss: 4.212853, mean_absolute_error: 6.635465, mean_q: 11.971694\n","  2083/50000: episode: 130, duration: 0.112s, episode steps: 14, steps per second: 125, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.173 [-0.954, 2.596], loss: 3.532260, mean_absolute_error: 6.914774, mean_q: 12.541927\n","  2098/50000: episode: 131, duration: 0.115s, episode steps: 15, steps per second: 131, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.150 [-2.352, 0.742], loss: 1.634955, mean_absolute_error: 5.843287, mean_q: 10.571545\n","  2113/50000: episode: 132, duration: 0.120s, episode steps: 15, steps per second: 125, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.168 [-1.087, 2.787], loss: 2.605193, mean_absolute_error: 6.776248, mean_q: 12.402273\n","  2123/50000: episode: 133, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.185 [-0.808, 1.894], loss: 3.739851, mean_absolute_error: 6.218029, mean_q: 11.252881\n","  2135/50000: episode: 134, duration: 0.111s, episode steps: 12, steps per second: 108, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.151 [-1.861, 0.859], loss: 1.027546, mean_absolute_error: 5.385528, mean_q: 9.864600\n","  2152/50000: episode: 135, duration: 0.119s, episode steps: 17, steps per second: 143, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: -0.189 [-1.489, 0.361], loss: 1.580773, mean_absolute_error: 5.794749, mean_q: 10.471717\n","  2163/50000: episode: 136, duration: 0.110s, episode steps: 11, steps per second: 100, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.139 [-0.813, 1.461], loss: 3.635958, mean_absolute_error: 6.187397, mean_q: 11.197559\n","  2180/50000: episode: 137, duration: 0.125s, episode steps: 17, steps per second: 136, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: -0.186 [-1.082, 0.387], loss: 2.308548, mean_absolute_error: 5.701764, mean_q: 10.533616\n","  2197/50000: episode: 138, duration: 0.133s, episode steps: 17, steps per second: 128, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.116 [-0.722, 1.288], loss: 3.442401, mean_absolute_error: 6.397562, mean_q: 11.437756\n","  2213/50000: episode: 139, duration: 0.119s, episode steps: 16, steps per second: 135, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.099 [-1.007, 1.769], loss: 2.086722, mean_absolute_error: 6.119887, mean_q: 11.425644\n","  2229/50000: episode: 140, duration: 0.124s, episode steps: 16, steps per second: 129, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.036 [-1.279, 2.034], loss: 1.773511, mean_absolute_error: 5.788625, mean_q: 10.804428\n","  2242/50000: episode: 141, duration: 0.123s, episode steps: 13, steps per second: 105, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.162 [-0.789, 1.380], loss: 5.800554, mean_absolute_error: 6.483753, mean_q: 11.271483\n","  2256/50000: episode: 142, duration: 0.120s, episode steps: 14, steps per second: 117, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.208 [-2.043, 0.408], loss: 1.238847, mean_absolute_error: 6.080156, mean_q: 11.441227\n","  2264/50000: episode: 143, duration: 0.105s, episode steps: 8, steps per second: 76, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.224 [-0.919, 1.888], loss: 3.516658, mean_absolute_error: 5.726019, mean_q: 10.204360\n","  2281/50000: episode: 144, duration: 0.126s, episode steps: 17, steps per second: 135, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.147 [-0.527, 1.700], loss: 2.885919, mean_absolute_error: 6.544354, mean_q: 11.631051\n","  2304/50000: episode: 145, duration: 0.142s, episode steps: 23, steps per second: 162, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.081 [-2.053, 0.600], loss: 1.414155, mean_absolute_error: 6.099797, mean_q: 11.614540\n","  2363/50000: episode: 146, duration: 0.340s, episode steps: 59, steps per second: 173, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: -0.370 [-2.060, 0.579], loss: 3.085028, mean_absolute_error: 7.472770, mean_q: 13.948096\n","  2380/50000: episode: 147, duration: 0.129s, episode steps: 17, steps per second: 131, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.174 [-1.689, 0.538], loss: 1.650336, mean_absolute_error: 6.824664, mean_q: 12.779814\n","  2402/50000: episode: 148, duration: 0.144s, episode steps: 22, steps per second: 153, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.112 [-1.541, 0.540], loss: 1.651782, mean_absolute_error: 6.749466, mean_q: 12.387098\n","  2427/50000: episode: 149, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.320 [0.000, 1.000], mean observation: -0.212 [-1.185, 0.271], loss: 3.570601, mean_absolute_error: 7.318227, mean_q: 13.523751\n","  2453/50000: episode: 150, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: 0.114 [-0.900, 1.213], loss: 7.101550, mean_absolute_error: 8.581304, mean_q: 15.643944\n","  2467/50000: episode: 151, duration: 0.124s, episode steps: 14, steps per second: 113, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.130 [-2.245, 1.021], loss: 1.970944, mean_absolute_error: 6.843693, mean_q: 12.382746\n","  2479/50000: episode: 152, duration: 0.124s, episode steps: 12, steps per second: 97, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.164 [-1.885, 0.881], loss: 2.025125, mean_absolute_error: 5.938987, mean_q: 11.222994\n","  2493/50000: episode: 153, duration: 0.126s, episode steps: 14, steps per second: 111, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: -0.154 [-1.119, 0.743], loss: 2.986627, mean_absolute_error: 5.959642, mean_q: 11.031885\n","  2545/50000: episode: 154, duration: 0.204s, episode steps: 52, steps per second: 255, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.009 [-1.566, 1.791], loss: 1.825645, mean_absolute_error: 7.478209, mean_q: 14.119219\n","  2570/50000: episode: 155, duration: 0.153s, episode steps: 25, steps per second: 164, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.128 [-0.370, 1.660], loss: 4.321378, mean_absolute_error: 9.117244, mean_q: 16.914939\n","  2596/50000: episode: 156, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.346 [0.000, 1.000], mean observation: -0.238 [-1.324, 0.255], loss: 3.665976, mean_absolute_error: 8.403509, mean_q: 16.165869\n","  2606/50000: episode: 157, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.195 [-0.938, 2.515], loss: 7.664108, mean_absolute_error: 8.738904, mean_q: 15.332136\n","  2630/50000: episode: 158, duration: 0.158s, episode steps: 24, steps per second: 151, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: -0.148 [-1.228, 0.419], loss: 2.460041, mean_absolute_error: 8.077641, mean_q: 15.382855\n","  2664/50000: episode: 159, duration: 0.166s, episode steps: 34, steps per second: 205, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.164 [-0.499, 1.624], loss: 6.771737, mean_absolute_error: 9.624281, mean_q: 17.978093\n","  2675/50000: episode: 160, duration: 0.131s, episode steps: 11, steps per second: 84, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.162 [-1.750, 0.587], loss: 5.024626, mean_absolute_error: 8.296892, mean_q: 15.264513\n","  2699/50000: episode: 161, duration: 0.164s, episode steps: 24, steps per second: 147, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: 0.077 [-1.483, 1.492], loss: 2.521574, mean_absolute_error: 7.995304, mean_q: 15.576557\n","  2718/50000: episode: 162, duration: 0.138s, episode steps: 19, steps per second: 137, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.015 [-1.683, 1.121], loss: 1.938565, mean_absolute_error: 7.420925, mean_q: 14.216107\n","  2738/50000: episode: 163, duration: 0.140s, episode steps: 20, steps per second: 143, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: -0.228 [-1.242, 0.533], loss: 3.132132, mean_absolute_error: 7.296148, mean_q: 13.337441\n","  2755/50000: episode: 164, duration: 0.148s, episode steps: 17, steps per second: 115, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: 0.230 [-0.378, 0.911], loss: 10.376208, mean_absolute_error: 9.456098, mean_q: 17.175058\n","  2773/50000: episode: 165, duration: 0.139s, episode steps: 18, steps per second: 130, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.147 [-0.568, 1.605], loss: 5.373771, mean_absolute_error: 8.569767, mean_q: 15.948623\n","  2787/50000: episode: 166, duration: 0.253s, episode steps: 14, steps per second: 55, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.164 [-0.719, 2.231], loss: 4.836051, mean_absolute_error: 8.110949, mean_q: 14.993782\n","  2810/50000: episode: 167, duration: 0.156s, episode steps: 23, steps per second: 147, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: 0.105 [-0.769, 1.118], loss: 6.585722, mean_absolute_error: 8.077584, mean_q: 14.656011\n","  2827/50000: episode: 168, duration: 0.143s, episode steps: 17, steps per second: 119, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.147 [-0.597, 1.886], loss: 4.593367, mean_absolute_error: 7.900140, mean_q: 14.569935\n","  2859/50000: episode: 169, duration: 0.167s, episode steps: 32, steps per second: 192, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: -0.227 [-1.223, 0.434], loss: 2.723322, mean_absolute_error: 7.730378, mean_q: 14.758036\n","  2878/50000: episode: 170, duration: 0.151s, episode steps: 19, steps per second: 126, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.126 [-0.880, 1.581], loss: 5.621046, mean_absolute_error: 8.109108, mean_q: 14.776329\n","  2904/50000: episode: 171, duration: 0.165s, episode steps: 26, steps per second: 158, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.117 [-1.524, 0.425], loss: 2.129482, mean_absolute_error: 7.619471, mean_q: 14.685706\n","  2926/50000: episode: 172, duration: 0.150s, episode steps: 22, steps per second: 146, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.176 [-0.521, 1.861], loss: 4.336704, mean_absolute_error: 8.782856, mean_q: 16.312284\n","  2946/50000: episode: 173, duration: 0.148s, episode steps: 20, steps per second: 135, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: -0.219 [-1.364, 0.148], loss: 2.742611, mean_absolute_error: 7.960219, mean_q: 14.639837\n","  2974/50000: episode: 174, duration: 0.171s, episode steps: 28, steps per second: 164, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: 0.219 [-0.350, 1.434], loss: 5.288946, mean_absolute_error: 9.386079, mean_q: 17.789287\n","  2999/50000: episode: 175, duration: 0.156s, episode steps: 25, steps per second: 160, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: 0.202 [-0.416, 1.175], loss: 8.393839, mean_absolute_error: 9.539606, mean_q: 17.818477\n","  3017/50000: episode: 176, duration: 0.148s, episode steps: 18, steps per second: 121, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: -0.192 [-1.262, 0.378], loss: 2.935089, mean_absolute_error: 7.949916, mean_q: 15.147470\n","  3059/50000: episode: 177, duration: 0.198s, episode steps: 42, steps per second: 212, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: -0.228 [-1.240, 0.437], loss: 2.752648, mean_absolute_error: 8.606803, mean_q: 16.694644\n","  3071/50000: episode: 178, duration: 0.135s, episode steps: 12, steps per second: 89, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: 0.182 [-0.766, 1.516], loss: 12.232586, mean_absolute_error: 8.901720, mean_q: 15.595922\n","  3089/50000: episode: 179, duration: 0.145s, episode steps: 18, steps per second: 124, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: 0.189 [-0.342, 1.370], loss: 5.962977, mean_absolute_error: 8.972553, mean_q: 16.783394\n","  3101/50000: episode: 180, duration: 0.148s, episode steps: 12, steps per second: 81, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: -0.183 [-1.189, 0.423], loss: 3.982938, mean_absolute_error: 7.227461, mean_q: 13.406246\n","  3131/50000: episode: 181, duration: 0.170s, episode steps: 30, steps per second: 177, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-2.108, 0.423], loss: 1.212676, mean_absolute_error: 7.911479, mean_q: 15.411567\n","  3147/50000: episode: 182, duration: 0.146s, episode steps: 16, steps per second: 109, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.094 [-1.009, 1.812], loss: 3.450809, mean_absolute_error: 8.034941, mean_q: 14.930505\n","  3178/50000: episode: 183, duration: 0.180s, episode steps: 31, steps per second: 172, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.323 [0.000, 1.000], mean observation: -0.271 [-1.456, 0.298], loss: 3.280861, mean_absolute_error: 8.187240, mean_q: 15.701359\n","  3195/50000: episode: 184, duration: 0.223s, episode steps: 17, steps per second: 76, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.043 [-1.311, 1.806], loss: 3.323503, mean_absolute_error: 7.811266, mean_q: 14.531396\n","  3210/50000: episode: 185, duration: 0.142s, episode steps: 15, steps per second: 106, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: 0.219 [-0.727, 1.229], loss: 7.252064, mean_absolute_error: 8.332125, mean_q: 14.932904\n","  3242/50000: episode: 186, duration: 0.253s, episode steps: 32, steps per second: 127, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.594 [0.000, 1.000], mean observation: 0.124 [-0.575, 0.993], loss: 4.631396, mean_absolute_error: 8.949210, mean_q: 16.740588\n","  3277/50000: episode: 187, duration: 0.185s, episode steps: 35, steps per second: 190, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.257 [0.000, 1.000], mean observation: -0.373 [-2.375, 0.213], loss: 5.795642, mean_absolute_error: 9.161218, mean_q: 17.367147\n","  3290/50000: episode: 188, duration: 0.147s, episode steps: 13, steps per second: 88, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.141 [-0.852, 2.202], loss: 4.976458, mean_absolute_error: 8.582668, mean_q: 15.275641\n","  3305/50000: episode: 189, duration: 0.148s, episode steps: 15, steps per second: 101, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.171 [-0.629, 1.983], loss: 4.162687, mean_absolute_error: 8.300872, mean_q: 15.192385\n","  3318/50000: episode: 190, duration: 0.157s, episode steps: 13, steps per second: 83, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.154 [-1.069, 2.697], loss: 2.431595, mean_absolute_error: 7.052422, mean_q: 12.811309\n","  3335/50000: episode: 191, duration: 0.149s, episode steps: 17, steps per second: 114, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: 0.175 [-0.459, 1.228], loss: 6.602979, mean_absolute_error: 8.026692, mean_q: 14.278358\n","  3364/50000: episode: 192, duration: 0.184s, episode steps: 29, steps per second: 158, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.345 [0.000, 1.000], mean observation: -0.172 [-1.150, 0.240], loss: 3.128438, mean_absolute_error: 8.083073, mean_q: 15.863155\n","  3378/50000: episode: 193, duration: 0.148s, episode steps: 14, steps per second: 95, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: -0.179 [-1.101, 0.381], loss: 5.032200, mean_absolute_error: 7.698094, mean_q: 15.062116\n","  3406/50000: episode: 194, duration: 0.182s, episode steps: 28, steps per second: 154, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: -0.294 [-1.563, 0.535], loss: 2.780509, mean_absolute_error: 7.886517, mean_q: 15.285458\n","  3432/50000: episode: 195, duration: 0.178s, episode steps: 26, steps per second: 146, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: -0.254 [-1.318, 0.269], loss: 3.073943, mean_absolute_error: 7.861336, mean_q: 15.528197\n","  3449/50000: episode: 196, duration: 0.159s, episode steps: 17, steps per second: 107, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: 0.224 [-0.210, 1.783], loss: 6.428902, mean_absolute_error: 8.906320, mean_q: 15.520069\n","  3463/50000: episode: 197, duration: 0.149s, episode steps: 14, steps per second: 94, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: 0.176 [-0.765, 1.157], loss: 6.355330, mean_absolute_error: 6.961389, mean_q: 12.703455\n","  3521/50000: episode: 198, duration: 0.243s, episode steps: 58, steps per second: 239, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.184 [-1.078, 1.550], loss: 1.642388, mean_absolute_error: 9.289710, mean_q: 18.172468\n","  3547/50000: episode: 199, duration: 0.171s, episode steps: 26, steps per second: 152, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: -0.121 [-1.320, 0.720], loss: 2.190725, mean_absolute_error: 8.815877, mean_q: 17.268839\n","  3572/50000: episode: 200, duration: 0.178s, episode steps: 25, steps per second: 140, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.158 [-1.674, 0.454], loss: 2.500404, mean_absolute_error: 8.610773, mean_q: 16.833273\n","  3600/50000: episode: 201, duration: 0.180s, episode steps: 28, steps per second: 156, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.679 [0.000, 1.000], mean observation: 0.279 [-0.364, 1.305], loss: 9.154769, mean_absolute_error: 11.187609, mean_q: 21.015129\n","  3616/50000: episode: 202, duration: 0.171s, episode steps: 16, steps per second: 93, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.144 [-1.435, 0.690], loss: 2.823416, mean_absolute_error: 8.340489, mean_q: 16.356218\n","  3653/50000: episode: 203, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.351 [0.000, 1.000], mean observation: -0.211 [-1.436, 0.355], loss: 1.863812, mean_absolute_error: 8.325493, mean_q: 16.636132\n","  3704/50000: episode: 204, duration: 0.229s, episode steps: 51, steps per second: 223, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: 0.015 [-1.159, 0.848], loss: 1.287509, mean_absolute_error: 9.888811, mean_q: 19.479014\n","  3716/50000: episode: 205, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: 0.172 [-0.607, 1.133], loss: 9.245092, mean_absolute_error: 8.723052, mean_q: 15.935580\n","  3739/50000: episode: 206, duration: 0.251s, episode steps: 23, steps per second: 91, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: 0.227 [-0.261, 1.483], loss: 11.204778, mean_absolute_error: 10.834882, mean_q: 20.479598\n","  3761/50000: episode: 207, duration: 0.174s, episode steps: 22, steps per second: 127, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.318 [0.000, 1.000], mean observation: -0.155 [-1.095, 0.541], loss: 2.865879, mean_absolute_error: 8.216098, mean_q: 16.016827\n","  3771/50000: episode: 208, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.166 [-1.753, 0.783], loss: 4.502983, mean_absolute_error: 7.434566, mean_q: 14.723068\n","  3785/50000: episode: 209, duration: 0.174s, episode steps: 14, steps per second: 80, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.129 [-1.360, 0.662], loss: 3.279681, mean_absolute_error: 7.321722, mean_q: 14.127857\n","  3817/50000: episode: 210, duration: 0.193s, episode steps: 32, steps per second: 166, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.010 [-1.241, 1.269], loss: 1.863104, mean_absolute_error: 8.106038, mean_q: 16.098274\n","  3896/50000: episode: 211, duration: 0.290s, episode steps: 79, steps per second: 273, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.405 [0.000, 1.000], mean observation: -0.346 [-2.250, 0.544], loss: 3.505904, mean_absolute_error: 8.955406, mean_q: 17.658364\n","  3915/50000: episode: 212, duration: 0.162s, episode steps: 19, steps per second: 117, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: 0.179 [-0.260, 1.261], loss: 9.394700, mean_absolute_error: 10.952027, mean_q: 21.084051\n","  3951/50000: episode: 213, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.245 [-1.552, 0.310], loss: 1.701963, mean_absolute_error: 8.819492, mean_q: 17.527892\n","  3988/50000: episode: 214, duration: 0.193s, episode steps: 37, steps per second: 192, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.757 [0.000, 1.000], mean observation: 0.370 [-0.333, 2.645], loss: 20.442744, mean_absolute_error: 13.537384, mean_q: 26.082722\n","  4009/50000: episode: 215, duration: 0.167s, episode steps: 21, steps per second: 126, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.153 [-1.012, 0.397], loss: 4.099317, mean_absolute_error: 9.422198, mean_q: 18.779722\n","  4032/50000: episode: 216, duration: 0.174s, episode steps: 23, steps per second: 132, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: 0.193 [-0.206, 1.309], loss: 6.414400, mean_absolute_error: 10.857002, mean_q: 20.848401\n","  4079/50000: episode: 217, duration: 0.219s, episode steps: 47, steps per second: 215, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.340 [0.000, 1.000], mean observation: -0.295 [-2.074, 0.354], loss: 2.016228, mean_absolute_error: 9.074920, mean_q: 17.934133\n","  4099/50000: episode: 218, duration: 0.162s, episode steps: 20, steps per second: 123, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.034 [-0.992, 1.617], loss: 3.973130, mean_absolute_error: 9.324752, mean_q: 17.716687\n","  4150/50000: episode: 219, duration: 0.230s, episode steps: 51, steps per second: 222, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.373 [0.000, 1.000], mean observation: -0.291 [-1.703, 0.365], loss: 2.976996, mean_absolute_error: 9.989571, mean_q: 20.009924\n","  4193/50000: episode: 220, duration: 0.209s, episode steps: 43, steps per second: 206, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.051 [-2.033, 0.575], loss: 1.652420, mean_absolute_error: 11.767348, mean_q: 23.079104\n","  4282/50000: episode: 221, duration: 0.307s, episode steps: 89, steps per second: 290, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.427 [0.000, 1.000], mean observation: -0.245 [-2.042, 0.464], loss: 1.994015, mean_absolute_error: 12.208054, mean_q: 24.474930\n","  4299/50000: episode: 222, duration: 0.152s, episode steps: 17, steps per second: 112, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: 0.137 [-0.588, 1.277], loss: 17.569530, mean_absolute_error: 14.104568, mean_q: 25.233765\n","  4320/50000: episode: 223, duration: 0.167s, episode steps: 21, steps per second: 126, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: 0.167 [-0.512, 1.185], loss: 13.761181, mean_absolute_error: 12.982810, mean_q: 24.342027\n","  4405/50000: episode: 224, duration: 0.300s, episode steps: 85, steps per second: 283, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.318 [0.000, 1.000], mean observation: -0.582 [-4.393, 0.501], loss: 11.954536, mean_absolute_error: 12.898259, mean_q: 25.753078\n","  4460/50000: episode: 225, duration: 0.240s, episode steps: 55, steps per second: 229, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.054 [-0.924, 1.171], loss: 2.604374, mean_absolute_error: 13.825654, mean_q: 27.202950\n","  4526/50000: episode: 226, duration: 0.389s, episode steps: 66, steps per second: 170, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: -0.207 [-2.406, 0.666], loss: 1.667014, mean_absolute_error: 11.708813, mean_q: 23.435642\n","  4597/50000: episode: 227, duration: 0.275s, episode steps: 71, steps per second: 259, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.121 [-1.279, 2.063], loss: 4.474379, mean_absolute_error: 16.240779, mean_q: 32.213257\n","  4615/50000: episode: 228, duration: 0.159s, episode steps: 18, steps per second: 113, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: -0.227 [-1.381, 0.563], loss: 4.960706, mean_absolute_error: 9.971705, mean_q: 20.004387\n","  4641/50000: episode: 229, duration: 0.186s, episode steps: 26, steps per second: 140, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: -0.308 [-1.866, 0.422], loss: 2.841843, mean_absolute_error: 9.174369, mean_q: 17.870074\n","  4668/50000: episode: 230, duration: 0.183s, episode steps: 27, steps per second: 148, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.263 [-1.175, 0.292], loss: 2.604011, mean_absolute_error: 9.993787, mean_q: 19.651466\n","  4699/50000: episode: 231, duration: 0.201s, episode steps: 31, steps per second: 154, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.613 [0.000, 1.000], mean observation: 0.077 [-0.572, 0.942], loss: 15.504838, mean_absolute_error: 13.640805, mean_q: 26.366896\n","  4725/50000: episode: 232, duration: 0.183s, episode steps: 26, steps per second: 142, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: 0.284 [-0.277, 1.305], loss: 14.512316, mean_absolute_error: 14.731562, mean_q: 28.328191\n","  4767/50000: episode: 233, duration: 0.216s, episode steps: 42, steps per second: 194, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: -0.401 [-2.521, 0.247], loss: 1.658118, mean_absolute_error: 9.259077, mean_q: 18.970552\n","  4778/50000: episode: 234, duration: 0.157s, episode steps: 11, steps per second: 70, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.181 [-0.682, 1.773], loss: 8.817756, mean_absolute_error: 11.089780, mean_q: 20.688222\n","  4813/50000: episode: 235, duration: 0.200s, episode steps: 35, steps per second: 175, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.021 [-0.714, 1.136], loss: 7.446759, mean_absolute_error: 12.096924, mean_q: 23.343556\n","  4831/50000: episode: 236, duration: 0.169s, episode steps: 18, steps per second: 107, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: 0.220 [-0.612, 0.978], loss: 15.633102, mean_absolute_error: 12.150777, mean_q: 23.019985\n","  4864/50000: episode: 237, duration: 0.197s, episode steps: 33, steps per second: 167, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.697 [0.000, 1.000], mean observation: 0.309 [-0.330, 1.723], loss: 9.335474, mean_absolute_error: 13.405410, mean_q: 26.155266\n","  4995/50000: episode: 238, duration: 0.387s, episode steps: 131, steps per second: 338, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.427 [0.000, 1.000], mean observation: -0.115 [-2.693, 0.805], loss: 2.063080, mean_absolute_error: 12.970114, mean_q: 26.129799\n","  5077/50000: episode: 239, duration: 0.307s, episode steps: 82, steps per second: 267, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.354 [0.000, 1.000], mean observation: -0.384 [-3.388, 0.470], loss: 4.287871, mean_absolute_error: 13.364081, mean_q: 27.168689\n","  5156/50000: episode: 240, duration: 0.296s, episode steps: 79, steps per second: 267, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.392 [0.000, 1.000], mean observation: -0.075 [-2.390, 0.961], loss: 2.400949, mean_absolute_error: 15.483793, mean_q: 30.932093\n","  5235/50000: episode: 241, duration: 0.296s, episode steps: 79, steps per second: 267, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.392 [0.000, 1.000], mean observation: -0.491 [-2.325, 0.556], loss: 7.796863, mean_absolute_error: 16.225262, mean_q: 32.690219\n","  5300/50000: episode: 242, duration: 0.255s, episode steps: 65, steps per second: 255, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.277 [0.000, 1.000], mean observation: -0.687 [-4.239, 0.414], loss: 5.332864, mean_absolute_error: 12.844798, mean_q: 25.247876\n","  5356/50000: episode: 243, duration: 0.229s, episode steps: 56, steps per second: 244, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.411 [0.000, 1.000], mean observation: -0.033 [-1.310, 0.886], loss: 2.767859, mean_absolute_error: 17.418631, mean_q: 34.692220\n","  5401/50000: episode: 244, duration: 0.214s, episode steps: 45, steps per second: 211, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.238 [-2.099, 0.326], loss: 1.744085, mean_absolute_error: 13.896814, mean_q: 28.270090\n","  5447/50000: episode: 245, duration: 0.222s, episode steps: 46, steps per second: 207, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.783 [0.000, 1.000], mean observation: 0.502 [-2.045, 4.361], loss: 25.576240, mean_absolute_error: 25.147998, mean_q: 49.985047\n","  5485/50000: episode: 246, duration: 0.208s, episode steps: 38, steps per second: 183, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.342 [0.000, 1.000], mean observation: -0.238 [-1.843, 0.486], loss: 8.242281, mean_absolute_error: 10.714003, mean_q: 19.873121\n","  5604/50000: episode: 247, duration: 0.371s, episode steps: 119, steps per second: 321, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.580 [0.000, 1.000], mean observation: 0.372 [-0.803, 2.834], loss: 6.749889, mean_absolute_error: 18.455667, mean_q: 36.669451\n","  5622/50000: episode: 248, duration: 0.165s, episode steps: 18, steps per second: 109, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: 0.209 [-0.234, 1.627], loss: 23.578405, mean_absolute_error: 19.166786, mean_q: 36.143044\n","  5659/50000: episode: 249, duration: 0.206s, episode steps: 37, steps per second: 179, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.568 [0.000, 1.000], mean observation: 0.062 [-0.710, 1.278], loss: 11.574757, mean_absolute_error: 17.453506, mean_q: 33.754022\n","  5768/50000: episode: 250, duration: 0.341s, episode steps: 109, steps per second: 320, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.138 [-1.212, 3.306], loss: 1.336215, mean_absolute_error: 17.427033, mean_q: 35.428626\n","  5814/50000: episode: 251, duration: 0.217s, episode steps: 46, steps per second: 212, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.674 [0.000, 1.000], mean observation: 0.369 [-0.519, 2.190], loss: 17.656898, mean_absolute_error: 19.285337, mean_q: 38.001340\n","  5861/50000: episode: 252, duration: 0.222s, episode steps: 47, steps per second: 212, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: -0.091 [-0.968, 0.976], loss: 13.565351, mean_absolute_error: 17.892305, mean_q: 35.102051\n","  5887/50000: episode: 253, duration: 0.180s, episode steps: 26, steps per second: 144, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: 0.262 [-0.214, 1.407], loss: 22.978899, mean_absolute_error: 18.259403, mean_q: 34.576974\n","  5969/50000: episode: 254, duration: 0.310s, episode steps: 82, steps per second: 264, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.561 [0.000, 1.000], mean observation: 0.541 [-0.690, 2.239], loss: 4.401257, mean_absolute_error: 14.922178, mean_q: 30.025569\n","  6036/50000: episode: 255, duration: 0.294s, episode steps: 67, steps per second: 228, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.642 [0.000, 1.000], mean observation: 0.299 [-0.310, 2.596], loss: 9.302153, mean_absolute_error: 18.625721, mean_q: 37.159037\n","  6090/50000: episode: 256, duration: 0.240s, episode steps: 54, steps per second: 225, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.124 [-1.196, 0.773], loss: 12.960933, mean_absolute_error: 18.224409, mean_q: 35.771035\n","  6112/50000: episode: 257, duration: 0.177s, episode steps: 22, steps per second: 124, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.682 [0.000, 1.000], mean observation: 0.278 [-0.170, 1.406], loss: 14.245934, mean_absolute_error: 17.156735, mean_q: 32.856321\n","  6211/50000: episode: 258, duration: 0.328s, episode steps: 99, steps per second: 302, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.576 [0.000, 1.000], mean observation: 0.378 [-0.468, 1.991], loss: 2.176779, mean_absolute_error: 15.561194, mean_q: 31.194295\n","  6313/50000: episode: 259, duration: 0.334s, episode steps: 102, steps per second: 306, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.151 [-1.240, 1.112], loss: 6.234625, mean_absolute_error: 20.361015, mean_q: 40.680775\n","  6421/50000: episode: 260, duration: 0.351s, episode steps: 108, steps per second: 307, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: -0.387 [-3.385, 0.530], loss: 11.469809, mean_absolute_error: 22.480174, mean_q: 45.320888\n","  6441/50000: episode: 261, duration: 0.176s, episode steps: 20, steps per second: 113, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: 0.236 [-0.393, 1.333], loss: 29.143407, mean_absolute_error: 19.091901, mean_q: 37.132180\n","  6524/50000: episode: 262, duration: 0.305s, episode steps: 83, steps per second: 272, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.590 [0.000, 1.000], mean observation: 0.569 [-0.724, 2.915], loss: 4.876984, mean_absolute_error: 17.135719, mean_q: 33.980615\n","  6597/50000: episode: 263, duration: 0.294s, episode steps: 73, steps per second: 248, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.411 [0.000, 1.000], mean observation: -0.541 [-3.429, 0.984], loss: 21.918904, mean_absolute_error: 18.042834, mean_q: 34.865533\n","  6616/50000: episode: 264, duration: 0.181s, episode steps: 19, steps per second: 105, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.065 [-0.932, 1.372], loss: 16.129315, mean_absolute_error: 17.048849, mean_q: 31.728600\n","  6674/50000: episode: 265, duration: 0.256s, episode steps: 58, steps per second: 226, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.828 [0.000, 1.000], mean observation: 0.790 [-0.258, 5.342], loss: 11.206596, mean_absolute_error: 17.655294, mean_q: 34.764838\n","  6800/50000: episode: 266, duration: 0.403s, episode steps: 126, steps per second: 313, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.405 [0.000, 1.000], mean observation: -0.388 [-3.345, 0.531], loss: 6.616854, mean_absolute_error: 18.939970, mean_q: 38.627974\n","  6907/50000: episode: 267, duration: 0.354s, episode steps: 107, steps per second: 302, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.383 [0.000, 1.000], mean observation: -0.524 [-3.586, 0.551], loss: 9.393492, mean_absolute_error: 19.894153, mean_q: 40.358251\n","  6947/50000: episode: 268, duration: 0.225s, episode steps: 40, steps per second: 177, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.825 [0.000, 1.000], mean observation: 0.473 [-0.230, 3.668], loss: 23.664734, mean_absolute_error: 19.372948, mean_q: 38.476627\n","  7054/50000: episode: 269, duration: 0.355s, episode steps: 107, steps per second: 302, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.207 [-1.531, 1.101], loss: 3.446091, mean_absolute_error: 18.730349, mean_q: 37.592382\n","  7129/50000: episode: 270, duration: 0.298s, episode steps: 75, steps per second: 252, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.680 [0.000, 1.000], mean observation: 0.565 [-0.548, 3.721], loss: 4.415908, mean_absolute_error: 17.885510, mean_q: 35.971951\n","  7222/50000: episode: 271, duration: 0.352s, episode steps: 93, steps per second: 264, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.101 [-1.844, 2.308], loss: 9.112556, mean_absolute_error: 20.785875, mean_q: 41.734594\n","  7301/50000: episode: 272, duration: 0.320s, episode steps: 79, steps per second: 247, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.620 [0.000, 1.000], mean observation: 0.302 [-0.430, 2.581], loss: 4.100147, mean_absolute_error: 18.694047, mean_q: 37.689999\n","  7369/50000: episode: 273, duration: 0.291s, episode steps: 68, steps per second: 234, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.676 [0.000, 1.000], mean observation: 0.461 [-0.541, 3.279], loss: 4.125279, mean_absolute_error: 17.674332, mean_q: 35.548962\n","  7486/50000: episode: 274, duration: 0.381s, episode steps: 117, steps per second: 307, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: 0.337 [-0.697, 2.935], loss: 2.228461, mean_absolute_error: 18.822530, mean_q: 37.839535\n","  7563/50000: episode: 275, duration: 0.313s, episode steps: 77, steps per second: 246, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.597 [0.000, 1.000], mean observation: 0.214 [-0.480, 2.068], loss: 6.124027, mean_absolute_error: 20.693925, mean_q: 41.612627\n","  7629/50000: episode: 276, duration: 0.286s, episode steps: 66, steps per second: 231, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.682 [0.000, 1.000], mean observation: 0.583 [-0.530, 3.495], loss: 1.546413, mean_absolute_error: 16.772630, mean_q: 33.672752\n","  7760/50000: episode: 277, duration: 0.410s, episode steps: 131, steps per second: 320, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.263 [-0.919, 1.907], loss: 6.040225, mean_absolute_error: 20.366767, mean_q: 40.794099\n","  7791/50000: episode: 278, duration: 0.230s, episode steps: 31, steps per second: 135, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.774 [0.000, 1.000], mean observation: 0.348 [-0.205, 2.411], loss: 16.689430, mean_absolute_error: 20.961096, mean_q: 41.569274\n","  7878/50000: episode: 279, duration: 0.343s, episode steps: 87, steps per second: 254, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.598 [0.000, 1.000], mean observation: 0.558 [-0.666, 2.465], loss: 1.221593, mean_absolute_error: 15.854730, mean_q: 31.745507\n","  8012/50000: episode: 280, duration: 0.413s, episode steps: 134, steps per second: 324, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.087 [-1.094, 1.696], loss: 6.708805, mean_absolute_error: 23.126038, mean_q: 46.747449\n","  8129/50000: episode: 281, duration: 0.383s, episode steps: 117, steps per second: 305, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.121 [-1.345, 1.772], loss: 3.020383, mean_absolute_error: 23.464936, mean_q: 47.525851\n","  8314/50000: episode: 282, duration: 0.519s, episode steps: 185, steps per second: 356, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.313 [-2.437, 0.477], loss: 42.515280, mean_absolute_error: 35.684190, mean_q: 71.955887\n","  8398/50000: episode: 283, duration: 0.326s, episode steps: 84, steps per second: 258, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.573 [-2.832, 0.592], loss: 42.653278, mean_absolute_error: 33.338908, mean_q: 66.885121\n","  8426/50000: episode: 284, duration: 0.195s, episode steps: 28, steps per second: 143, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: 0.151 [-0.379, 1.050], loss: 20.819730, mean_absolute_error: 23.276483, mean_q: 43.248486\n","  8672/50000: episode: 285, duration: 0.614s, episode steps: 246, steps per second: 400, episode reward: 246.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.286 [-0.774, 2.691], loss: 1.445395, mean_absolute_error: 22.136496, mean_q: 44.861124\n","  8782/50000: episode: 286, duration: 0.360s, episode steps: 110, steps per second: 306, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.382 [0.000, 1.000], mean observation: -0.475 [-3.817, 0.694], loss: 31.113826, mean_absolute_error: 31.356948, mean_q: 64.138700\n","  8861/50000: episode: 287, duration: 0.304s, episode steps: 79, steps per second: 260, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.418 [0.000, 1.000], mean observation: -0.587 [-3.270, 0.646], loss: 24.665237, mean_absolute_error: 27.156151, mean_q: 55.209010\n","  9000/50000: episode: 288, duration: 0.421s, episode steps: 139, steps per second: 330, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.400 [-2.415, 0.453], loss: 10.333468, mean_absolute_error: 25.930918, mean_q: 52.639944\n","  9100/50000: episode: 289, duration: 0.345s, episode steps: 100, steps per second: 290, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.390 [0.000, 1.000], mean observation: -0.552 [-3.143, 0.560], loss: 12.535762, mean_absolute_error: 24.684455, mean_q: 50.599940\n","  9195/50000: episode: 290, duration: 0.324s, episode steps: 95, steps per second: 293, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: -0.576 [-3.033, 0.656], loss: 9.861052, mean_absolute_error: 23.119684, mean_q: 47.281606\n","  9302/50000: episode: 291, duration: 0.351s, episode steps: 107, steps per second: 305, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.393 [0.000, 1.000], mean observation: -0.504 [-3.289, 0.532], loss: 5.653939, mean_absolute_error: 21.919688, mean_q: 45.319099\n","  9390/50000: episode: 292, duration: 0.322s, episode steps: 88, steps per second: 273, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: -0.609 [-3.235, 0.793], loss: 5.029920, mean_absolute_error: 20.000465, mean_q: 41.506544\n","  9498/50000: episode: 293, duration: 0.359s, episode steps: 108, steps per second: 301, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.398 [0.000, 1.000], mean observation: -0.483 [-3.322, 0.600], loss: 2.846593, mean_absolute_error: 20.755846, mean_q: 42.787559\n","  9562/50000: episode: 294, duration: 0.317s, episode steps: 64, steps per second: 202, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.271 [-2.442, 0.981], loss: 8.267505, mean_absolute_error: 20.103124, mean_q: 42.812794\n","  9651/50000: episode: 295, duration: 0.329s, episode steps: 89, steps per second: 270, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.382 [0.000, 1.000], mean observation: -0.598 [-2.955, 0.682], loss: 1.913882, mean_absolute_error: 17.965341, mean_q: 37.487428\n","  9745/50000: episode: 296, duration: 0.337s, episode steps: 94, steps per second: 279, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.372 [0.000, 1.000], mean observation: -0.585 [-3.496, 0.668], loss: 1.320194, mean_absolute_error: 17.624054, mean_q: 36.911305\n","  9853/50000: episode: 297, duration: 0.364s, episode steps: 108, steps per second: 297, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.380 [0.000, 1.000], mean observation: -0.504 [-3.727, 0.709], loss: 0.804839, mean_absolute_error: 18.974595, mean_q: 39.486012\n","  9948/50000: episode: 298, duration: 0.339s, episode steps: 95, steps per second: 280, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.379 [0.000, 1.000], mean observation: -0.553 [-3.601, 0.547], loss: 1.246300, mean_absolute_error: 18.735163, mean_q: 38.548439\n"," 10047/50000: episode: 299, duration: 0.349s, episode steps: 99, steps per second: 283, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.384 [0.000, 1.000], mean observation: -0.544 [-3.265, 0.575], loss: 0.835111, mean_absolute_error: 18.760738, mean_q: 38.805913\n"," 10147/50000: episode: 300, duration: 0.351s, episode steps: 100, steps per second: 285, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.390 [0.000, 1.000], mean observation: -0.525 [-3.490, 0.751], loss: 0.878591, mean_absolute_error: 19.283461, mean_q: 39.501878\n"," 10249/50000: episode: 301, duration: 0.359s, episode steps: 102, steps per second: 284, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.392 [0.000, 1.000], mean observation: -0.553 [-3.017, 0.800], loss: 0.781827, mean_absolute_error: 18.844918, mean_q: 38.945810\n"," 10323/50000: episode: 302, duration: 0.314s, episode steps: 74, steps per second: 235, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: -0.483 [-2.246, 0.677], loss: 4.491542, mean_absolute_error: 20.113304, mean_q: 41.790576\n"," 10418/50000: episode: 303, duration: 0.344s, episode steps: 95, steps per second: 276, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.545 [-2.078, 0.540], loss: 1.096411, mean_absolute_error: 17.198758, mean_q: 35.402612\n"," 10533/50000: episode: 304, duration: 0.384s, episode steps: 115, steps per second: 299, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.521 [-2.431, 0.600], loss: 0.982332, mean_absolute_error: 18.174982, mean_q: 37.116550\n"," 10627/50000: episode: 305, duration: 0.344s, episode steps: 94, steps per second: 273, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.224 [-2.629, 1.263], loss: 8.564705, mean_absolute_error: 24.661766, mean_q: 50.386785\n"," 10747/50000: episode: 306, duration: 0.399s, episode steps: 120, steps per second: 300, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.358 [0.000, 1.000], mean observation: -0.427 [-4.935, 0.879], loss: 1.258682, mean_absolute_error: 18.728202, mean_q: 38.786003\n"," 10916/50000: episode: 307, duration: 0.492s, episode steps: 169, steps per second: 343, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.383 [-2.405, 0.849], loss: 1.526659, mean_absolute_error: 21.492253, mean_q: 43.691298\n"," 11001/50000: episode: 308, duration: 0.346s, episode steps: 85, steps per second: 246, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.072 [-1.551, 0.855], loss: 10.822839, mean_absolute_error: 27.852078, mean_q: 56.185295\n"," 11182/50000: episode: 309, duration: 0.538s, episode steps: 181, steps per second: 337, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.348 [-2.429, 0.839], loss: 0.880028, mean_absolute_error: 22.773598, mean_q: 46.369603\n"," 11310/50000: episode: 310, duration: 0.403s, episode steps: 128, steps per second: 318, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.501 [-2.429, 0.536], loss: 1.122571, mean_absolute_error: 21.483758, mean_q: 43.861245\n"," 11448/50000: episode: 311, duration: 0.427s, episode steps: 138, steps per second: 323, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.383 [-1.963, 0.849], loss: 1.204076, mean_absolute_error: 24.103089, mean_q: 49.198778\n"," 11597/50000: episode: 312, duration: 0.443s, episode steps: 149, steps per second: 337, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: -0.428 [-3.686, 1.118], loss: 1.231057, mean_absolute_error: 22.301120, mean_q: 45.584516\n"," 11737/50000: episode: 313, duration: 0.425s, episode steps: 140, steps per second: 330, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: -0.195 [-2.071, 3.292], loss: 31.703760, mean_absolute_error: 29.599671, mean_q: 59.981237\n"," 11784/50000: episode: 314, duration: 0.247s, episode steps: 47, steps per second: 190, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.194 [-2.143, 1.219], loss: 31.234293, mean_absolute_error: 28.680434, mean_q: 56.848874\n"," 12063/50000: episode: 315, duration: 0.693s, episode steps: 279, steps per second: 402, episode reward: 279.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.095 [-1.764, 1.320], loss: 1.929956, mean_absolute_error: 28.191451, mean_q: 57.267437\n"," 12295/50000: episode: 316, duration: 0.592s, episode steps: 232, steps per second: 392, episode reward: 232.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.011 [-1.630, 1.322], loss: 2.830757, mean_absolute_error: 32.995753, mean_q: 67.111358\n"," 12727/50000: episode: 317, duration: 0.974s, episode steps: 432, steps per second: 443, episode reward: 432.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.216 [-1.612, 1.369], loss: 3.690077, mean_absolute_error: 30.846997, mean_q: 62.596178\n"," 12813/50000: episode: 318, duration: 0.320s, episode steps: 86, steps per second: 269, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.410 [-0.953, 2.423], loss: 45.457615, mean_absolute_error: 39.770803, mean_q: 79.958525\n"," 12839/50000: episode: 319, duration: 0.190s, episode steps: 26, steps per second: 137, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.731 [0.000, 1.000], mean observation: 0.269 [-0.367, 1.649], loss: 42.773897, mean_absolute_error: 32.109258, mean_q: 62.933944\n"," 12862/50000: episode: 320, duration: 0.188s, episode steps: 23, steps per second: 122, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: 0.065 [-0.677, 1.108], loss: 31.871279, mean_absolute_error: 28.133431, mean_q: 54.561786\n"," 12997/50000: episode: 321, duration: 0.411s, episode steps: 135, steps per second: 328, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.390 [-0.962, 2.598], loss: 13.103256, mean_absolute_error: 28.198716, mean_q: 56.241405\n"," 13014/50000: episode: 322, duration: 0.173s, episode steps: 17, steps per second: 98, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: -0.158 [-0.924, 0.603], loss: 11.820156, mean_absolute_error: 14.556062, mean_q: 27.704296\n"," 13057/50000: episode: 323, duration: 0.231s, episode steps: 43, steps per second: 186, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.302 [0.000, 1.000], mean observation: -0.220 [-2.428, 0.372], loss: 16.803525, mean_absolute_error: 15.394291, mean_q: 28.112547\n"," 13148/50000: episode: 324, duration: 0.335s, episode steps: 91, steps per second: 272, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: 0.224 [-2.434, 2.117], loss: 14.518171, mean_absolute_error: 22.217823, mean_q: 45.062813\n"," 13307/50000: episode: 325, duration: 0.454s, episode steps: 159, steps per second: 350, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.359 [-1.071, 2.118], loss: 5.239601, mean_absolute_error: 24.739577, mean_q: 50.135415\n"," 13447/50000: episode: 326, duration: 0.418s, episode steps: 140, steps per second: 335, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.089 [-1.817, 2.491], loss: 10.062883, mean_absolute_error: 21.762621, mean_q: 43.064139\n"," 13550/50000: episode: 327, duration: 0.344s, episode steps: 103, steps per second: 299, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.631 [0.000, 1.000], mean observation: 0.579 [-0.683, 3.910], loss: 5.307576, mean_absolute_error: 22.955780, mean_q: 47.068096\n"," 13663/50000: episode: 328, duration: 0.366s, episode steps: 113, steps per second: 309, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.154 [-0.822, 1.827], loss: 9.885975, mean_absolute_error: 24.505106, mean_q: 49.504410\n"," 13751/50000: episode: 329, duration: 0.336s, episode steps: 88, steps per second: 262, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.349 [-0.883, 2.278], loss: 13.473610, mean_absolute_error: 23.319433, mean_q: 47.030378\n"," 13991/50000: episode: 330, duration: 0.612s, episode steps: 240, steps per second: 392, episode reward: 240.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.308 [-0.864, 2.402], loss: 6.372713, mean_absolute_error: 25.798242, mean_q: 52.517300\n"," 14121/50000: episode: 331, duration: 0.400s, episode steps: 130, steps per second: 325, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.124 [-1.031, 1.849], loss: 7.845150, mean_absolute_error: 26.763771, mean_q: 53.741800\n"," 14270/50000: episode: 332, duration: 0.449s, episode steps: 149, steps per second: 332, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.584 [0.000, 1.000], mean observation: 0.341 [-0.776, 3.653], loss: 2.510819, mean_absolute_error: 23.955067, mean_q: 48.245237\n"," 14447/50000: episode: 333, duration: 0.496s, episode steps: 177, steps per second: 357, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.135 [-1.293, 2.115], loss: 3.951946, mean_absolute_error: 28.653747, mean_q: 58.308992\n"," 14587/50000: episode: 334, duration: 0.416s, episode steps: 140, steps per second: 337, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.352 [-1.165, 3.265], loss: 7.253345, mean_absolute_error: 24.541729, mean_q: 49.157225\n"," 14659/50000: episode: 335, duration: 0.304s, episode steps: 72, steps per second: 237, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: 0.066 [-1.731, 3.083], loss: 11.650567, mean_absolute_error: 27.784965, mean_q: 56.187345\n"," 14789/50000: episode: 336, duration: 0.410s, episode steps: 130, steps per second: 317, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.546 [0.000, 1.000], mean observation: 0.320 [-0.609, 1.605], loss: 4.192363, mean_absolute_error: 25.306970, mean_q: 51.513531\n"," 14847/50000: episode: 337, duration: 0.275s, episode steps: 58, steps per second: 211, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: 0.186 [-1.613, 1.882], loss: 22.583688, mean_absolute_error: 25.094795, mean_q: 51.049292\n"," 14862/50000: episode: 338, duration: 0.207s, episode steps: 15, steps per second: 73, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: 0.234 [-0.243, 0.896], loss: 13.760561, mean_absolute_error: 18.629972, mean_q: 35.964108\n"," 15000/50000: episode: 339, duration: 0.438s, episode steps: 138, steps per second: 315, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.021 [-1.850, 1.415], loss: 4.839231, mean_absolute_error: 22.845974, mean_q: 47.263673\n"," 15104/50000: episode: 340, duration: 0.356s, episode steps: 104, steps per second: 292, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.037 [-1.798, 1.764], loss: 8.613464, mean_absolute_error: 22.225552, mean_q: 46.035438\n"," 15386/50000: episode: 341, duration: 0.713s, episode steps: 282, steps per second: 396, episode reward: 282.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.399 [-2.408, 0.541], loss: 11.516026, mean_absolute_error: 26.026516, mean_q: 53.764781\n"," 15662/50000: episode: 342, duration: 0.719s, episode steps: 276, steps per second: 384, episode reward: 276.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.425 [-2.869, 1.402], loss: 6.894992, mean_absolute_error: 28.783611, mean_q: 59.696660\n"," 15960/50000: episode: 343, duration: 0.729s, episode steps: 298, steps per second: 409, episode reward: 298.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.399 [-2.726, 0.941], loss: 8.566084, mean_absolute_error: 30.168564, mean_q: 62.060591\n"," 16143/50000: episode: 344, duration: 0.528s, episode steps: 183, steps per second: 346, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.436 [-2.414, 0.465], loss: 8.445717, mean_absolute_error: 29.817837, mean_q: 61.015864\n"," 16298/50000: episode: 345, duration: 0.467s, episode steps: 155, steps per second: 332, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.433 [-2.405, 0.478], loss: 5.799749, mean_absolute_error: 27.138475, mean_q: 56.013944\n"," 16483/50000: episode: 346, duration: 0.538s, episode steps: 185, steps per second: 344, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.392 [-2.421, 0.678], loss: 4.880212, mean_absolute_error: 26.833401, mean_q: 54.881842\n"," 16652/50000: episode: 347, duration: 0.485s, episode steps: 169, steps per second: 348, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.395 [-2.412, 0.685], loss: 3.303006, mean_absolute_error: 26.730170, mean_q: 55.058821\n"," 16804/50000: episode: 348, duration: 0.457s, episode steps: 152, steps per second: 333, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.434 [-2.407, 0.579], loss: 1.666434, mean_absolute_error: 25.064215, mean_q: 51.512481\n"," 16951/50000: episode: 349, duration: 0.448s, episode steps: 147, steps per second: 328, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.450 [-2.404, 0.594], loss: 2.226658, mean_absolute_error: 23.675384, mean_q: 48.967971\n"," 17088/50000: episode: 350, duration: 0.442s, episode steps: 137, steps per second: 310, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.457 [-2.427, 0.797], loss: 1.712567, mean_absolute_error: 22.889375, mean_q: 47.376115\n"," 17245/50000: episode: 351, duration: 0.462s, episode steps: 157, steps per second: 340, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.423 [-2.413, 1.039], loss: 3.362041, mean_absolute_error: 23.714148, mean_q: 49.054212\n"," 17430/50000: episode: 352, duration: 0.528s, episode steps: 185, steps per second: 351, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.422 [-2.408, 0.768], loss: 1.822144, mean_absolute_error: 23.916054, mean_q: 48.902562\n"," 17635/50000: episode: 353, duration: 0.558s, episode steps: 205, steps per second: 368, episode reward: 205.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.400 [-2.432, 0.919], loss: 1.085774, mean_absolute_error: 25.770312, mean_q: 53.127090\n"," 17952/50000: episode: 354, duration: 0.795s, episode steps: 317, steps per second: 399, episode reward: 317.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.430 [-2.436, 1.031], loss: 2.883843, mean_absolute_error: 29.602058, mean_q: 60.187633\n"," 18122/50000: episode: 355, duration: 0.508s, episode steps: 170, steps per second: 334, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.213 [-1.571, 1.560], loss: 6.667843, mean_absolute_error: 35.371728, mean_q: 72.397144\n"," 18362/50000: episode: 356, duration: 0.633s, episode steps: 240, steps per second: 379, episode reward: 240.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.072 [-1.452, 2.792], loss: 14.912384, mean_absolute_error: 36.675817, mean_q: 74.549597\n"," 18443/50000: episode: 357, duration: 0.335s, episode steps: 81, steps per second: 242, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.048 [-1.404, 2.381], loss: 17.193904, mean_absolute_error: 31.586199, mean_q: 63.645486\n"," 18587/50000: episode: 358, duration: 0.464s, episode steps: 144, steps per second: 310, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.446 [-0.736, 2.496], loss: 8.609270, mean_absolute_error: 28.926302, mean_q: 58.385262\n"," 18638/50000: episode: 359, duration: 0.276s, episode steps: 51, steps per second: 185, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: 0.299 [-1.234, 2.777], loss: 18.354363, mean_absolute_error: 23.624315, mean_q: 47.877975\n"," 18731/50000: episode: 360, duration: 0.368s, episode steps: 93, steps per second: 253, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.570 [0.000, 1.000], mean observation: 0.320 [-0.735, 1.860], loss: 3.287794, mean_absolute_error: 23.767005, mean_q: 48.424725\n"," 18821/50000: episode: 361, duration: 0.348s, episode steps: 90, steps per second: 258, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.578 [0.000, 1.000], mean observation: 0.345 [-0.833, 2.088], loss: 2.531842, mean_absolute_error: 22.934677, mean_q: 46.702694\n"," 18921/50000: episode: 362, duration: 0.378s, episode steps: 100, steps per second: 265, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.590 [0.000, 1.000], mean observation: 0.268 [-1.038, 2.686], loss: 1.544875, mean_absolute_error: 22.364348, mean_q: 45.801660\n"," 18962/50000: episode: 363, duration: 0.265s, episode steps: 41, steps per second: 155, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.634 [0.000, 1.000], mean observation: 0.325 [-1.430, 2.612], loss: 8.173866, mean_absolute_error: 16.699334, mean_q: 33.204058\n"," 19150/50000: episode: 364, duration: 0.552s, episode steps: 188, steps per second: 341, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.394 [-0.672, 2.420], loss: 1.009448, mean_absolute_error: 22.218589, mean_q: 45.164909\n"," 19268/50000: episode: 365, duration: 0.555s, episode steps: 118, steps per second: 213, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.585 [0.000, 1.000], mean observation: 0.307 [-1.573, 3.387], loss: 3.221200, mean_absolute_error: 21.799564, mean_q: 43.866327\n"," 19418/50000: episode: 366, duration: 0.475s, episode steps: 150, steps per second: 316, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.216 [-0.979, 1.947], loss: 1.894809, mean_absolute_error: 27.155595, mean_q: 55.198740\n"," 19483/50000: episode: 367, duration: 0.329s, episode steps: 65, steps per second: 198, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.106 [-1.565, 1.453], loss: 15.774471, mean_absolute_error: 28.155969, mean_q: 56.282299\n"," 19702/50000: episode: 368, duration: 0.628s, episode steps: 219, steps per second: 349, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: -0.045 [-1.578, 2.933], loss: 6.885459, mean_absolute_error: 26.571116, mean_q: 53.915500\n"," 19930/50000: episode: 369, duration: 0.638s, episode steps: 228, steps per second: 357, episode reward: 228.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.119 [-1.648, 2.844], loss: 4.146437, mean_absolute_error: 28.555456, mean_q: 58.399971\n"," 20108/50000: episode: 370, duration: 0.536s, episode steps: 178, steps per second: 332, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.102 [-1.765, 1.800], loss: 10.078474, mean_absolute_error: 30.312187, mean_q: 62.187611\n"," 20423/50000: episode: 371, duration: 0.800s, episode steps: 315, steps per second: 394, episode reward: 315.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.139 [-1.441, 2.539], loss: 1.505636, mean_absolute_error: 30.638833, mean_q: 63.261078\n"," 20923/50000: episode: 372, duration: 1.134s, episode steps: 500, steps per second: 441, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.101 [-0.963, 0.810], loss: 10.160760, mean_absolute_error: 38.709475, mean_q: 78.948093\n"," 21094/50000: episode: 373, duration: 0.546s, episode steps: 171, steps per second: 313, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.182 [-1.956, 1.455], loss: 3.449556, mean_absolute_error: 32.928536, mean_q: 68.728197\n"," 21208/50000: episode: 374, duration: 0.422s, episode steps: 114, steps per second: 270, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.060 [-1.671, 2.046], loss: 8.444346, mean_absolute_error: 30.452874, mean_q: 63.080243\n"," 21670/50000: episode: 375, duration: 1.139s, episode steps: 462, steps per second: 406, episode reward: 462.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.313 [-3.930, 1.764], loss: 2.974436, mean_absolute_error: 29.733446, mean_q: 61.344634\n"," 21801/50000: episode: 376, duration: 0.463s, episode steps: 131, steps per second: 283, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.028 [-2.399, 1.665], loss: 29.892574, mean_absolute_error: 37.535302, mean_q: 77.406746\n"," 21846/50000: episode: 377, duration: 0.290s, episode steps: 45, steps per second: 155, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.221 [-1.114, 2.124], loss: 24.072379, mean_absolute_error: 29.978130, mean_q: 60.351323\n"," 22135/50000: episode: 378, duration: 0.784s, episode steps: 289, steps per second: 368, episode reward: 289.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.099 [-1.728, 2.510], loss: 5.295266, mean_absolute_error: 30.855924, mean_q: 64.055710\n"," 22433/50000: episode: 379, duration: 0.794s, episode steps: 298, steps per second: 375, episode reward: 298.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.336 [-0.669, 2.425], loss: 8.829161, mean_absolute_error: 32.277880, mean_q: 66.062887\n"," 22625/50000: episode: 380, duration: 0.586s, episode steps: 192, steps per second: 327, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.413 [-0.511, 2.431], loss: 1.170207, mean_absolute_error: 28.581621, mean_q: 58.328654\n"," 22773/50000: episode: 381, duration: 0.521s, episode steps: 148, steps per second: 284, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.241 [-1.118, 1.973], loss: 4.427035, mean_absolute_error: 30.990241, mean_q: 63.399847\n"," 23011/50000: episode: 382, duration: 0.654s, episode steps: 238, steps per second: 364, episode reward: 238.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.313 [-1.019, 2.063], loss: 1.423118, mean_absolute_error: 30.608473, mean_q: 62.831119\n"," 23198/50000: episode: 383, duration: 0.558s, episode steps: 187, steps per second: 335, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.183 [-1.302, 1.544], loss: 5.454672, mean_absolute_error: 33.233981, mean_q: 67.816164\n"," 23698/50000: episode: 384, duration: 1.163s, episode steps: 500, steps per second: 430, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.122 [-1.304, 1.356], loss: 9.131104, mean_absolute_error: 38.404860, mean_q: 78.793858\n"," 23817/50000: episode: 385, duration: 0.452s, episode steps: 119, steps per second: 263, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.029 [-1.395, 1.716], loss: 11.678528, mean_absolute_error: 33.929355, mean_q: 69.584647\n"," 24106/50000: episode: 386, duration: 0.751s, episode steps: 289, steps per second: 385, episode reward: 289.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.114 [-3.065, 1.701], loss: 17.204389, mean_absolute_error: 32.410410, mean_q: 66.638950\n"," 24600/50000: episode: 387, duration: 1.170s, episode steps: 494, steps per second: 422, episode reward: 494.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.337 [-2.405, 1.259], loss: 8.919299, mean_absolute_error: 32.341053, mean_q: 66.819481\n"," 24856/50000: episode: 388, duration: 0.701s, episode steps: 256, steps per second: 365, episode reward: 256.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.412 [-2.409, 0.884], loss: 10.594686, mean_absolute_error: 31.714993, mean_q: 66.086883\n"," 24940/50000: episode: 389, duration: 0.373s, episode steps: 84, steps per second: 225, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.063 [-2.026, 1.247], loss: 12.169775, mean_absolute_error: 32.290306, mean_q: 66.844847\n"," 25189/50000: episode: 390, duration: 0.693s, episode steps: 249, steps per second: 360, episode reward: 249.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.485 [-2.411, 0.565], loss: 6.754555, mean_absolute_error: 26.365972, mean_q: 54.529589\n"," 25375/50000: episode: 391, duration: 0.566s, episode steps: 186, steps per second: 329, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.470 [-2.408, 0.565], loss: 7.235085, mean_absolute_error: 27.206722, mean_q: 56.595352\n"," 25569/50000: episode: 392, duration: 0.584s, episode steps: 194, steps per second: 332, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.422 [-2.401, 0.698], loss: 7.290921, mean_absolute_error: 28.097965, mean_q: 58.224283\n"," 25729/50000: episode: 393, duration: 0.517s, episode steps: 160, steps per second: 309, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.507 [-2.412, 0.775], loss: 6.830131, mean_absolute_error: 25.448242, mean_q: 53.265757\n"," 25935/50000: episode: 394, duration: 0.605s, episode steps: 206, steps per second: 341, episode reward: 206.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.469 [-2.407, 0.537], loss: 4.669408, mean_absolute_error: 25.257699, mean_q: 52.594471\n"," 26115/50000: episode: 395, duration: 0.564s, episode steps: 180, steps per second: 319, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.502 [-2.402, 0.601], loss: 6.781043, mean_absolute_error: 25.654663, mean_q: 53.392680\n"," 26314/50000: episode: 396, duration: 0.593s, episode steps: 199, steps per second: 335, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.448 [-2.400, 0.512], loss: 4.529153, mean_absolute_error: 28.033075, mean_q: 57.975164\n"," 26464/50000: episode: 397, duration: 0.502s, episode steps: 150, steps per second: 299, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.547 [-2.416, 0.808], loss: 3.698963, mean_absolute_error: 23.926200, mean_q: 50.508227\n"," 26617/50000: episode: 398, duration: 0.505s, episode steps: 153, steps per second: 303, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.482 [-2.425, 0.815], loss: 3.495700, mean_absolute_error: 24.832319, mean_q: 52.076515\n"," 26667/50000: episode: 399, duration: 0.328s, episode steps: 50, steps per second: 153, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.340 [0.000, 1.000], mean observation: -0.376 [-3.674, 1.471], loss: 23.454927, mean_absolute_error: 25.450317, mean_q: 55.113141\n"," 26822/50000: episode: 400, duration: 0.515s, episode steps: 155, steps per second: 301, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.481 [-2.416, 0.526], loss: 2.139287, mean_absolute_error: 18.199586, mean_q: 37.833619\n"," 26970/50000: episode: 401, duration: 0.508s, episode steps: 148, steps per second: 291, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.490 [-2.603, 0.960], loss: 1.762233, mean_absolute_error: 19.303208, mean_q: 41.426067\n"," 27050/50000: episode: 402, duration: 0.387s, episode steps: 80, steps per second: 207, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.311 [-2.593, 2.022], loss: 13.536835, mean_absolute_error: 23.103255, mean_q: 50.863416\n"," 27298/50000: episode: 403, duration: 0.695s, episode steps: 248, steps per second: 357, episode reward: 248.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.478 [-2.407, 1.131], loss: 4.267411, mean_absolute_error: 19.207740, mean_q: 40.448634\n"," 27594/50000: episode: 404, duration: 0.783s, episode steps: 296, steps per second: 378, episode reward: 296.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.470 [-2.401, 0.570], loss: 6.625418, mean_absolute_error: 25.595341, mean_q: 53.176481\n"," 27888/50000: episode: 405, duration: 0.908s, episode steps: 294, steps per second: 324, episode reward: 294.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.482 [-2.401, 0.777], loss: 7.549589, mean_absolute_error: 29.236863, mean_q: 60.680944\n"," 28151/50000: episode: 406, duration: 0.730s, episode steps: 263, steps per second: 360, episode reward: 263.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.505 [-2.401, 0.652], loss: 7.964759, mean_absolute_error: 29.988807, mean_q: 62.091405\n"," 28366/50000: episode: 407, duration: 0.643s, episode steps: 215, steps per second: 335, episode reward: 215.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.452 [-2.404, 0.727], loss: 7.279371, mean_absolute_error: 31.698928, mean_q: 65.131554\n"," 28530/50000: episode: 408, duration: 0.535s, episode steps: 164, steps per second: 306, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.516 [-2.400, 0.834], loss: 7.862921, mean_absolute_error: 28.804454, mean_q: 59.882344\n"," 28752/50000: episode: 409, duration: 0.668s, episode steps: 222, steps per second: 332, episode reward: 222.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.496 [-2.405, 0.521], loss: 4.931177, mean_absolute_error: 26.198250, mean_q: 54.252549\n"," 28934/50000: episode: 410, duration: 0.587s, episode steps: 182, steps per second: 310, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.518 [-2.406, 0.684], loss: 5.639773, mean_absolute_error: 26.203423, mean_q: 54.053024\n"," 29102/50000: episode: 411, duration: 0.548s, episode steps: 168, steps per second: 307, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.463 [-2.412, 0.524], loss: 4.339297, mean_absolute_error: 28.520598, mean_q: 58.399822\n"," 29302/50000: episode: 412, duration: 0.624s, episode steps: 200, steps per second: 321, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.501 [-2.407, 0.773], loss: 2.874985, mean_absolute_error: 23.900669, mean_q: 49.351323\n"," 29518/50000: episode: 413, duration: 0.640s, episode steps: 216, steps per second: 337, episode reward: 216.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.488 [-2.401, 0.819], loss: 4.424228, mean_absolute_error: 24.682616, mean_q: 51.070068\n"," 29699/50000: episode: 414, duration: 0.579s, episode steps: 181, steps per second: 313, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.454 [-2.404, 0.728], loss: 3.812584, mean_absolute_error: 27.643271, mean_q: 56.688119\n"," 29744/50000: episode: 415, duration: 0.323s, episode steps: 45, steps per second: 139, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.404 [-3.510, 1.501], loss: 26.199982, mean_absolute_error: 28.270803, mean_q: 60.541816\n"," 29894/50000: episode: 416, duration: 0.535s, episode steps: 150, steps per second: 280, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.456 [-2.168, 0.624], loss: 1.971047, mean_absolute_error: 19.501367, mean_q: 39.870572\n"," 30043/50000: episode: 417, duration: 0.524s, episode steps: 149, steps per second: 285, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.490 [-2.394, 0.936], loss: 1.538865, mean_absolute_error: 17.809703, mean_q: 37.783639\n"," 30260/50000: episode: 418, duration: 0.679s, episode steps: 217, steps per second: 320, episode reward: 217.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.491 [-2.414, 0.984], loss: 3.977505, mean_absolute_error: 19.465905, mean_q: 40.669563\n"," 30557/50000: episode: 419, duration: 0.808s, episode steps: 297, steps per second: 367, episode reward: 297.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.461 [-2.409, 0.935], loss: 7.185598, mean_absolute_error: 26.490842, mean_q: 54.645052\n"," 30843/50000: episode: 420, duration: 0.777s, episode steps: 286, steps per second: 368, episode reward: 286.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.474 [-2.404, 0.755], loss: 7.571565, mean_absolute_error: 28.890881, mean_q: 59.059088\n"," 31071/50000: episode: 421, duration: 0.676s, episode steps: 228, steps per second: 337, episode reward: 228.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.479 [-2.403, 0.895], loss: 7.258551, mean_absolute_error: 31.173605, mean_q: 64.380534\n"," 31303/50000: episode: 422, duration: 0.700s, episode steps: 232, steps per second: 332, episode reward: 232.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.471 [-2.401, 0.602], loss: 5.519827, mean_absolute_error: 29.652922, mean_q: 60.878730\n"," 31628/50000: episode: 423, duration: 0.863s, episode steps: 325, steps per second: 377, episode reward: 325.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.464 [-2.408, 0.674], loss: 7.158285, mean_absolute_error: 29.680339, mean_q: 60.516690\n"," 31885/50000: episode: 424, duration: 0.732s, episode steps: 257, steps per second: 351, episode reward: 257.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.429 [-2.404, 0.570], loss: 5.128685, mean_absolute_error: 32.593325, mean_q: 66.239951\n"," 31927/50000: episode: 425, duration: 0.456s, episode steps: 42, steps per second: 92, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: -0.539 [-4.429, 1.999], loss: 57.145573, mean_absolute_error: 30.181525, mean_q: 64.243905\n"," 32121/50000: episode: 426, duration: 0.616s, episode steps: 194, steps per second: 315, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.467 [-2.413, 0.744], loss: 3.191111, mean_absolute_error: 22.821042, mean_q: 46.528361\n"," 32297/50000: episode: 427, duration: 0.590s, episode steps: 176, steps per second: 298, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.497 [-2.403, 1.004], loss: 3.700467, mean_absolute_error: 23.112735, mean_q: 47.891015\n"," 32405/50000: episode: 428, duration: 0.457s, episode steps: 108, steps per second: 236, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.361 [0.000, 1.000], mean observation: -0.404 [-4.961, 2.820], loss: 2.554957, mean_absolute_error: 25.486411, mean_q: 55.150035\n"," 32579/50000: episode: 429, duration: 0.583s, episode steps: 174, steps per second: 298, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.444 [-2.407, 0.966], loss: 2.895088, mean_absolute_error: 23.131779, mean_q: 48.073507\n"," 32746/50000: episode: 430, duration: 0.572s, episode steps: 167, steps per second: 292, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.538 [-2.418, 0.951], loss: 2.136206, mean_absolute_error: 20.630913, mean_q: 42.972731\n"," 32911/50000: episode: 431, duration: 0.554s, episode steps: 165, steps per second: 298, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.515 [-2.404, 0.857], loss: 3.658633, mean_absolute_error: 22.628269, mean_q: 47.266497\n"," 33121/50000: episode: 432, duration: 0.664s, episode steps: 210, steps per second: 316, episode reward: 210.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.503 [-2.540, 1.028], loss: 4.512170, mean_absolute_error: 23.010007, mean_q: 47.866476\n"," 33234/50000: episode: 433, duration: 0.460s, episode steps: 113, steps per second: 246, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.398 [0.000, 1.000], mean observation: -0.490 [-3.403, 1.105], loss: 1.144871, mean_absolute_error: 24.082453, mean_q: 52.011941\n"," 33510/50000: episode: 434, duration: 0.778s, episode steps: 276, steps per second: 355, episode reward: 276.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.493 [-2.408, 0.798], loss: 7.149986, mean_absolute_error: 24.282437, mean_q: 50.026785\n"," 33767/50000: episode: 435, duration: 0.747s, episode steps: 257, steps per second: 344, episode reward: 257.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.400 [-2.410, 0.680], loss: 5.484064, mean_absolute_error: 30.428657, mean_q: 62.159352\n"," 33960/50000: episode: 436, duration: 0.621s, episode steps: 193, steps per second: 311, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.505 [-2.405, 0.534], loss: 1.928923, mean_absolute_error: 26.938925, mean_q: 55.033400\n"," 34003/50000: episode: 437, duration: 0.336s, episode steps: 43, steps per second: 128, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.395 [0.000, 1.000], mean observation: -0.387 [-2.723, 1.101], loss: 45.263529, mean_absolute_error: 29.805643, mean_q: 62.054132\n"," 34191/50000: episode: 438, duration: 0.629s, episode steps: 188, steps per second: 299, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.463 [-2.307, 0.541], loss: 0.983637, mean_absolute_error: 20.540640, mean_q: 42.388943\n"," 34257/50000: episode: 439, duration: 0.405s, episode steps: 66, steps per second: 163, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.621 [0.000, 1.000], mean observation: -0.253 [-2.868, 2.242], loss: 4.154300, mean_absolute_error: 20.871201, mean_q: 46.888876\n"," 34456/50000: episode: 440, duration: 0.654s, episode steps: 199, steps per second: 304, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.514 [-2.410, 0.799], loss: 2.664238, mean_absolute_error: 19.491896, mean_q: 40.537778\n"," 34628/50000: episode: 441, duration: 0.594s, episode steps: 172, steps per second: 290, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.505 [-2.407, 0.728], loss: 3.942818, mean_absolute_error: 23.659203, mean_q: 48.977520\n"," 34794/50000: episode: 442, duration: 0.601s, episode steps: 166, steps per second: 276, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.490 [-2.400, 0.835], loss: 2.944142, mean_absolute_error: 25.071286, mean_q: 51.718491\n"," 34944/50000: episode: 443, duration: 0.551s, episode steps: 150, steps per second: 272, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.545 [-2.432, 0.863], loss: 3.179280, mean_absolute_error: 22.602532, mean_q: 46.841490\n"," 34993/50000: episode: 444, duration: 0.377s, episode steps: 49, steps per second: 130, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.265 [0.000, 1.000], mean observation: -0.597 [-4.655, 1.788], loss: 17.750098, mean_absolute_error: 21.076490, mean_q: 45.881099\n"," 35137/50000: episode: 445, duration: 0.567s, episode steps: 144, steps per second: 254, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.503 [-2.367, 0.668], loss: 1.234894, mean_absolute_error: 17.506384, mean_q: 36.450388\n"," 35260/50000: episode: 446, duration: 0.528s, episode steps: 123, steps per second: 233, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.527 [-2.403, 0.620], loss: 3.318195, mean_absolute_error: 18.389077, mean_q: 38.219441\n"," 35407/50000: episode: 447, duration: 0.571s, episode steps: 147, steps per second: 257, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.560 [-2.412, 0.717], loss: 1.024709, mean_absolute_error: 16.668986, mean_q: 34.953879\n"," 35584/50000: episode: 448, duration: 0.613s, episode steps: 177, steps per second: 289, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.501 [-2.400, 0.654], loss: 3.301042, mean_absolute_error: 21.468483, mean_q: 44.614036\n"," 35713/50000: episode: 449, duration: 0.527s, episode steps: 129, steps per second: 245, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.528 [-2.410, 0.790], loss: 3.492681, mean_absolute_error: 23.349004, mean_q: 48.582743\n"," 35902/50000: episode: 450, duration: 0.645s, episode steps: 189, steps per second: 293, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.501 [-2.401, 0.618], loss: 3.857327, mean_absolute_error: 21.909225, mean_q: 45.333148\n"," 36083/50000: episode: 451, duration: 0.640s, episode steps: 181, steps per second: 283, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.528 [-2.400, 0.508], loss: 3.998496, mean_absolute_error: 23.543094, mean_q: 48.407325\n"," 36315/50000: episode: 452, duration: 0.730s, episode steps: 232, steps per second: 318, episode reward: 232.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.441 [-2.401, 0.521], loss: 4.614356, mean_absolute_error: 27.051503, mean_q: 55.305902\n"," 36570/50000: episode: 453, duration: 0.776s, episode steps: 255, steps per second: 328, episode reward: 255.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.498 [-2.407, 0.848], loss: 5.367124, mean_absolute_error: 25.685196, mean_q: 52.724305\n"," 36741/50000: episode: 454, duration: 0.630s, episode steps: 171, steps per second: 272, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.440 [-2.404, 0.608], loss: 4.198157, mean_absolute_error: 30.353615, mean_q: 61.921204\n"," 36894/50000: episode: 455, duration: 0.568s, episode steps: 153, steps per second: 269, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.506 [-2.568, 0.994], loss: 3.053807, mean_absolute_error: 24.526241, mean_q: 50.608960\n"," 37103/50000: episode: 456, duration: 0.686s, episode steps: 209, steps per second: 305, episode reward: 209.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.515 [-2.406, 0.852], loss: 3.075289, mean_absolute_error: 21.444205, mean_q: 44.079773\n"," 37330/50000: episode: 457, duration: 0.717s, episode steps: 227, steps per second: 316, episode reward: 227.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.464 [-2.425, 1.023], loss: 3.788617, mean_absolute_error: 26.243575, mean_q: 53.792651\n"," 37504/50000: episode: 458, duration: 0.605s, episode steps: 174, steps per second: 287, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.481 [-2.408, 0.801], loss: 1.136228, mean_absolute_error: 25.753612, mean_q: 52.792439\n"," 37542/50000: episode: 459, duration: 0.368s, episode steps: 38, steps per second: 103, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.211 [0.000, 1.000], mean observation: -0.495 [-3.991, 1.766], loss: 21.127052, mean_absolute_error: 24.822940, mean_q: 53.315418\n"," 37703/50000: episode: 460, duration: 0.592s, episode steps: 161, steps per second: 272, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.460 [-2.122, 0.809], loss: 2.447041, mean_absolute_error: 18.855825, mean_q: 38.333557\n"," 37785/50000: episode: 461, duration: 0.454s, episode steps: 82, steps per second: 181, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.415 [0.000, 1.000], mean observation: -0.445 [-2.651, 1.372], loss: 3.627200, mean_absolute_error: 19.230383, mean_q: 40.800057\n"," 38042/50000: episode: 462, duration: 0.786s, episode steps: 257, steps per second: 327, episode reward: 257.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.396 [-2.416, 1.065], loss: 3.491832, mean_absolute_error: 21.592946, mean_q: 44.281750\n"," 38339/50000: episode: 463, duration: 0.875s, episode steps: 297, steps per second: 339, episode reward: 297.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.481 [-2.400, 0.682], loss: 5.736113, mean_absolute_error: 26.261699, mean_q: 53.613248\n"," 38722/50000: episode: 464, duration: 1.035s, episode steps: 383, steps per second: 370, episode reward: 383.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.493 [-2.402, 0.721], loss: 8.671442, mean_absolute_error: 31.327440, mean_q: 63.370531\n"," 39222/50000: episode: 465, duration: 1.403s, episode steps: 500, steps per second: 356, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.438 [-2.241, 0.853], loss: 8.862174, mean_absolute_error: 36.583252, mean_q: 73.927440\n"," 39236/50000: episode: 466, duration: 0.323s, episode steps: 14, steps per second: 43, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.064 [-1.399, 2.010], loss: 48.925462, mean_absolute_error: 39.333768, mean_q: 74.462329\n"," 39425/50000: episode: 467, duration: 0.655s, episode steps: 189, steps per second: 289, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.383 [-2.402, 0.981], loss: 5.748839, mean_absolute_error: 29.724680, mean_q: 60.183429\n"," 39625/50000: episode: 468, duration: 0.691s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.478 [-2.407, 0.621], loss: 4.343837, mean_absolute_error: 26.424321, mean_q: 53.667630\n"," 39898/50000: episode: 469, duration: 0.845s, episode steps: 273, steps per second: 323, episode reward: 273.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.505 [-2.409, 1.019], loss: 6.354416, mean_absolute_error: 26.128931, mean_q: 53.282788\n"," 40089/50000: episode: 470, duration: 0.683s, episode steps: 191, steps per second: 280, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.460 [-2.409, 0.783], loss: 5.028331, mean_absolute_error: 30.453548, mean_q: 62.135201\n"," 40448/50000: episode: 471, duration: 0.988s, episode steps: 359, steps per second: 363, episode reward: 359.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.499 [0.000, 1.000], mean observation: -0.417 [-2.406, 0.636], loss: 5.049165, mean_absolute_error: 30.129097, mean_q: 61.233721\n"," 40948/50000: episode: 472, duration: 1.244s, episode steps: 500, steps per second: 402, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.487 [-2.171, 0.706], loss: 8.521415, mean_absolute_error: 35.261803, mean_q: 71.447502\n"," 41448/50000: episode: 473, duration: 1.255s, episode steps: 500, steps per second: 398, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.389 [-1.868, 0.640], loss: 7.904101, mean_absolute_error: 39.555338, mean_q: 80.275551\n"," 41459/50000: episode: 474, duration: 0.334s, episode steps: 11, steps per second: 33, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.172 [-0.765, 1.446], loss: 67.374658, mean_absolute_error: 27.267427, mean_q: 48.073588\n"," 41749/50000: episode: 475, duration: 0.870s, episode steps: 290, steps per second: 333, episode reward: 290.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.440 [-2.402, 0.688], loss: 7.311294, mean_absolute_error: 29.913374, mean_q: 60.403691\n"," 42236/50000: episode: 476, duration: 1.258s, episode steps: 487, steps per second: 387, episode reward: 487.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.499 [0.000, 1.000], mean observation: -0.512 [-2.406, 0.655], loss: 7.104664, mean_absolute_error: 32.759172, mean_q: 66.499188\n"," 42634/50000: episode: 477, duration: 1.059s, episode steps: 398, steps per second: 376, episode reward: 398.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.497 [-2.409, 0.665], loss: 7.985937, mean_absolute_error: 35.956570, mean_q: 72.922420\n"," 42810/50000: episode: 478, duration: 0.633s, episode steps: 176, steps per second: 278, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.485 [-2.402, 0.694], loss: 8.889496, mean_absolute_error: 36.381865, mean_q: 73.609466\n"," 43069/50000: episode: 479, duration: 0.797s, episode steps: 259, steps per second: 325, episode reward: 259.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.409 [-2.403, 0.843], loss: 4.595320, mean_absolute_error: 32.461895, mean_q: 65.693633\n"," 43295/50000: episode: 480, duration: 0.760s, episode steps: 226, steps per second: 297, episode reward: 226.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.454 [-2.414, 1.165], loss: 5.456060, mean_absolute_error: 29.588171, mean_q: 60.291237\n"," 43454/50000: episode: 481, duration: 0.617s, episode steps: 159, steps per second: 258, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.436 [-2.393, 1.686], loss: 1.489225, mean_absolute_error: 29.294005, mean_q: 59.750234\n"," 43472/50000: episode: 482, duration: 0.351s, episode steps: 18, steps per second: 51, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: -0.120 [-1.072, 0.693], loss: 85.301201, mean_absolute_error: 36.469453, mean_q: 73.110640\n"," 43566/50000: episode: 483, duration: 0.496s, episode steps: 94, steps per second: 190, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.325 [-2.386, 1.406], loss: 6.520645, mean_absolute_error: 24.181465, mean_q: 49.925732\n"," 44066/50000: episode: 484, duration: 1.272s, episode steps: 500, steps per second: 393, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.318 [-1.933, 0.679], loss: 4.908667, mean_absolute_error: 28.302633, mean_q: 57.580450\n"," 44566/50000: episode: 485, duration: 1.276s, episode steps: 500, steps per second: 392, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: -0.020 [-1.108, 1.104], loss: 8.672474, mean_absolute_error: 42.452383, mean_q: 85.952160\n"," 45066/50000: episode: 486, duration: 1.267s, episode steps: 500, steps per second: 395, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.348 [-1.980, 0.786], loss: 7.115389, mean_absolute_error: 34.774936, mean_q: 70.305702\n"," 45566/50000: episode: 487, duration: 1.264s, episode steps: 500, steps per second: 396, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.087 [-1.253, 0.939], loss: 7.219273, mean_absolute_error: 43.472245, mean_q: 87.547221\n"," 45606/50000: episode: 488, duration: 0.410s, episode steps: 40, steps per second: 97, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-1.143, 1.468], loss: 68.431642, mean_absolute_error: 40.446721, mean_q: 78.878818\n"," 45799/50000: episode: 489, duration: 0.685s, episode steps: 193, steps per second: 282, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.297 [-1.651, 1.046], loss: 2.072237, mean_absolute_error: 27.868738, mean_q: 55.995705\n"," 46243/50000: episode: 490, duration: 1.169s, episode steps: 444, steps per second: 380, episode reward: 444.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.459 [-2.414, 1.420], loss: 4.994123, mean_absolute_error: 28.029206, mean_q: 56.353186\n"," 46469/50000: episode: 491, duration: 0.758s, episode steps: 226, steps per second: 298, episode reward: 226.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.401 [-2.414, 0.835], loss: 5.409064, mean_absolute_error: 34.008624, mean_q: 68.404454\n"," 46676/50000: episode: 492, duration: 0.721s, episode steps: 207, steps per second: 287, episode reward: 207.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.451 [-2.326, 1.319], loss: 2.477531, mean_absolute_error: 29.028127, mean_q: 59.074645\n"," 47176/50000: episode: 493, duration: 1.281s, episode steps: 500, steps per second: 390, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.352 [-2.546, 1.306], loss: 7.474599, mean_absolute_error: 35.377410, mean_q: 71.726442\n"," 47676/50000: episode: 494, duration: 1.286s, episode steps: 500, steps per second: 389, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.427 [-2.242, 0.825], loss: 6.865890, mean_absolute_error: 36.207158, mean_q: 73.858216\n"," 48176/50000: episode: 495, duration: 1.279s, episode steps: 500, steps per second: 391, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.282 [-2.407, 1.206], loss: 5.564485, mean_absolute_error: 39.832842, mean_q: 80.460915\n"," 48496/50000: episode: 496, duration: 0.965s, episode steps: 320, steps per second: 332, episode reward: 320.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.308 [-2.406, 0.770], loss: 4.364115, mean_absolute_error: 38.673015, mean_q: 78.143673\n"," 48791/50000: episode: 497, duration: 0.907s, episode steps: 295, steps per second: 325, episode reward: 295.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.429 [-2.403, 0.943], loss: 4.315918, mean_absolute_error: 31.102950, mean_q: 63.129460\n"," 49018/50000: episode: 498, duration: 0.778s, episode steps: 227, steps per second: 292, episode reward: 227.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.449 [-2.412, 0.704], loss: 3.161117, mean_absolute_error: 30.751172, mean_q: 62.154369\n"," 49056/50000: episode: 499, duration: 0.421s, episode steps: 38, steps per second: 90, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.132 [0.000, 1.000], mean observation: -0.509 [-3.972, 0.725], loss: 2.349315, mean_absolute_error: 22.949011, mean_q: 49.312661\n"," 49556/50000: episode: 500, duration: 1.310s, episode steps: 500, steps per second: 382, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-1.102, 0.908], loss: 10.089389, mean_absolute_error: 42.446779, mean_q: 86.162833\n"," 49808/50000: episode: 501, duration: 0.829s, episode steps: 252, steps per second: 304, episode reward: 252.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.390 [-0.917, 2.414], loss: 21.560341, mean_absolute_error: 41.717366, mean_q: 84.302967\n","done, took 176.175 seconds\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"7qnDd3bg_9BC","colab_type":"code","outputId":"c6b2c603-51e9-4d03-9590-7678e8bf16c4","executionInfo":{"status":"ok","timestamp":1587486391446,"user_tz":-330,"elapsed":363054,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# scores=sarsa.test(env, nb_episodes=1000, nb_max_episode_steps=500, visualize=False)"],"execution_count":77,"outputs":[{"output_type":"stream","text":["Testing for 1000 episodes ...\n","Episode 1: reward: 452.000, steps: 452\n","Episode 2: reward: 421.000, steps: 421\n","Episode 3: reward: 402.000, steps: 402\n","Episode 4: reward: 372.000, steps: 372\n","Episode 5: reward: 405.000, steps: 405\n","Episode 6: reward: 382.000, steps: 382\n","Episode 7: reward: 456.000, steps: 456\n","Episode 8: reward: 390.000, steps: 390\n","Episode 9: reward: 420.000, steps: 420\n","Episode 10: reward: 368.000, steps: 368\n","Episode 11: reward: 353.000, steps: 353\n","Episode 12: reward: 396.000, steps: 396\n","Episode 13: reward: 433.000, steps: 433\n","Episode 14: reward: 406.000, steps: 406\n","Episode 15: reward: 477.000, steps: 477\n","Episode 16: reward: 409.000, steps: 409\n","Episode 17: reward: 444.000, steps: 444\n","Episode 18: reward: 422.000, steps: 422\n","Episode 19: reward: 383.000, steps: 383\n","Episode 20: reward: 449.000, steps: 449\n","Episode 21: reward: 412.000, steps: 412\n","Episode 22: reward: 383.000, steps: 383\n","Episode 23: reward: 398.000, steps: 398\n","Episode 24: reward: 432.000, steps: 432\n","Episode 25: reward: 373.000, steps: 373\n","Episode 26: reward: 385.000, steps: 385\n","Episode 27: reward: 472.000, steps: 472\n","Episode 28: reward: 414.000, steps: 414\n","Episode 29: reward: 420.000, steps: 420\n","Episode 30: reward: 432.000, steps: 432\n","Episode 31: reward: 437.000, steps: 437\n","Episode 32: reward: 392.000, steps: 392\n","Episode 33: reward: 437.000, steps: 437\n","Episode 34: reward: 431.000, steps: 431\n","Episode 35: reward: 391.000, steps: 391\n","Episode 36: reward: 427.000, steps: 427\n","Episode 37: reward: 438.000, steps: 438\n","Episode 38: reward: 436.000, steps: 436\n","Episode 39: reward: 359.000, steps: 359\n","Episode 40: reward: 422.000, steps: 422\n","Episode 41: reward: 431.000, steps: 431\n","Episode 42: reward: 388.000, steps: 388\n","Episode 43: reward: 394.000, steps: 394\n","Episode 44: reward: 389.000, steps: 389\n","Episode 45: reward: 403.000, steps: 403\n","Episode 46: reward: 384.000, steps: 384\n","Episode 47: reward: 399.000, steps: 399\n","Episode 48: reward: 386.000, steps: 386\n","Episode 49: reward: 470.000, steps: 470\n","Episode 50: reward: 366.000, steps: 366\n","Episode 51: reward: 401.000, steps: 401\n","Episode 52: reward: 448.000, steps: 448\n","Episode 53: reward: 393.000, steps: 393\n","Episode 54: reward: 469.000, steps: 469\n","Episode 55: reward: 381.000, steps: 381\n","Episode 56: reward: 476.000, steps: 476\n","Episode 57: reward: 411.000, steps: 411\n","Episode 58: reward: 387.000, steps: 387\n","Episode 59: reward: 419.000, steps: 419\n","Episode 60: reward: 435.000, steps: 435\n","Episode 61: reward: 413.000, steps: 413\n","Episode 62: reward: 492.000, steps: 492\n","Episode 63: reward: 422.000, steps: 422\n","Episode 64: reward: 454.000, steps: 454\n","Episode 65: reward: 423.000, steps: 423\n","Episode 66: reward: 417.000, steps: 417\n","Episode 67: reward: 425.000, steps: 425\n","Episode 68: reward: 371.000, steps: 371\n","Episode 69: reward: 393.000, steps: 393\n","Episode 70: reward: 421.000, steps: 421\n","Episode 71: reward: 420.000, steps: 420\n","Episode 72: reward: 439.000, steps: 439\n","Episode 73: reward: 446.000, steps: 446\n","Episode 74: reward: 418.000, steps: 418\n","Episode 75: reward: 442.000, steps: 442\n","Episode 76: reward: 397.000, steps: 397\n","Episode 77: reward: 471.000, steps: 471\n","Episode 78: reward: 375.000, steps: 375\n","Episode 79: reward: 458.000, steps: 458\n","Episode 80: reward: 359.000, steps: 359\n","Episode 81: reward: 473.000, steps: 473\n","Episode 82: reward: 370.000, steps: 370\n","Episode 83: reward: 447.000, steps: 447\n","Episode 84: reward: 429.000, steps: 429\n","Episode 85: reward: 409.000, steps: 409\n","Episode 86: reward: 407.000, steps: 407\n","Episode 87: reward: 395.000, steps: 395\n","Episode 88: reward: 397.000, steps: 397\n","Episode 89: reward: 407.000, steps: 407\n","Episode 90: reward: 449.000, steps: 449\n","Episode 91: reward: 433.000, steps: 433\n","Episode 92: reward: 380.000, steps: 380\n","Episode 93: reward: 405.000, steps: 405\n","Episode 94: reward: 440.000, steps: 440\n","Episode 95: reward: 412.000, steps: 412\n","Episode 96: reward: 428.000, steps: 428\n","Episode 97: reward: 405.000, steps: 405\n","Episode 98: reward: 418.000, steps: 418\n","Episode 99: reward: 422.000, steps: 422\n","Episode 100: reward: 428.000, steps: 428\n","Episode 101: reward: 424.000, steps: 424\n","Episode 102: reward: 435.000, steps: 435\n","Episode 103: reward: 442.000, steps: 442\n","Episode 104: reward: 447.000, steps: 447\n","Episode 105: reward: 438.000, steps: 438\n","Episode 106: reward: 454.000, steps: 454\n","Episode 107: reward: 382.000, steps: 382\n","Episode 108: reward: 391.000, steps: 391\n","Episode 109: reward: 384.000, steps: 384\n","Episode 110: reward: 443.000, steps: 443\n","Episode 111: reward: 416.000, steps: 416\n","Episode 112: reward: 391.000, steps: 391\n","Episode 113: reward: 403.000, steps: 403\n","Episode 114: reward: 426.000, steps: 426\n","Episode 115: reward: 411.000, steps: 411\n","Episode 116: reward: 393.000, steps: 393\n","Episode 117: reward: 376.000, steps: 376\n","Episode 118: reward: 449.000, steps: 449\n","Episode 119: reward: 389.000, steps: 389\n","Episode 120: reward: 360.000, steps: 360\n","Episode 121: reward: 414.000, steps: 414\n","Episode 122: reward: 387.000, steps: 387\n","Episode 123: reward: 398.000, steps: 398\n","Episode 124: reward: 500.000, steps: 500\n","Episode 125: reward: 383.000, steps: 383\n","Episode 126: reward: 433.000, steps: 433\n","Episode 127: reward: 396.000, steps: 396\n","Episode 128: reward: 447.000, steps: 447\n","Episode 129: reward: 412.000, steps: 412\n","Episode 130: reward: 474.000, steps: 474\n","Episode 131: reward: 418.000, steps: 418\n","Episode 132: reward: 391.000, steps: 391\n","Episode 133: reward: 414.000, steps: 414\n","Episode 134: reward: 445.000, steps: 445\n","Episode 135: reward: 392.000, steps: 392\n","Episode 136: reward: 398.000, steps: 398\n","Episode 137: reward: 445.000, steps: 445\n","Episode 138: reward: 443.000, steps: 443\n","Episode 139: reward: 428.000, steps: 428\n","Episode 140: reward: 447.000, steps: 447\n","Episode 141: reward: 418.000, steps: 418\n","Episode 142: reward: 410.000, steps: 410\n","Episode 143: reward: 416.000, steps: 416\n","Episode 144: reward: 374.000, steps: 374\n","Episode 145: reward: 375.000, steps: 375\n","Episode 146: reward: 475.000, steps: 475\n","Episode 147: reward: 437.000, steps: 437\n","Episode 148: reward: 399.000, steps: 399\n","Episode 149: reward: 392.000, steps: 392\n","Episode 150: reward: 376.000, steps: 376\n","Episode 151: reward: 422.000, steps: 422\n","Episode 152: reward: 425.000, steps: 425\n","Episode 153: reward: 434.000, steps: 434\n","Episode 154: reward: 441.000, steps: 441\n","Episode 155: reward: 411.000, steps: 411\n","Episode 156: reward: 402.000, steps: 402\n","Episode 157: reward: 372.000, steps: 372\n","Episode 158: reward: 383.000, steps: 383\n","Episode 159: reward: 434.000, steps: 434\n","Episode 160: reward: 431.000, steps: 431\n","Episode 161: reward: 432.000, steps: 432\n","Episode 162: reward: 432.000, steps: 432\n","Episode 163: reward: 420.000, steps: 420\n","Episode 164: reward: 359.000, steps: 359\n","Episode 165: reward: 463.000, steps: 463\n","Episode 166: reward: 419.000, steps: 419\n","Episode 167: reward: 447.000, steps: 447\n","Episode 168: reward: 397.000, steps: 397\n","Episode 169: reward: 360.000, steps: 360\n","Episode 170: reward: 426.000, steps: 426\n","Episode 171: reward: 389.000, steps: 389\n","Episode 172: reward: 445.000, steps: 445\n","Episode 173: reward: 422.000, steps: 422\n","Episode 174: reward: 418.000, steps: 418\n","Episode 175: reward: 369.000, steps: 369\n","Episode 176: reward: 385.000, steps: 385\n","Episode 177: reward: 353.000, steps: 353\n","Episode 178: reward: 469.000, steps: 469\n","Episode 179: reward: 415.000, steps: 415\n","Episode 180: reward: 377.000, steps: 377\n","Episode 181: reward: 437.000, steps: 437\n","Episode 182: reward: 357.000, steps: 357\n","Episode 183: reward: 476.000, steps: 476\n","Episode 184: reward: 419.000, steps: 419\n","Episode 185: reward: 397.000, steps: 397\n","Episode 186: reward: 419.000, steps: 419\n","Episode 187: reward: 496.000, steps: 496\n","Episode 188: reward: 395.000, steps: 395\n","Episode 189: reward: 434.000, steps: 434\n","Episode 190: reward: 475.000, steps: 475\n","Episode 191: reward: 367.000, steps: 367\n","Episode 192: reward: 403.000, steps: 403\n","Episode 193: reward: 353.000, steps: 353\n","Episode 194: reward: 424.000, steps: 424\n","Episode 195: reward: 412.000, steps: 412\n","Episode 196: reward: 370.000, steps: 370\n","Episode 197: reward: 376.000, steps: 376\n","Episode 198: reward: 437.000, steps: 437\n","Episode 199: reward: 408.000, steps: 408\n","Episode 200: reward: 372.000, steps: 372\n","Episode 201: reward: 473.000, steps: 473\n","Episode 202: reward: 397.000, steps: 397\n","Episode 203: reward: 419.000, steps: 419\n","Episode 204: reward: 374.000, steps: 374\n","Episode 205: reward: 398.000, steps: 398\n","Episode 206: reward: 386.000, steps: 386\n","Episode 207: reward: 367.000, steps: 367\n","Episode 208: reward: 416.000, steps: 416\n","Episode 209: reward: 454.000, steps: 454\n","Episode 210: reward: 384.000, steps: 384\n","Episode 211: reward: 418.000, steps: 418\n","Episode 212: reward: 448.000, steps: 448\n","Episode 213: reward: 380.000, steps: 380\n","Episode 214: reward: 374.000, steps: 374\n","Episode 215: reward: 356.000, steps: 356\n","Episode 216: reward: 383.000, steps: 383\n","Episode 217: reward: 385.000, steps: 385\n","Episode 218: reward: 343.000, steps: 343\n","Episode 219: reward: 386.000, steps: 386\n","Episode 220: reward: 446.000, steps: 446\n","Episode 221: reward: 446.000, steps: 446\n","Episode 222: reward: 414.000, steps: 414\n","Episode 223: reward: 392.000, steps: 392\n","Episode 224: reward: 368.000, steps: 368\n","Episode 225: reward: 477.000, steps: 477\n","Episode 226: reward: 420.000, steps: 420\n","Episode 227: reward: 408.000, steps: 408\n","Episode 228: reward: 399.000, steps: 399\n","Episode 229: reward: 413.000, steps: 413\n","Episode 230: reward: 437.000, steps: 437\n","Episode 231: reward: 397.000, steps: 397\n","Episode 232: reward: 396.000, steps: 396\n","Episode 233: reward: 388.000, steps: 388\n","Episode 234: reward: 393.000, steps: 393\n","Episode 235: reward: 461.000, steps: 461\n","Episode 236: reward: 393.000, steps: 393\n","Episode 237: reward: 399.000, steps: 399\n","Episode 238: reward: 415.000, steps: 415\n","Episode 239: reward: 479.000, steps: 479\n","Episode 240: reward: 387.000, steps: 387\n","Episode 241: reward: 382.000, steps: 382\n","Episode 242: reward: 378.000, steps: 378\n","Episode 243: reward: 454.000, steps: 454\n","Episode 244: reward: 378.000, steps: 378\n","Episode 245: reward: 436.000, steps: 436\n","Episode 246: reward: 389.000, steps: 389\n","Episode 247: reward: 451.000, steps: 451\n","Episode 248: reward: 404.000, steps: 404\n","Episode 249: reward: 382.000, steps: 382\n","Episode 250: reward: 500.000, steps: 500\n","Episode 251: reward: 433.000, steps: 433\n","Episode 252: reward: 378.000, steps: 378\n","Episode 253: reward: 421.000, steps: 421\n","Episode 254: reward: 378.000, steps: 378\n","Episode 255: reward: 389.000, steps: 389\n","Episode 256: reward: 401.000, steps: 401\n","Episode 257: reward: 463.000, steps: 463\n","Episode 258: reward: 411.000, steps: 411\n","Episode 259: reward: 437.000, steps: 437\n","Episode 260: reward: 450.000, steps: 450\n","Episode 261: reward: 457.000, steps: 457\n","Episode 262: reward: 376.000, steps: 376\n","Episode 263: reward: 395.000, steps: 395\n","Episode 264: reward: 485.000, steps: 485\n","Episode 265: reward: 444.000, steps: 444\n","Episode 266: reward: 447.000, steps: 447\n","Episode 267: reward: 375.000, steps: 375\n","Episode 268: reward: 392.000, steps: 392\n","Episode 269: reward: 435.000, steps: 435\n","Episode 270: reward: 390.000, steps: 390\n","Episode 271: reward: 388.000, steps: 388\n","Episode 272: reward: 415.000, steps: 415\n","Episode 273: reward: 394.000, steps: 394\n","Episode 274: reward: 421.000, steps: 421\n","Episode 275: reward: 481.000, steps: 481\n","Episode 276: reward: 438.000, steps: 438\n","Episode 277: reward: 400.000, steps: 400\n","Episode 278: reward: 425.000, steps: 425\n","Episode 279: reward: 424.000, steps: 424\n","Episode 280: reward: 403.000, steps: 403\n","Episode 281: reward: 383.000, steps: 383\n","Episode 282: reward: 371.000, steps: 371\n","Episode 283: reward: 474.000, steps: 474\n","Episode 284: reward: 441.000, steps: 441\n","Episode 285: reward: 415.000, steps: 415\n","Episode 286: reward: 450.000, steps: 450\n","Episode 287: reward: 372.000, steps: 372\n","Episode 288: reward: 421.000, steps: 421\n","Episode 289: reward: 421.000, steps: 421\n","Episode 290: reward: 397.000, steps: 397\n","Episode 291: reward: 365.000, steps: 365\n","Episode 292: reward: 406.000, steps: 406\n","Episode 293: reward: 381.000, steps: 381\n","Episode 294: reward: 433.000, steps: 433\n","Episode 295: reward: 403.000, steps: 403\n","Episode 296: reward: 434.000, steps: 434\n","Episode 297: reward: 415.000, steps: 415\n","Episode 298: reward: 384.000, steps: 384\n","Episode 299: reward: 410.000, steps: 410\n","Episode 300: reward: 427.000, steps: 427\n","Episode 301: reward: 390.000, steps: 390\n","Episode 302: reward: 396.000, steps: 396\n","Episode 303: reward: 417.000, steps: 417\n","Episode 304: reward: 385.000, steps: 385\n","Episode 305: reward: 398.000, steps: 398\n","Episode 306: reward: 368.000, steps: 368\n","Episode 307: reward: 372.000, steps: 372\n","Episode 308: reward: 440.000, steps: 440\n","Episode 309: reward: 400.000, steps: 400\n","Episode 310: reward: 444.000, steps: 444\n","Episode 311: reward: 408.000, steps: 408\n","Episode 312: reward: 388.000, steps: 388\n","Episode 313: reward: 404.000, steps: 404\n","Episode 314: reward: 479.000, steps: 479\n","Episode 315: reward: 430.000, steps: 430\n","Episode 316: reward: 406.000, steps: 406\n","Episode 317: reward: 417.000, steps: 417\n","Episode 318: reward: 450.000, steps: 450\n","Episode 319: reward: 401.000, steps: 401\n","Episode 320: reward: 406.000, steps: 406\n","Episode 321: reward: 364.000, steps: 364\n","Episode 322: reward: 442.000, steps: 442\n","Episode 323: reward: 431.000, steps: 431\n","Episode 324: reward: 362.000, steps: 362\n","Episode 325: reward: 365.000, steps: 365\n","Episode 326: reward: 416.000, steps: 416\n","Episode 327: reward: 444.000, steps: 444\n","Episode 328: reward: 471.000, steps: 471\n","Episode 329: reward: 430.000, steps: 430\n","Episode 330: reward: 357.000, steps: 357\n","Episode 331: reward: 417.000, steps: 417\n","Episode 332: reward: 418.000, steps: 418\n","Episode 333: reward: 448.000, steps: 448\n","Episode 334: reward: 454.000, steps: 454\n","Episode 335: reward: 362.000, steps: 362\n","Episode 336: reward: 386.000, steps: 386\n","Episode 337: reward: 378.000, steps: 378\n","Episode 338: reward: 393.000, steps: 393\n","Episode 339: reward: 447.000, steps: 447\n","Episode 340: reward: 383.000, steps: 383\n","Episode 341: reward: 389.000, steps: 389\n","Episode 342: reward: 369.000, steps: 369\n","Episode 343: reward: 425.000, steps: 425\n","Episode 344: reward: 406.000, steps: 406\n","Episode 345: reward: 444.000, steps: 444\n","Episode 346: reward: 434.000, steps: 434\n","Episode 347: reward: 441.000, steps: 441\n","Episode 348: reward: 396.000, steps: 396\n","Episode 349: reward: 429.000, steps: 429\n","Episode 350: reward: 403.000, steps: 403\n","Episode 351: reward: 437.000, steps: 437\n","Episode 352: reward: 472.000, steps: 472\n","Episode 353: reward: 410.000, steps: 410\n","Episode 354: reward: 383.000, steps: 383\n","Episode 355: reward: 409.000, steps: 409\n","Episode 356: reward: 438.000, steps: 438\n","Episode 357: reward: 461.000, steps: 461\n","Episode 358: reward: 402.000, steps: 402\n","Episode 359: reward: 467.000, steps: 467\n","Episode 360: reward: 461.000, steps: 461\n","Episode 361: reward: 407.000, steps: 407\n","Episode 362: reward: 410.000, steps: 410\n","Episode 363: reward: 399.000, steps: 399\n","Episode 364: reward: 391.000, steps: 391\n","Episode 365: reward: 359.000, steps: 359\n","Episode 366: reward: 469.000, steps: 469\n","Episode 367: reward: 402.000, steps: 402\n","Episode 368: reward: 407.000, steps: 407\n","Episode 369: reward: 352.000, steps: 352\n","Episode 370: reward: 403.000, steps: 403\n","Episode 371: reward: 396.000, steps: 396\n","Episode 372: reward: 414.000, steps: 414\n","Episode 373: reward: 471.000, steps: 471\n","Episode 374: reward: 400.000, steps: 400\n","Episode 375: reward: 428.000, steps: 428\n","Episode 376: reward: 356.000, steps: 356\n","Episode 377: reward: 403.000, steps: 403\n","Episode 378: reward: 362.000, steps: 362\n","Episode 379: reward: 420.000, steps: 420\n","Episode 380: reward: 417.000, steps: 417\n","Episode 381: reward: 463.000, steps: 463\n","Episode 382: reward: 384.000, steps: 384\n","Episode 383: reward: 420.000, steps: 420\n","Episode 384: reward: 437.000, steps: 437\n","Episode 385: reward: 388.000, steps: 388\n","Episode 386: reward: 467.000, steps: 467\n","Episode 387: reward: 463.000, steps: 463\n","Episode 388: reward: 442.000, steps: 442\n","Episode 389: reward: 490.000, steps: 490\n","Episode 390: reward: 412.000, steps: 412\n","Episode 391: reward: 403.000, steps: 403\n","Episode 392: reward: 411.000, steps: 411\n","Episode 393: reward: 397.000, steps: 397\n","Episode 394: reward: 403.000, steps: 403\n","Episode 395: reward: 412.000, steps: 412\n","Episode 396: reward: 481.000, steps: 481\n","Episode 397: reward: 401.000, steps: 401\n","Episode 398: reward: 457.000, steps: 457\n","Episode 399: reward: 399.000, steps: 399\n","Episode 400: reward: 395.000, steps: 395\n","Episode 401: reward: 413.000, steps: 413\n","Episode 402: reward: 418.000, steps: 418\n","Episode 403: reward: 382.000, steps: 382\n","Episode 404: reward: 421.000, steps: 421\n","Episode 405: reward: 418.000, steps: 418\n","Episode 406: reward: 407.000, steps: 407\n","Episode 407: reward: 383.000, steps: 383\n","Episode 408: reward: 374.000, steps: 374\n","Episode 409: reward: 446.000, steps: 446\n","Episode 410: reward: 363.000, steps: 363\n","Episode 411: reward: 406.000, steps: 406\n","Episode 412: reward: 475.000, steps: 475\n","Episode 413: reward: 398.000, steps: 398\n","Episode 414: reward: 430.000, steps: 430\n","Episode 415: reward: 403.000, steps: 403\n","Episode 416: reward: 360.000, steps: 360\n","Episode 417: reward: 395.000, steps: 395\n","Episode 418: reward: 400.000, steps: 400\n","Episode 419: reward: 392.000, steps: 392\n","Episode 420: reward: 460.000, steps: 460\n","Episode 421: reward: 390.000, steps: 390\n","Episode 422: reward: 379.000, steps: 379\n","Episode 423: reward: 413.000, steps: 413\n","Episode 424: reward: 418.000, steps: 418\n","Episode 425: reward: 372.000, steps: 372\n","Episode 426: reward: 472.000, steps: 472\n","Episode 427: reward: 459.000, steps: 459\n","Episode 428: reward: 441.000, steps: 441\n","Episode 429: reward: 445.000, steps: 445\n","Episode 430: reward: 419.000, steps: 419\n","Episode 431: reward: 438.000, steps: 438\n","Episode 432: reward: 414.000, steps: 414\n","Episode 433: reward: 354.000, steps: 354\n","Episode 434: reward: 411.000, steps: 411\n","Episode 435: reward: 434.000, steps: 434\n","Episode 436: reward: 395.000, steps: 395\n","Episode 437: reward: 389.000, steps: 389\n","Episode 438: reward: 380.000, steps: 380\n","Episode 439: reward: 414.000, steps: 414\n","Episode 440: reward: 394.000, steps: 394\n","Episode 441: reward: 416.000, steps: 416\n","Episode 442: reward: 418.000, steps: 418\n","Episode 443: reward: 463.000, steps: 463\n","Episode 444: reward: 390.000, steps: 390\n","Episode 445: reward: 400.000, steps: 400\n","Episode 446: reward: 431.000, steps: 431\n","Episode 447: reward: 426.000, steps: 426\n","Episode 448: reward: 464.000, steps: 464\n","Episode 449: reward: 361.000, steps: 361\n","Episode 450: reward: 436.000, steps: 436\n","Episode 451: reward: 406.000, steps: 406\n","Episode 452: reward: 393.000, steps: 393\n","Episode 453: reward: 359.000, steps: 359\n","Episode 454: reward: 399.000, steps: 399\n","Episode 455: reward: 383.000, steps: 383\n","Episode 456: reward: 389.000, steps: 389\n","Episode 457: reward: 401.000, steps: 401\n","Episode 458: reward: 417.000, steps: 417\n","Episode 459: reward: 399.000, steps: 399\n","Episode 460: reward: 383.000, steps: 383\n","Episode 461: reward: 473.000, steps: 473\n","Episode 462: reward: 408.000, steps: 408\n","Episode 463: reward: 403.000, steps: 403\n","Episode 464: reward: 398.000, steps: 398\n","Episode 465: reward: 431.000, steps: 431\n","Episode 466: reward: 468.000, steps: 468\n","Episode 467: reward: 388.000, steps: 388\n","Episode 468: reward: 389.000, steps: 389\n","Episode 469: reward: 406.000, steps: 406\n","Episode 470: reward: 408.000, steps: 408\n","Episode 471: reward: 401.000, steps: 401\n","Episode 472: reward: 443.000, steps: 443\n","Episode 473: reward: 398.000, steps: 398\n","Episode 474: reward: 464.000, steps: 464\n","Episode 475: reward: 434.000, steps: 434\n","Episode 476: reward: 458.000, steps: 458\n","Episode 477: reward: 380.000, steps: 380\n","Episode 478: reward: 406.000, steps: 406\n","Episode 479: reward: 387.000, steps: 387\n","Episode 480: reward: 402.000, steps: 402\n","Episode 481: reward: 404.000, steps: 404\n","Episode 482: reward: 394.000, steps: 394\n","Episode 483: reward: 471.000, steps: 471\n","Episode 484: reward: 427.000, steps: 427\n","Episode 485: reward: 500.000, steps: 500\n","Episode 486: reward: 434.000, steps: 434\n","Episode 487: reward: 373.000, steps: 373\n","Episode 488: reward: 491.000, steps: 491\n","Episode 489: reward: 404.000, steps: 404\n","Episode 490: reward: 477.000, steps: 477\n","Episode 491: reward: 372.000, steps: 372\n","Episode 492: reward: 426.000, steps: 426\n","Episode 493: reward: 440.000, steps: 440\n","Episode 494: reward: 426.000, steps: 426\n","Episode 495: reward: 387.000, steps: 387\n","Episode 496: reward: 469.000, steps: 469\n","Episode 497: reward: 421.000, steps: 421\n","Episode 498: reward: 406.000, steps: 406\n","Episode 499: reward: 444.000, steps: 444\n","Episode 500: reward: 408.000, steps: 408\n","Episode 501: reward: 408.000, steps: 408\n","Episode 502: reward: 455.000, steps: 455\n","Episode 503: reward: 362.000, steps: 362\n","Episode 504: reward: 403.000, steps: 403\n","Episode 505: reward: 396.000, steps: 396\n","Episode 506: reward: 417.000, steps: 417\n","Episode 507: reward: 399.000, steps: 399\n","Episode 508: reward: 400.000, steps: 400\n","Episode 509: reward: 429.000, steps: 429\n","Episode 510: reward: 428.000, steps: 428\n","Episode 511: reward: 374.000, steps: 374\n","Episode 512: reward: 419.000, steps: 419\n","Episode 513: reward: 464.000, steps: 464\n","Episode 514: reward: 495.000, steps: 495\n","Episode 515: reward: 398.000, steps: 398\n","Episode 516: reward: 416.000, steps: 416\n","Episode 517: reward: 388.000, steps: 388\n","Episode 518: reward: 387.000, steps: 387\n","Episode 519: reward: 424.000, steps: 424\n","Episode 520: reward: 416.000, steps: 416\n","Episode 521: reward: 406.000, steps: 406\n","Episode 522: reward: 466.000, steps: 466\n","Episode 523: reward: 375.000, steps: 375\n","Episode 524: reward: 425.000, steps: 425\n","Episode 525: reward: 398.000, steps: 398\n","Episode 526: reward: 377.000, steps: 377\n","Episode 527: reward: 470.000, steps: 470\n","Episode 528: reward: 379.000, steps: 379\n","Episode 529: reward: 399.000, steps: 399\n","Episode 530: reward: 423.000, steps: 423\n","Episode 531: reward: 397.000, steps: 397\n","Episode 532: reward: 384.000, steps: 384\n","Episode 533: reward: 435.000, steps: 435\n","Episode 534: reward: 360.000, steps: 360\n","Episode 535: reward: 352.000, steps: 352\n","Episode 536: reward: 416.000, steps: 416\n","Episode 537: reward: 421.000, steps: 421\n","Episode 538: reward: 440.000, steps: 440\n","Episode 539: reward: 430.000, steps: 430\n","Episode 540: reward: 453.000, steps: 453\n","Episode 541: reward: 387.000, steps: 387\n","Episode 542: reward: 394.000, steps: 394\n","Episode 543: reward: 446.000, steps: 446\n","Episode 544: reward: 447.000, steps: 447\n","Episode 545: reward: 410.000, steps: 410\n","Episode 546: reward: 426.000, steps: 426\n","Episode 547: reward: 429.000, steps: 429\n","Episode 548: reward: 444.000, steps: 444\n","Episode 549: reward: 466.000, steps: 466\n","Episode 550: reward: 414.000, steps: 414\n","Episode 551: reward: 423.000, steps: 423\n","Episode 552: reward: 383.000, steps: 383\n","Episode 553: reward: 458.000, steps: 458\n","Episode 554: reward: 410.000, steps: 410\n","Episode 555: reward: 438.000, steps: 438\n","Episode 556: reward: 385.000, steps: 385\n","Episode 557: reward: 363.000, steps: 363\n","Episode 558: reward: 454.000, steps: 454\n","Episode 559: reward: 401.000, steps: 401\n","Episode 560: reward: 460.000, steps: 460\n","Episode 561: reward: 389.000, steps: 389\n","Episode 562: reward: 432.000, steps: 432\n","Episode 563: reward: 379.000, steps: 379\n","Episode 564: reward: 443.000, steps: 443\n","Episode 565: reward: 385.000, steps: 385\n","Episode 566: reward: 392.000, steps: 392\n","Episode 567: reward: 448.000, steps: 448\n","Episode 568: reward: 391.000, steps: 391\n","Episode 569: reward: 410.000, steps: 410\n","Episode 570: reward: 418.000, steps: 418\n","Episode 571: reward: 409.000, steps: 409\n","Episode 572: reward: 440.000, steps: 440\n","Episode 573: reward: 376.000, steps: 376\n","Episode 574: reward: 426.000, steps: 426\n","Episode 575: reward: 416.000, steps: 416\n","Episode 576: reward: 439.000, steps: 439\n","Episode 577: reward: 401.000, steps: 401\n","Episode 578: reward: 491.000, steps: 491\n","Episode 579: reward: 448.000, steps: 448\n","Episode 580: reward: 407.000, steps: 407\n","Episode 581: reward: 435.000, steps: 435\n","Episode 582: reward: 413.000, steps: 413\n","Episode 583: reward: 410.000, steps: 410\n","Episode 584: reward: 441.000, steps: 441\n","Episode 585: reward: 449.000, steps: 449\n","Episode 586: reward: 389.000, steps: 389\n","Episode 587: reward: 372.000, steps: 372\n","Episode 588: reward: 395.000, steps: 395\n","Episode 589: reward: 393.000, steps: 393\n","Episode 590: reward: 418.000, steps: 418\n","Episode 591: reward: 412.000, steps: 412\n","Episode 592: reward: 427.000, steps: 427\n","Episode 593: reward: 386.000, steps: 386\n","Episode 594: reward: 393.000, steps: 393\n","Episode 595: reward: 420.000, steps: 420\n","Episode 596: reward: 435.000, steps: 435\n","Episode 597: reward: 437.000, steps: 437\n","Episode 598: reward: 473.000, steps: 473\n","Episode 599: reward: 409.000, steps: 409\n","Episode 600: reward: 412.000, steps: 412\n","Episode 601: reward: 416.000, steps: 416\n","Episode 602: reward: 414.000, steps: 414\n","Episode 603: reward: 384.000, steps: 384\n","Episode 604: reward: 381.000, steps: 381\n","Episode 605: reward: 446.000, steps: 446\n","Episode 606: reward: 421.000, steps: 421\n","Episode 607: reward: 371.000, steps: 371\n","Episode 608: reward: 424.000, steps: 424\n","Episode 609: reward: 500.000, steps: 500\n","Episode 610: reward: 438.000, steps: 438\n","Episode 611: reward: 468.000, steps: 468\n","Episode 612: reward: 392.000, steps: 392\n","Episode 613: reward: 387.000, steps: 387\n","Episode 614: reward: 400.000, steps: 400\n","Episode 615: reward: 388.000, steps: 388\n","Episode 616: reward: 460.000, steps: 460\n","Episode 617: reward: 420.000, steps: 420\n","Episode 618: reward: 397.000, steps: 397\n","Episode 619: reward: 414.000, steps: 414\n","Episode 620: reward: 401.000, steps: 401\n","Episode 621: reward: 365.000, steps: 365\n","Episode 622: reward: 430.000, steps: 430\n","Episode 623: reward: 423.000, steps: 423\n","Episode 624: reward: 356.000, steps: 356\n","Episode 625: reward: 438.000, steps: 438\n","Episode 626: reward: 477.000, steps: 477\n","Episode 627: reward: 396.000, steps: 396\n","Episode 628: reward: 412.000, steps: 412\n","Episode 629: reward: 383.000, steps: 383\n","Episode 630: reward: 423.000, steps: 423\n","Episode 631: reward: 427.000, steps: 427\n","Episode 632: reward: 421.000, steps: 421\n","Episode 633: reward: 483.000, steps: 483\n","Episode 634: reward: 500.000, steps: 500\n","Episode 635: reward: 401.000, steps: 401\n","Episode 636: reward: 407.000, steps: 407\n","Episode 637: reward: 410.000, steps: 410\n","Episode 638: reward: 374.000, steps: 374\n","Episode 639: reward: 375.000, steps: 375\n","Episode 640: reward: 355.000, steps: 355\n","Episode 641: reward: 407.000, steps: 407\n","Episode 642: reward: 432.000, steps: 432\n","Episode 643: reward: 420.000, steps: 420\n","Episode 644: reward: 391.000, steps: 391\n","Episode 645: reward: 398.000, steps: 398\n","Episode 646: reward: 440.000, steps: 440\n","Episode 647: reward: 391.000, steps: 391\n","Episode 648: reward: 393.000, steps: 393\n","Episode 649: reward: 481.000, steps: 481\n","Episode 650: reward: 360.000, steps: 360\n","Episode 651: reward: 393.000, steps: 393\n","Episode 652: reward: 398.000, steps: 398\n","Episode 653: reward: 375.000, steps: 375\n","Episode 654: reward: 406.000, steps: 406\n","Episode 655: reward: 393.000, steps: 393\n","Episode 656: reward: 379.000, steps: 379\n","Episode 657: reward: 369.000, steps: 369\n","Episode 658: reward: 388.000, steps: 388\n","Episode 659: reward: 434.000, steps: 434\n","Episode 660: reward: 441.000, steps: 441\n","Episode 661: reward: 392.000, steps: 392\n","Episode 662: reward: 382.000, steps: 382\n","Episode 663: reward: 387.000, steps: 387\n","Episode 664: reward: 349.000, steps: 349\n","Episode 665: reward: 381.000, steps: 381\n","Episode 666: reward: 430.000, steps: 430\n","Episode 667: reward: 437.000, steps: 437\n","Episode 668: reward: 399.000, steps: 399\n","Episode 669: reward: 451.000, steps: 451\n","Episode 670: reward: 442.000, steps: 442\n","Episode 671: reward: 447.000, steps: 447\n","Episode 672: reward: 444.000, steps: 444\n","Episode 673: reward: 357.000, steps: 357\n","Episode 674: reward: 454.000, steps: 454\n","Episode 675: reward: 440.000, steps: 440\n","Episode 676: reward: 466.000, steps: 466\n","Episode 677: reward: 395.000, steps: 395\n","Episode 678: reward: 364.000, steps: 364\n","Episode 679: reward: 387.000, steps: 387\n","Episode 680: reward: 403.000, steps: 403\n","Episode 681: reward: 463.000, steps: 463\n","Episode 682: reward: 463.000, steps: 463\n","Episode 683: reward: 471.000, steps: 471\n","Episode 684: reward: 413.000, steps: 413\n","Episode 685: reward: 392.000, steps: 392\n","Episode 686: reward: 412.000, steps: 412\n","Episode 687: reward: 402.000, steps: 402\n","Episode 688: reward: 371.000, steps: 371\n","Episode 689: reward: 446.000, steps: 446\n","Episode 690: reward: 436.000, steps: 436\n","Episode 691: reward: 367.000, steps: 367\n","Episode 692: reward: 398.000, steps: 398\n","Episode 693: reward: 412.000, steps: 412\n","Episode 694: reward: 399.000, steps: 399\n","Episode 695: reward: 356.000, steps: 356\n","Episode 696: reward: 440.000, steps: 440\n","Episode 697: reward: 427.000, steps: 427\n","Episode 698: reward: 396.000, steps: 396\n","Episode 699: reward: 418.000, steps: 418\n","Episode 700: reward: 382.000, steps: 382\n","Episode 701: reward: 453.000, steps: 453\n","Episode 702: reward: 482.000, steps: 482\n","Episode 703: reward: 405.000, steps: 405\n","Episode 704: reward: 415.000, steps: 415\n","Episode 705: reward: 420.000, steps: 420\n","Episode 706: reward: 418.000, steps: 418\n","Episode 707: reward: 460.000, steps: 460\n","Episode 708: reward: 423.000, steps: 423\n","Episode 709: reward: 401.000, steps: 401\n","Episode 710: reward: 398.000, steps: 398\n","Episode 711: reward: 423.000, steps: 423\n","Episode 712: reward: 420.000, steps: 420\n","Episode 713: reward: 420.000, steps: 420\n","Episode 714: reward: 418.000, steps: 418\n","Episode 715: reward: 441.000, steps: 441\n","Episode 716: reward: 360.000, steps: 360\n","Episode 717: reward: 404.000, steps: 404\n","Episode 718: reward: 390.000, steps: 390\n","Episode 719: reward: 374.000, steps: 374\n","Episode 720: reward: 391.000, steps: 391\n","Episode 721: reward: 387.000, steps: 387\n","Episode 722: reward: 466.000, steps: 466\n","Episode 723: reward: 500.000, steps: 500\n","Episode 724: reward: 398.000, steps: 398\n","Episode 725: reward: 411.000, steps: 411\n","Episode 726: reward: 407.000, steps: 407\n","Episode 727: reward: 379.000, steps: 379\n","Episode 728: reward: 388.000, steps: 388\n","Episode 729: reward: 403.000, steps: 403\n","Episode 730: reward: 369.000, steps: 369\n","Episode 731: reward: 418.000, steps: 418\n","Episode 732: reward: 429.000, steps: 429\n","Episode 733: reward: 357.000, steps: 357\n","Episode 734: reward: 354.000, steps: 354\n","Episode 735: reward: 400.000, steps: 400\n","Episode 736: reward: 363.000, steps: 363\n","Episode 737: reward: 384.000, steps: 384\n","Episode 738: reward: 368.000, steps: 368\n","Episode 739: reward: 351.000, steps: 351\n","Episode 740: reward: 414.000, steps: 414\n","Episode 741: reward: 420.000, steps: 420\n","Episode 742: reward: 460.000, steps: 460\n","Episode 743: reward: 434.000, steps: 434\n","Episode 744: reward: 427.000, steps: 427\n","Episode 745: reward: 431.000, steps: 431\n","Episode 746: reward: 387.000, steps: 387\n","Episode 747: reward: 411.000, steps: 411\n","Episode 748: reward: 435.000, steps: 435\n","Episode 749: reward: 445.000, steps: 445\n","Episode 750: reward: 406.000, steps: 406\n","Episode 751: reward: 486.000, steps: 486\n","Episode 752: reward: 431.000, steps: 431\n","Episode 753: reward: 421.000, steps: 421\n","Episode 754: reward: 418.000, steps: 418\n","Episode 755: reward: 414.000, steps: 414\n","Episode 756: reward: 500.000, steps: 500\n","Episode 757: reward: 390.000, steps: 390\n","Episode 758: reward: 445.000, steps: 445\n","Episode 759: reward: 422.000, steps: 422\n","Episode 760: reward: 460.000, steps: 460\n","Episode 761: reward: 407.000, steps: 407\n","Episode 762: reward: 386.000, steps: 386\n","Episode 763: reward: 393.000, steps: 393\n","Episode 764: reward: 404.000, steps: 404\n","Episode 765: reward: 406.000, steps: 406\n","Episode 766: reward: 461.000, steps: 461\n","Episode 767: reward: 403.000, steps: 403\n","Episode 768: reward: 478.000, steps: 478\n","Episode 769: reward: 385.000, steps: 385\n","Episode 770: reward: 407.000, steps: 407\n","Episode 771: reward: 407.000, steps: 407\n","Episode 772: reward: 397.000, steps: 397\n","Episode 773: reward: 404.000, steps: 404\n","Episode 774: reward: 459.000, steps: 459\n","Episode 775: reward: 415.000, steps: 415\n","Episode 776: reward: 428.000, steps: 428\n","Episode 777: reward: 437.000, steps: 437\n","Episode 778: reward: 399.000, steps: 399\n","Episode 779: reward: 384.000, steps: 384\n","Episode 780: reward: 484.000, steps: 484\n","Episode 781: reward: 394.000, steps: 394\n","Episode 782: reward: 358.000, steps: 358\n","Episode 783: reward: 430.000, steps: 430\n","Episode 784: reward: 437.000, steps: 437\n","Episode 785: reward: 441.000, steps: 441\n","Episode 786: reward: 460.000, steps: 460\n","Episode 787: reward: 395.000, steps: 395\n","Episode 788: reward: 412.000, steps: 412\n","Episode 789: reward: 495.000, steps: 495\n","Episode 790: reward: 365.000, steps: 365\n","Episode 791: reward: 411.000, steps: 411\n","Episode 792: reward: 364.000, steps: 364\n","Episode 793: reward: 429.000, steps: 429\n","Episode 794: reward: 410.000, steps: 410\n","Episode 795: reward: 457.000, steps: 457\n","Episode 796: reward: 403.000, steps: 403\n","Episode 797: reward: 419.000, steps: 419\n","Episode 798: reward: 366.000, steps: 366\n","Episode 799: reward: 447.000, steps: 447\n","Episode 800: reward: 392.000, steps: 392\n","Episode 801: reward: 389.000, steps: 389\n","Episode 802: reward: 397.000, steps: 397\n","Episode 803: reward: 394.000, steps: 394\n","Episode 804: reward: 383.000, steps: 383\n","Episode 805: reward: 412.000, steps: 412\n","Episode 806: reward: 468.000, steps: 468\n","Episode 807: reward: 373.000, steps: 373\n","Episode 808: reward: 440.000, steps: 440\n","Episode 809: reward: 352.000, steps: 352\n","Episode 810: reward: 431.000, steps: 431\n","Episode 811: reward: 382.000, steps: 382\n","Episode 812: reward: 403.000, steps: 403\n","Episode 813: reward: 383.000, steps: 383\n","Episode 814: reward: 413.000, steps: 413\n","Episode 815: reward: 403.000, steps: 403\n","Episode 816: reward: 472.000, steps: 472\n","Episode 817: reward: 430.000, steps: 430\n","Episode 818: reward: 381.000, steps: 381\n","Episode 819: reward: 413.000, steps: 413\n","Episode 820: reward: 429.000, steps: 429\n","Episode 821: reward: 373.000, steps: 373\n","Episode 822: reward: 378.000, steps: 378\n","Episode 823: reward: 410.000, steps: 410\n","Episode 824: reward: 388.000, steps: 388\n","Episode 825: reward: 404.000, steps: 404\n","Episode 826: reward: 448.000, steps: 448\n","Episode 827: reward: 463.000, steps: 463\n","Episode 828: reward: 387.000, steps: 387\n","Episode 829: reward: 477.000, steps: 477\n","Episode 830: reward: 437.000, steps: 437\n","Episode 831: reward: 444.000, steps: 444\n","Episode 832: reward: 372.000, steps: 372\n","Episode 833: reward: 428.000, steps: 428\n","Episode 834: reward: 452.000, steps: 452\n","Episode 835: reward: 381.000, steps: 381\n","Episode 836: reward: 384.000, steps: 384\n","Episode 837: reward: 408.000, steps: 408\n","Episode 838: reward: 363.000, steps: 363\n","Episode 839: reward: 381.000, steps: 381\n","Episode 840: reward: 428.000, steps: 428\n","Episode 841: reward: 423.000, steps: 423\n","Episode 842: reward: 376.000, steps: 376\n","Episode 843: reward: 416.000, steps: 416\n","Episode 844: reward: 391.000, steps: 391\n","Episode 845: reward: 418.000, steps: 418\n","Episode 846: reward: 364.000, steps: 364\n","Episode 847: reward: 384.000, steps: 384\n","Episode 848: reward: 369.000, steps: 369\n","Episode 849: reward: 419.000, steps: 419\n","Episode 850: reward: 410.000, steps: 410\n","Episode 851: reward: 398.000, steps: 398\n","Episode 852: reward: 453.000, steps: 453\n","Episode 853: reward: 446.000, steps: 446\n","Episode 854: reward: 432.000, steps: 432\n","Episode 855: reward: 473.000, steps: 473\n","Episode 856: reward: 387.000, steps: 387\n","Episode 857: reward: 381.000, steps: 381\n","Episode 858: reward: 432.000, steps: 432\n","Episode 859: reward: 436.000, steps: 436\n","Episode 860: reward: 397.000, steps: 397\n","Episode 861: reward: 442.000, steps: 442\n","Episode 862: reward: 382.000, steps: 382\n","Episode 863: reward: 381.000, steps: 381\n","Episode 864: reward: 419.000, steps: 419\n","Episode 865: reward: 399.000, steps: 399\n","Episode 866: reward: 407.000, steps: 407\n","Episode 867: reward: 429.000, steps: 429\n","Episode 868: reward: 359.000, steps: 359\n","Episode 869: reward: 389.000, steps: 389\n","Episode 870: reward: 398.000, steps: 398\n","Episode 871: reward: 431.000, steps: 431\n","Episode 872: reward: 410.000, steps: 410\n","Episode 873: reward: 362.000, steps: 362\n","Episode 874: reward: 417.000, steps: 417\n","Episode 875: reward: 434.000, steps: 434\n","Episode 876: reward: 409.000, steps: 409\n","Episode 877: reward: 378.000, steps: 378\n","Episode 878: reward: 422.000, steps: 422\n","Episode 879: reward: 465.000, steps: 465\n","Episode 880: reward: 490.000, steps: 490\n","Episode 881: reward: 373.000, steps: 373\n","Episode 882: reward: 376.000, steps: 376\n","Episode 883: reward: 483.000, steps: 483\n","Episode 884: reward: 412.000, steps: 412\n","Episode 885: reward: 372.000, steps: 372\n","Episode 886: reward: 396.000, steps: 396\n","Episode 887: reward: 399.000, steps: 399\n","Episode 888: reward: 449.000, steps: 449\n","Episode 889: reward: 431.000, steps: 431\n","Episode 890: reward: 443.000, steps: 443\n","Episode 891: reward: 399.000, steps: 399\n","Episode 892: reward: 406.000, steps: 406\n","Episode 893: reward: 392.000, steps: 392\n","Episode 894: reward: 432.000, steps: 432\n","Episode 895: reward: 410.000, steps: 410\n","Episode 896: reward: 371.000, steps: 371\n","Episode 897: reward: 468.000, steps: 468\n","Episode 898: reward: 460.000, steps: 460\n","Episode 899: reward: 386.000, steps: 386\n","Episode 900: reward: 386.000, steps: 386\n","Episode 901: reward: 426.000, steps: 426\n","Episode 902: reward: 415.000, steps: 415\n","Episode 903: reward: 384.000, steps: 384\n","Episode 904: reward: 448.000, steps: 448\n","Episode 905: reward: 500.000, steps: 500\n","Episode 906: reward: 391.000, steps: 391\n","Episode 907: reward: 468.000, steps: 468\n","Episode 908: reward: 446.000, steps: 446\n","Episode 909: reward: 400.000, steps: 400\n","Episode 910: reward: 468.000, steps: 468\n","Episode 911: reward: 400.000, steps: 400\n","Episode 912: reward: 474.000, steps: 474\n","Episode 913: reward: 365.000, steps: 365\n","Episode 914: reward: 452.000, steps: 452\n","Episode 915: reward: 395.000, steps: 395\n","Episode 916: reward: 361.000, steps: 361\n","Episode 917: reward: 442.000, steps: 442\n","Episode 918: reward: 357.000, steps: 357\n","Episode 919: reward: 421.000, steps: 421\n","Episode 920: reward: 398.000, steps: 398\n","Episode 921: reward: 395.000, steps: 395\n","Episode 922: reward: 366.000, steps: 366\n","Episode 923: reward: 418.000, steps: 418\n","Episode 924: reward: 368.000, steps: 368\n","Episode 925: reward: 424.000, steps: 424\n","Episode 926: reward: 371.000, steps: 371\n","Episode 927: reward: 431.000, steps: 431\n","Episode 928: reward: 435.000, steps: 435\n","Episode 929: reward: 464.000, steps: 464\n","Episode 930: reward: 439.000, steps: 439\n","Episode 931: reward: 378.000, steps: 378\n","Episode 932: reward: 464.000, steps: 464\n","Episode 933: reward: 489.000, steps: 489\n","Episode 934: reward: 417.000, steps: 417\n","Episode 935: reward: 420.000, steps: 420\n","Episode 936: reward: 371.000, steps: 371\n","Episode 937: reward: 410.000, steps: 410\n","Episode 938: reward: 426.000, steps: 426\n","Episode 939: reward: 443.000, steps: 443\n","Episode 940: reward: 414.000, steps: 414\n","Episode 941: reward: 358.000, steps: 358\n","Episode 942: reward: 374.000, steps: 374\n","Episode 943: reward: 393.000, steps: 393\n","Episode 944: reward: 460.000, steps: 460\n","Episode 945: reward: 440.000, steps: 440\n","Episode 946: reward: 363.000, steps: 363\n","Episode 947: reward: 439.000, steps: 439\n","Episode 948: reward: 377.000, steps: 377\n","Episode 949: reward: 420.000, steps: 420\n","Episode 950: reward: 384.000, steps: 384\n","Episode 951: reward: 402.000, steps: 402\n","Episode 952: reward: 398.000, steps: 398\n","Episode 953: reward: 466.000, steps: 466\n","Episode 954: reward: 404.000, steps: 404\n","Episode 955: reward: 374.000, steps: 374\n","Episode 956: reward: 407.000, steps: 407\n","Episode 957: reward: 365.000, steps: 365\n","Episode 958: reward: 403.000, steps: 403\n","Episode 959: reward: 410.000, steps: 410\n","Episode 960: reward: 372.000, steps: 372\n","Episode 961: reward: 419.000, steps: 419\n","Episode 962: reward: 443.000, steps: 443\n","Episode 963: reward: 446.000, steps: 446\n","Episode 964: reward: 417.000, steps: 417\n","Episode 965: reward: 390.000, steps: 390\n","Episode 966: reward: 403.000, steps: 403\n","Episode 967: reward: 454.000, steps: 454\n","Episode 968: reward: 419.000, steps: 419\n","Episode 969: reward: 403.000, steps: 403\n","Episode 970: reward: 387.000, steps: 387\n","Episode 971: reward: 402.000, steps: 402\n","Episode 972: reward: 368.000, steps: 368\n","Episode 973: reward: 468.000, steps: 468\n","Episode 974: reward: 408.000, steps: 408\n","Episode 975: reward: 418.000, steps: 418\n","Episode 976: reward: 475.000, steps: 475\n","Episode 977: reward: 398.000, steps: 398\n","Episode 978: reward: 431.000, steps: 431\n","Episode 979: reward: 422.000, steps: 422\n","Episode 980: reward: 382.000, steps: 382\n","Episode 981: reward: 385.000, steps: 385\n","Episode 982: reward: 415.000, steps: 415\n","Episode 983: reward: 466.000, steps: 466\n","Episode 984: reward: 442.000, steps: 442\n","Episode 985: reward: 445.000, steps: 445\n","Episode 986: reward: 409.000, steps: 409\n","Episode 987: reward: 387.000, steps: 387\n","Episode 988: reward: 364.000, steps: 364\n","Episode 989: reward: 365.000, steps: 365\n","Episode 990: reward: 380.000, steps: 380\n","Episode 991: reward: 475.000, steps: 475\n","Episode 992: reward: 405.000, steps: 405\n","Episode 993: reward: 405.000, steps: 405\n","Episode 994: reward: 393.000, steps: 393\n","Episode 995: reward: 413.000, steps: 413\n","Episode 996: reward: 458.000, steps: 458\n","Episode 997: reward: 413.000, steps: 413\n","Episode 998: reward: 433.000, steps: 433\n","Episode 999: reward: 375.000, steps: 375\n","Episode 1000: reward: 386.000, steps: 386\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GclvRZ0IDJNw","colab_type":"code","outputId":"09ce40bd-6e09-4663-9b40-d7396709155a","executionInfo":{"status":"ok","timestamp":1587486391447,"user_tz":-330,"elapsed":363025,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# print('Average score over 1000 test games: {}'.format(np.mean(scores.history['episode_reward'])))"],"execution_count":78,"outputs":[{"output_type":"stream","text":["Average score over 1000 test games: 413.584\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7trDo3MvDNj2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"30b1c5a2-9ad1-4417-ab53-a0eaba8d30d2","executionInfo":{"status":"ok","timestamp":1587486424854,"user_tz":-330,"elapsed":396374,"user":{"displayName":"Akshit Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT7mEzo4Xl5HM1PVpZcT_xG8Y80JRpzG7zoIty9w=s64","userId":"12957378049941848214"}}},"source":["# sarsa.save_weights(model_file)"],"execution_count":79,"outputs":[{"output_type":"stream","text":["[WARNING] /content/drive/My Drive/ML Major Project/Models/Akshit's Models/Task1/Model#11.h5 already exists - overwrite? [y/n]y\n","[TIP] Next time specify overwrite=True!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_Gk1lx9RL4rZ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}